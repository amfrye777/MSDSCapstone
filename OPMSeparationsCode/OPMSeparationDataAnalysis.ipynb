{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPM Data Separation Analysis\n",
    "<i><b>\n",
    "Christopher Boomhower<sub>1</sub>, Stacey Fabricant<sub>2</sub>, Alex Frye<sub>1</sub>, David Mumford<sub>2</sub>, Michael Smith<sub>1</sub>, Lindsay Vitovsky<sub>1</sub>\n",
    "\n",
    "<sub>1</sub> Southern Methodis Univeristy, Dallas, TX, US\n",
    "<sub>2</sub> Penn Mutual Life Insurance Co, Horsham PA\n",
    "</i></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "background text...\n",
    "\n",
    "**our intent is to: 1)..2)...3)........**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "Data Source Background Text & citation links\n",
    "\n",
    "Dataset Attribute Descriptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "To begin our analysis, we need to load the data from our 89 source .txt files. Data is separated into two separate groups of files; Separation and Non-Separation, thus data is loaded in two separate phases, then unioned together. Once data is loaded, Steps taken to remove non-US observations or those with no specified occupation, no specified salary, or no specified length of service level.  Of a total 8,423,336 observations, we end with 8,232,375 after removal of these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import pickle\n",
    "import os\n",
    "import psutil\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import missingno as msno\n",
    "import prettytable\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Library Options\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pre-defined Functions for use later\n",
    "def pickleObject(objectname, filename, filepath = \"PickleJar/\"):\n",
    "    fullpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create a variable to pickle and open it in write mode\n",
    "    picklefile = open(fullpicklepath, 'wb')\n",
    "    pickle.dump(objectname, picklefile)\n",
    "    picklefile.close()\n",
    "    \n",
    "def unpickleObject(filename, filepath = \"PickleJar/\"):\n",
    "    fullunpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create an variable to pickle and open it in write mode\n",
    "    unpicklefile = open(fullunpicklepath, 'rb')\n",
    "    unpickleObject = pickle.load(unpicklefile)\n",
    "    unpicklefile.close()\n",
    "\n",
    "    return unpickleObject\n",
    "    \n",
    "def clear_display():\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "## Pre-defined variables for use later\n",
    "dataOPMPath = \"dataOPM\"\n",
    "dataEMPPath = \"dataEMP\"\n",
    "PickleJarPath = \"PickleJar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Load OPMSeparation Files\n",
    "\n",
    "OPMDataFiles = glob.glob(os.path.join(dataOPMPath, \"*.txt\"))\n",
    "\n",
    "for i in range(0,len(OPMDataFiles)):\n",
    "    OPMDataFiles[i] = OPMDataFiles[i].replace(\"\\\\\",\"/\")\n",
    "\n",
    "OPMDataList = []\n",
    "\n",
    "for i,j in zip(OPMDataFiles,range(0,len(OPMDataFiles))):\n",
    "    OPMDataList.append(pd.read_csv(i, dtype = 'str'))\n",
    "    display(OPMDataList[j].head())\n",
    "\n",
    "## Load the SEPDATA_FY2015 file into it's own object\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/SEPDATA_FY2015.txt']\n",
    "OPMDataOrig = OPMDataList[indexes[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#print(OPMDataFiles)\n",
    "\n",
    "print(len(OPMDataOrig))\n",
    "\n",
    "##### Merge / Modify Codes / Aggregate Attributes to be more descriptive per the metadata files\n",
    "\n",
    "OPMDataMerged = OPMDataOrig.copy()\n",
    "\n",
    "##AGYSUB - AGYTYP, AGY\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTagy.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'AGYSUB', how = 'left')\n",
    "\n",
    "##EFDate - quarter, month\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTefdate.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'EFDATE', how = 'left')\n",
    "\n",
    "##AGELVL - AGELVLT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTagelvl.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'AGELVL', how = 'left')\n",
    "\n",
    "##LOSLVL - LOSLVLT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTloslvl.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'LOSLVL', how = 'left')\n",
    "\n",
    "##LOC - LocTypeT, LocT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTloc.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'LOC', how = 'left')\n",
    "\n",
    "##OCC - OCCTYPT, OCCFAM\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTocc.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'OCC', how = 'left')\n",
    "\n",
    "##PATCO - PATCOT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTpatco.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'PATCO', how = 'left')\n",
    "\n",
    "##PPGRD - PayPlan, PPGroup, PPTYP\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTppgrd.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'PPGRD', how = 'left')\n",
    "\n",
    "##SALLVL - SALLVLT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTsallvl.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'SALLVL', how = 'left')\n",
    "\n",
    "##TOA - TOATYP\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTtoa.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'TOA', how = 'left')\n",
    "\n",
    "##WORKSCH - WSTYPT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTwrksch.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'WORKSCH', how = 'left')\n",
    "\n",
    "\n",
    "## Modify Data Types for numeric objects\n",
    "OPMDataMerged[\"SALARY\"] = OPMDataMerged[\"SALARY\"].apply(pd.to_numeric)\n",
    "OPMDataMerged[\"COUNT\"]  = OPMDataMerged[\"COUNT\"].apply(pd.to_numeric)\n",
    "OPMDataMerged[\"LOS\"]    = OPMDataMerged[\"LOS\"].apply(pd.to_numeric)\n",
    "\n",
    "print(\"Original SEP data size of: \"+str(len(OPMDataMerged)))\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"LOCTYP\"] != \"1\"]))+\" Non-US observations.\")\n",
    "\n",
    "    ## Remove Non-US Data\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"LOCTYP\"] == \"1\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"OCCTYP\"] == \"3\"]))+\" observations with no specified Occupation.\")\n",
    "\n",
    "   ## Remove Observations with no specified occupation\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"OCCTYP\"] != \"3\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"SALLVL\"] == \"Z\"]))+\" observations with no specified Salary.\")\n",
    "\n",
    "   ## Remove Observations with no specified salary\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"SALLVL\"] != \"Z\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"LOSLVL\"] == \"Z\"]))+\" observations with no specified Length of Service.\")\n",
    "\n",
    "   ## Remove Observations with no specified LOSLVL\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"LOSLVL\"] != \"Z\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"AGELVL\"] == \"A\"]))+\" observations of Age Level A\")\n",
    "\n",
    "## Remove Observations from Age Level A (less than 20 years old)\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"AGELVL\"] != \"A\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"AGELVL\"] == \"Z\"]))+\" observations with no specified Age Level.\")\n",
    "\n",
    "   ## Remove Observations with no specified Age Level\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"AGELVL\"] != \"Z\"]\n",
    "\n",
    "    ## Fix differences in spaces on WORKSCHT Column\n",
    "OPMDataMerged[\"WORKSCHT\"] = np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"F\", 'Full-time Nonseasonal',\n",
    "                                np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"I\", 'Intermittent Nonseasonal',\n",
    "                                         np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"P\", 'Part-time Nonseasonal',\n",
    "                                                  np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"G\", 'Full-time Seasonal',\n",
    "                                                        np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"J\", 'Intermittent Seasonal',\n",
    "                                                                np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"Q\", 'Part-time Seasonal',\n",
    "                                                                        np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"T\", 'Part-time Job Sharer Seasonal',\n",
    "                                                                                np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"S\", 'Part-time Job Sharer Nonseasonal',\n",
    "                                                                                        np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"B\", 'Full-time Nonseasonal Baylor Plan',\n",
    "                                                                                                'NO WORK SCHEDULE REPORTED' ### ELSE case represents Night\n",
    "                                                                                                 )\n",
    "                                                                                         )\n",
    "                                                                                 )\n",
    "                                                                         )\n",
    "                                                                 )\n",
    "                                                          )\n",
    "                                                 )\n",
    "                                        )\n",
    "                               )    \n",
    "\n",
    "display(OPMDataMerged.head())\n",
    "print(\"New SEP data size of: \"+str(len(OPMDataMerged)))\n",
    "display(OPMDataMerged.describe().transpose())\n",
    "#del OPMDataList,OPMDataFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.isfile(PickleJarPath+\"/EMPDataOrig4Q.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    EMPDataOrig4Q = unpickleObject(\"EMPDataOrig4Q\")\n",
    "else:\n",
    "    ## Load EMPData Files\n",
    "\n",
    "    indexes = []\n",
    "    EMPDataFiles = []\n",
    "    EMPDataList = []\n",
    "    EMPDataOrig = []\n",
    "\n",
    "    for i,qtr in enumerate([\"Q1\", \"Q2\", \"Q3\", \"Q4\"]): \n",
    "        EMPDataFiles.append(glob.glob(os.path.join(dataEMPPath, qtr + \"/*.txt\")))\n",
    "\n",
    "        for j in range(0,len(EMPDataFiles[i])):\n",
    "            EMPDataFiles[i][j] = EMPDataFiles[i][j].replace(\"\\\\\",\"/\")\n",
    "\n",
    "        EMPDataList.append([])\n",
    "\n",
    "        for j,file in enumerate(EMPDataFiles[i]):\n",
    "            EMPDataList[i].append(pd.read_csv(file, dtype = 'str'))\n",
    "            if i == 0:\n",
    "                display(EMPDataList[i][j].head())\n",
    "\n",
    "        ## Load the FactData files into it's own object\n",
    "        indexes.append([])\n",
    "            ##[qtr][fileindex from EMPDataList]\n",
    "        indexes[i]=[j for j,x in enumerate(EMPDataFiles[i]) if dataEMPPath + '/' + qtr + '/FACTDATA' in x]   \n",
    "\n",
    "        EMPDataOrig.append([])\n",
    "\n",
    "        EMPDataOrig[i] = pd.concat([EMPDataList[i][indexes[i][j]] for j in range(0,len(indexes[i]))]) \n",
    "        EMPDataOrig[i][\"QTR\"] = str(i+1)\n",
    "\n",
    "            ## modify data type for numerics\n",
    "        EMPDataOrig[i][\"SALARY\"] = EMPDataOrig[i][\"SALARY\"].str.replace(',', '').str.replace('$', '').str.replace(' ', '').apply(pd.to_numeric)\n",
    "      \n",
    "        ## Load Metadata\n",
    "        ##AGYSUB - AGYTYP, AGY\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTagy.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'AGYSUB', how = 'left')\n",
    "\n",
    "        ##AGELVL - AGELVLT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTagelvl.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'AGELVL', how = 'left')\n",
    "\n",
    "        #LOSLVL - LOSLVLT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTloslvl.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'LOSLVL', how = 'left')\n",
    "        EMPDataOrig[i][\"LOS\"] = EMPDataOrig[i][\"LOS\"].apply(pd.to_numeric)\n",
    "        \n",
    "        ##LOC - LocTypeT, LocT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTloc.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'LOC', how = 'left')\n",
    " \n",
    "        ##OCC - OCCTYPT, OCCFAM\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTocc.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'OCC', how = 'left')\n",
    "\n",
    "        ##PATCO - PATCOT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTpatco.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'PATCO', how = 'left')\n",
    "\n",
    "        ##PPGRD - PayPlan, PPGroup, PPTYP\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTppgrd.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'PPGRD', how = 'left')\n",
    "\n",
    "        ##SALLVL - SALLVLT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTsallvl.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'SALLVL', how = 'left')\n",
    "\n",
    "        ##TOA - TOATYP\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTtoa.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'TOA', how = 'left')\n",
    "\n",
    "        ##WORKSCH - WSTYPT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTwrksch.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'WORKSCH', how = 'left')\n",
    "\n",
    "        display(EMPDataOrig[i].head())\n",
    "\n",
    "    EMPDataOrig4Q = pd.concat([EMPDataOrig[j] for j in range(0,len(EMPDataOrig))])\n",
    "    print(\"Original EMP data size of: \"+str(len(EMPDataOrig4Q)))\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"LOCTYP\"] != \"1\"]))+\" Non-US observations.\")\n",
    "    \n",
    "       ## Remove Non-US Data\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"LOCTYP\"] == \"1\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"OCCTYP\"] == \"3\"]))+\" observations with no specified Occupation.\")\n",
    "\n",
    "       ## Remove Observations with no specified occupation\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"OCCTYP\"] != \"3\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"SALLVL\"] == \"Z\"]))+\" observations with no specified Salary.\")\n",
    "\n",
    "       ## Remove Observations with no specified salary\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"SALLVL\"] != \"Z\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"LOSLVL\"] == \"Z\"]))+\" observations with no specified Length of Service.\")\n",
    "\n",
    "       ## Remove Observations with no specified LOSLVL\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"LOSLVL\"] != \"Z\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] == \"A\"]))+\" observations of Age Level A.\")\n",
    "\n",
    "        ## Remove Observations from Age Level A (less than 20 years old)\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] != \"A\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] == \"Z\"]))+\" observations with no specified Age Level.\")\n",
    "\n",
    "        ## Remove Observations with no specified Age Level\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] != \"Z\"]\n",
    "\n",
    "        ## Fix differences in spaces on WORKSCHT Column\n",
    "    EMPDataOrig4Q[\"WORKSCHT\"] = np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"F\", 'Full-time Nonseasonal',\n",
    "                                    np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"I\", 'Intermittent Nonseasonal',\n",
    "                                             np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"P\", 'Part-time Nonseasonal',\n",
    "                                                      np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"G\", 'Full-time Seasonal',\n",
    "                                                            np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"J\", 'Intermittent Seasonal',\n",
    "                                                                    np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"Q\", 'Part-time Seasonal',\n",
    "                                                                            np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"T\", 'Part-time Job Sharer Seasonal',\n",
    "                                                                                    np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"S\", 'Part-time Job Sharer Nonseasonal',\n",
    "                                                                                            np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"B\", 'Full-time Nonseasonal Baylor Plan',\n",
    "                                                                                                    'NO WORK SCHEDULE REPORTED' ### ELSE case represents Night\n",
    "                                                                                                     )\n",
    "                                                                                             )\n",
    "                                                                                     )\n",
    "                                                                             )\n",
    "                                                                     )\n",
    "                                                              )\n",
    "                                                     )\n",
    "                                            )\n",
    "                                   )    \n",
    "\n",
    "    pickleObject(EMPDataOrig4Q, \"EMPDataOrig4Q\")\n",
    "\n",
    "print(\"New EMP data size of: \"+str(len(EMPDataOrig4Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(EMPDataOrig4Q.describe().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#sns.boxplot(y = \"SALARY\", data = EMPDataOrig4Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both our separation and non-separation data loaded, we calculate three new attributes through aggregation or calculation amongst various attributes. \n",
    "\n",
    "1) SEP Count by Date & Occupation – total number of separations (of any type) for a given Date and Occupation; \n",
    "\n",
    "2) SEP Count by Date & Location – total number of separations (of any type) for a given Date and Location; \n",
    "\n",
    "3) Industry Average Salary – Average salary amongst non-separated employees, grouped by quarter, occupation, pay grade, and work schedule; \n",
    "\n",
    "We proceed, by concatenating our Separation and Non-Separation observations, and merge these newly calculated attributes to the concatenated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "##Aggregate Number of Total Separations in current month for given Occ\n",
    "AggSEPCount_EFDATE_OCC= pd.DataFrame({'SEPCount_EFDATE_OCC' : OPMDataMerged.groupby([\"EFDATE\", \"OCC\"]).size()}).reset_index()\n",
    "display(AggSEPCount_EFDATE_OCC.head())\n",
    "\n",
    "\n",
    "##Aggregate Number of Total Separations in current month for given LOC\n",
    "AggSEPCount_EFDATE_LOC = pd.DataFrame({'SEPCount_EFDATE_LOC' : OPMDataMerged.groupby([\"EFDATE\", \"LOC\"]).size()}).reset_index()\n",
    "display(AggSEPCount_EFDATE_LOC.head())\n",
    "\n",
    "##Average Quarterly EMP Salary by occ \n",
    "AggIndAvgSalary = pd.DataFrame({'count' : EMPDataOrig4Q.groupby([\"QTR\", \"OCC\", \"PPGRD\", \"WORKSCHT\"]).size()}).reset_index()\n",
    "AggIndAvgSalary2 = pd.DataFrame({'IndSalarySum' : EMPDataOrig4Q.groupby([\"QTR\", \"OCC\", \"PPGRD\", \"WORKSCHT\"])[\"SALARY\"].sum()}).reset_index()\n",
    "AggIndAvgSalary = AggIndAvgSalary.merge(AggIndAvgSalary2,on=[\"QTR\", \"OCC\", \"PPGRD\", \"WORKSCHT\"])\n",
    "AggIndAvgSalary[\"IndAvgSalary\"] = AggIndAvgSalary[\"IndSalarySum\"]/AggIndAvgSalary[\"count\"]\n",
    "del AggIndAvgSalary[\"count\"]\n",
    "del AggIndAvgSalary[\"IndSalarySum\"]\n",
    "display(AggIndAvgSalary.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Two Datasets\n",
    "### NS SEP code means NonSeparation\n",
    "###add hardcoded null value columns where applicable\n",
    "EMPDataOrig4Q[\"SEP\"] = \"NS\"\n",
    "EMPDataOrig4Q[\"GENDER\"] = np.nan\n",
    "EMPDataOrig4Q[\"COUNT\"] = np.nan\n",
    "\n",
    "OPMDataMerged[\"DATECODE\"] = OPMDataMerged[\"EFDATE\"]\n",
    "\n",
    "OPMColList = [\"AGYSUB\", \"SEP\", \"DATECODE\",   \"AGELVL\", \"GENDER\", \"GSEGRD\", \"LOSLVL\", \"LOC\", \"OCC\", \"PATCO\", \"PPGRD\", \"SALLVL\", \"TOA\", \"WORKSCH\", \"COUNT\", \"SALARY\", \"LOS\", \"AGYTYP\", \"AGYTYPT\", \"AGY\", \"AGYT\", \"AGYSUBT\", \"QTR\", \"AGELVLT\", \"LOSLVLT\", \"LOCTYP\", \"LOCTYPT\", \"LOCT\", \"OCCTYP\", \"OCCTYPT\", \"OCCFAM\", \"OCCFAMT\", \"OCCT\", \"PATCOT\", \"PPTYP\", \"PPTYPT\", \"PPGROUP\", \"PPGROUPT\", \"PAYPLAN\", \"PAYPLANT\", \"SALLVLT\", \"TOATYP\", \"TOATYPT\", \"TOAT\", \"WSTYP\", \"WSTYPT\", \"WORKSCHT\"]\n",
    "EMPColList = [\"AGYSUB\", \"SEP\", \"DATECODE\", \"AGELVL\", \"GENDER\", \"GSEGRD\", \"LOSLVL\", \"LOC\", \"OCC\", \"PATCO\", \"PPGRD\", \"SALLVL\", \"TOA\", \"WORKSCH\", \"COUNT\", \"SALARY\", \"LOS\", \"AGYTYP\", \"AGYTYPT\", \"AGY\", \"AGYT\", \"AGYSUBT\", \"QTR\", \"AGELVLT\", \"LOSLVLT\", \"LOCTYP\", \"LOCTYPT\", \"LOCT\", \"OCCTYP\", \"OCCTYPT\", \"OCCFAM\", \"OCCFAMT\", \"OCCT\", \"PATCOT\", \"PPTYP\", \"PPTYPT\", \"PPGROUP\", \"PPGROUPT\", \"PAYPLAN\", \"PAYPLANT\", \"SALLVLT\", \"TOATYP\", \"TOATYPT\", \"TOAT\", \"WSTYP\", \"WSTYPT\", \"WORKSCHT\"]\n",
    "\n",
    "OPMDataMerged = pd.concat([OPMDataMerged[OPMColList], EMPDataOrig4Q[EMPColList]], ignore_index=True)\n",
    "print(\"Total concatenated data size for SEP and non-SEP: \"+str(len(OPMDataMerged)))\n",
    "\n",
    "OPMDataMerged = OPMDataMerged.merge(AggSEPCount_EFDATE_OCC, left_on = ['DATECODE','OCC'], right_on = ['EFDATE','OCC'], how = 'left')\n",
    "OPMDataMerged = OPMDataMerged.merge(AggSEPCount_EFDATE_LOC, left_on = ['DATECODE','LOC'], right_on = ['EFDATE','LOC'], how = 'left')\n",
    "OPMDataMerged = OPMDataMerged.merge(AggIndAvgSalary, on = ['QTR','OCC', 'PPGRD', 'WORKSCHT'], how = 'left')\n",
    "OPMDataMerged[\"SalaryOverUnderIndAvg\"] = OPMDataMerged[\"SALARY\"] - OPMDataMerged[\"IndAvgSalary\"]\n",
    "\n",
    "del OPMDataMerged[\"EFDATE_x\"]\n",
    "del OPMDataMerged[\"EFDATE_y\"]\n",
    "\n",
    "display(OPMDataMerged.head())\n",
    "display(OPMDataMerged.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].isnull()]))\n",
    "\n",
    "display(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].isnull()][[\"SEP\",\"DATECODE\", \"OCC\"]].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 50993 Non-Separation observations do not have coverage within the Separation Dataset, thus, we will remove these observations as out of scope demographic in our analysis. Any attempt in predicting these values will not have enough data to support a significant response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].notnull()]\n",
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].isnull()]))\n",
    "\n",
    "print(len(OPMDataMerged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_LOC\"].isnull()]))\n",
    "\n",
    "display(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_LOC\"].isnull()][[\"SEP\",\"DATECODE\",\"LOC\"]].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].isnull()]))\n",
    "\n",
    "display(OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].isnull()][[\"QTR\", \"SEP\",\"OCCT\", \"PPGRD\", \"WORKSCHT\"]].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 1293 separation observations do not have coverage within the EMP Dataset, thus, we will remove these observations as out of scope demographic in our analysis. Any attempt in predicting these values will not have enough data to support a significant response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].notnull()]\n",
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].isnull()]))\n",
    "\n",
    "print(len(OPMDataMerged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************************\n",
    "*************************************\n",
    "\n",
    "# Placeholder Chunks for Data Quality check of salary against GS Grade Level Ranges\n",
    "\n",
    "*************************************\n",
    "*************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder Chunks for Data Quality check of salary against GS Grade Level Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are iterested to see how federal pension plans may impact attrition in this dataset. An interesting attribute to complement Length of service, is Years to Retirement. Utilizing a FERS retirement eligibility baseline of 57 years of age for all observations, and the lower limitation of age level ranges we compute a numeric value for length of retirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Column YearsToRetirement\n",
    "\n",
    "\"\"\"\n",
    "    AGELVL,AGELVLT\n",
    "    A,Less than 20\n",
    "    B,20-24\n",
    "    C,25-29\n",
    "    D,30-34\n",
    "    E,35-39\n",
    "    F,40-44\n",
    "    G,45-49\n",
    "    H,50-54\n",
    "    I,55-59\n",
    "    J,60-64\n",
    "    K,65 or more\n",
    "    Z,Unspecified\n",
    "\"\"\"\n",
    "OPMDataMerged[\"LowerLimitAge\"] = np.where(OPMDataMerged[\"AGELVL\"]==\"B\", 20,\n",
    "                                                np.where(OPMDataMerged[\"AGELVL\"]==\"C\", 25,\n",
    "                                                         np.where(OPMDataMerged[\"AGELVL\"]==\"D\", 30,\n",
    "                                                                  np.where(OPMDataMerged[\"AGELVL\"]==\"E\", 35,\n",
    "                                                                           np.where(OPMDataMerged[\"AGELVL\"]==\"F\", 40,\n",
    "                                                                                    np.where(OPMDataMerged[\"AGELVL\"]==\"G\", 45,\n",
    "                                                                                             np.where(OPMDataMerged[\"AGELVL\"]==\"H\", 50,\n",
    "                                                                                                      np.where(OPMDataMerged[\"AGELVL\"]==\"I\", 55,\n",
    "                                                                                                               np.where(OPMDataMerged[\"AGELVL\"]==\"J\", 60,\n",
    "                                                                                                                        np.where(OPMDataMerged[\"AGELVL\"]==\"K\", 65,\n",
    "                                                                                                                                 np.nan\n",
    "                                                                                                                                )\n",
    "                                                                                                                        )\n",
    "                                                                                                               )\n",
    "                                                                                                      )\n",
    "                                                                                            )\n",
    "                                                                                   )\n",
    "                                                                          )\n",
    "                                                                 )\n",
    "                                                        )\n",
    "                                               )  \n",
    "\n",
    "retAge = 57\n",
    "\n",
    "OPMDataMerged[\"YearsToRetirement\"] = np.where(OPMDataMerged[\"AGELVL\"]==\"B\", retAge-20,\n",
    "                                                np.where(OPMDataMerged[\"AGELVL\"]==\"C\", retAge-25,\n",
    "                                                         np.where(OPMDataMerged[\"AGELVL\"]==\"D\", retAge-30,\n",
    "                                                                  np.where(OPMDataMerged[\"AGELVL\"]==\"E\", retAge-35,\n",
    "                                                                           np.where(OPMDataMerged[\"AGELVL\"]==\"F\", retAge-40,\n",
    "                                                                                    np.where(OPMDataMerged[\"AGELVL\"]==\"G\", retAge-45,\n",
    "                                                                                             np.where(OPMDataMerged[\"AGELVL\"]==\"H\", retAge-50,\n",
    "                                                                                                      np.where(OPMDataMerged[\"AGELVL\"]==\"I\", retAge-55,\n",
    "                                                                                                               np.where(OPMDataMerged[\"AGELVL\"]==\"J\", retAge-60,\n",
    "                                                                                                                        np.where(OPMDataMerged[\"AGELVL\"]==\"K\", retAge-65,\n",
    "                                                                                                                                 np.nan\n",
    "                                                                                                                                )\n",
    "                                                                                                                        )\n",
    "                                                                                                               )\n",
    "                                                                                                      )\n",
    "                                                                                            )\n",
    "                                                                                   )\n",
    "                                                                          )\n",
    "                                                                 )\n",
    "                                                        )\n",
    "                                               )  \n",
    "\n",
    "print(\"Null Values for LowerLimitAge: \" + str(len(OPMDataMerged[OPMDataMerged[\"LowerLimitAge\"].isnull()])))\n",
    "print(\"Null Values for YearsToRetirement: \" + str(len(OPMDataMerged[OPMDataMerged[\"YearsToRetirement\"].isnull()])))\n",
    "\n",
    "display(OPMDataMerged.head())\n",
    "display(OPMDataMerged.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Bureau of Labor Statistics data\n",
    "\n",
    "In addition to the OPM data, we merge 10 attributes from the Bureau of Labor Statistics (BLS). Data is sourced from Federal Government industry codes across all regions. Although assumed to be highly correlated, we source both Level (Total number) and Rate (Percentage of Level to total employment and / or job openings) for the following statistics: 1) Job Openings, 2) Layoffs, 3) Quits, 4) Total Separations, and 5) Other Separations. While Rate paints an aggregated, holistic picture for job market trends, Level provides a raw count for total separations alone. Both these statistics were captured by a monthly aggregate and merged to the OPM data by their respective months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def bls(series, start, end):\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    sID   = []\n",
    "    \n",
    "    for i in range(0,len(series)):\n",
    "        sID.append(series[i][0])\n",
    "    \n",
    "    data = json.dumps({\"seriesid\": sID,\n",
    "                       \"startyear\":start,\n",
    "                       \"endyear\":end,\n",
    "                       \"catalog\":False,\n",
    "                       \"calculations\":False,\n",
    "                       \"annualaverage\":False,\n",
    "                       \"registrationkey\":\"7a89c8d7979349fba8914b8be16a1646\"})\n",
    "    \n",
    "    p = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=data, headers=headers)\n",
    "    json_data = json.loads(p.text)\n",
    "    bls = []\n",
    "    for series in json_data['Results']['series']:\n",
    "        #x=prettytable.PrettyTable([\"series id\",\"year\",\"period\",\"value\",\"footnotes\"])\n",
    "        result = pd.DataFrame(columns=[\"series id\",\"year\",\"period\",\"value\",\"footnotes\"])\n",
    "        seriesId = series['seriesID']\n",
    "        for item in series['data']:\n",
    "            year = item['year']\n",
    "            period = item['period']\n",
    "            value = item['value']\n",
    "            footnotes=\"\"\n",
    "            for footnote in item['footnotes']:\n",
    "                if footnote:\n",
    "                    footnotes = footnotes + footnote['text'] + ','\n",
    "            if 'M01' <= period <= 'M12':\n",
    "                #x.add_row([seriesId,year,period,value,footnotes[0:-1]])\n",
    "                y = pd.DataFrame({\"series id\" : seriesId,\n",
    "                                  \"year\" : year,\n",
    "                                  \"period\" : period,\n",
    "                                  \"value\" : value,\n",
    "                                  \"footnotes\" : footnotes}, index = [0])\n",
    "                result = result.append(y, ignore_index = True)\n",
    "        bls.append(result)\n",
    "    return(bls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seriesList = [\n",
    "              ['JTU91000000JOL','BLS_FEDERAL_JobOpenings_Level'],\n",
    "              ['JTU91000000LDL','BLS_FEDERAL_Layoffs_Level'],\n",
    "              ['JTU91000000OSL','BLS_FEDERAL_OtherSep_Level'],\n",
    "              ['JTU91000000QUL','BLS_FEDERAL_Quits_Level'],\n",
    "              ['JTU91000000TSL','BLS_FEDERAL_TotalSep_Level'],\n",
    "              ['JTU91000000JOR','BLS_FEDERAL_JobOpenings_Rate'],\n",
    "              ['JTU91000000LDR','BLS_FEDERAL_Layoffs_Rate'],\n",
    "              ['JTU91000000OSR','BLS_FEDERAL_OtherSep_Rate'],\n",
    "              ['JTU91000000QUR','BLS_FEDERAL_Quits_Rate'],\n",
    "              ['JTU91000000TSR','BLS_FEDERAL_TotalSep_Rate']\n",
    "             ]\n",
    "\n",
    "# Pull job openings and labor turnover data\n",
    "JTL = bls(seriesList, \"2014\", \"2015\")\n",
    "\n",
    "seriesList = pd.DataFrame(seriesList, columns = [\"series id\",\"sName\"])\n",
    "\n",
    "##We need to replace these with actual Descriptor Column Names\n",
    "\n",
    "for i in range(0,len(seriesList)):\n",
    "    \n",
    "    JTL[i] = JTL[i].merge(seriesList, on = \"series id\", how = 'inner')\n",
    "\n",
    "    if len(JTL[i]) >0:\n",
    "        name = JTL[i][\"sName\"].drop_duplicates().values[0]\n",
    "    else:\n",
    "        name = str(i)\n",
    "\n",
    "    JTL[i][name] = JTL[i][\"value\"].apply(pd.to_numeric)\n",
    "    JTL[i][\"DATECODE\"] = JTL[i][\"year\"] + JTL[i][\"period\"].str[-2:]\n",
    "    del JTL[i][\"value\"]\n",
    "    del JTL[i][\"year\"]\n",
    "    del JTL[i][\"period\"]\n",
    "    del JTL[i][\"series id\"]\n",
    "    del JTL[i][\"footnotes\"]\n",
    "    del JTL[i][\"sName\"]\n",
    "    \n",
    "    \n",
    "    OPMDataMerged = OPMDataMerged.merge(JTL[i], on = \"DATECODE\", how = 'left')\n",
    "    display(JTL[i].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(OPMDataMerged.head())\n",
    "display(OPMDataMerged.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame({'StratCount' : OPMDataMerged.groupby([\"SEP\"]).size()}).reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several separation types we would like to either roll up, or remove altogether.\n",
    "\n",
    "**Roll-Up**\n",
    "\n",
    "We have chosen to roll up all retirement separation together. Separation categories of 1) SD,Retirement - Voluntary; 2)  SE,Retirement - Early Out; 3) SF,Retirement - Disability; 4) SG,Retirement - Other are consolidated into one category \"SD\".\n",
    "\n",
    "**Removal**\n",
    "\n",
    "We have chosen to remove the following. 1) SB,Transfer Out - Mass Transfer; 2) SK,Death; 3) SL,Other Separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OPMDataMerged = OPMDataMerged[(OPMDataMerged[\"SEP\"] != \"SB\") & (OPMDataMerged[\"SEP\"] != \"SK\") & (OPMDataMerged[\"SEP\"] != \"SL\")]\n",
    "\n",
    "OPMDataMerged.loc[(OPMDataMerged[\"SEP\"] == \"SD\") | (OPMDataMerged[\"SEP\"] == \"SE\") | (OPMDataMerged[\"SEP\"] == \"SF\") | (OPMDataMerged[\"SEP\"] == \"SG\"), \"SEP\"]=\"SD\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratum = pd.DataFrame({'StratCount' : OPMDataMerged.groupby([\"SEP\"]).size()}).reset_index()\n",
    "\n",
    "stratum.loc[stratum[\"StratCount\"]>50000,\"StratCountSample\"] = 50000\n",
    "stratum.loc[stratum[\"StratCount\"]<=50000,\"StratCountSample\"] = stratum[\"StratCount\"]\n",
    "#else: stratum[\"StratCountSample\"] = stratum[\"StratCount\"]\n",
    "\n",
    "display(stratum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MaxSampleSize = 50000\n",
    "\n",
    "AggStrat = []\n",
    "\n",
    "for i in range(0,len(stratum)):\n",
    "    sep = stratum[\"SEP\"].ix[i]\n",
    "    StratCountSample = stratum[\"StratCountSample\"].ix[i]\n",
    "    print(\"Stratum Sample Size Calculations for SEP: {}\".format(sep))   \n",
    "    AggStrat.append(pd.DataFrame({'StratCount' : OPMDataMerged[OPMDataMerged[\"SEP\"]==sep].groupby([\"DATECODE\", \"AGELVL\"]).size()}).reset_index())\n",
    "    AggStrat[i][\"SEP\"] = sep\n",
    "    AggStrat[i][\"TotalCount\"] = len(OPMDataMerged[OPMDataMerged[\"SEP\"]==sep])\n",
    "    AggStrat[i][\"p\"] = AggStrat[i][\"StratCount\"] / AggStrat[i][\"TotalCount\"]\n",
    "    AggStrat[i][\"StratCountSample\"] = StratCountSample\n",
    "    AggStrat[i][\"StratSampleSize\"] = round(AggStrat[i][\"p\"] * StratCountSample).apply(int)\n",
    "    \n",
    "    display(AggStrat[i].head())\n",
    "    print(\"totalStratumSampleSize: \", AggStrat[i][\"StratSampleSize\"].sum())\n",
    "    #print(len(AggStrat[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a seed value equal to each strata sample size, we take random samples according to the computed sizes above. We loop through each Separation Type's Aggregated Strata Sample Sizes; Identify all observations matching on Datecode, Separation Type, and AgeLevel; and finally sample those observations with the computed sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(PickleJarPath+\"/SampledOPMData.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    SampledOPMData = unpickleObject(\"SampledOPMData\")\n",
    "else:\n",
    "    SampledOPMStratumDataList = []\n",
    "\n",
    "    for i,StratSampleSize in enumerate(AggStrat):\n",
    "        SampledOPMStratumData = []\n",
    "        for j in range(0,len(StratSampleSize)):\n",
    "            SEP = StratSampleSize[\"SEP\"].ix[j]\n",
    "            DATECODE = StratSampleSize[\"DATECODE\"].ix[j]\n",
    "            AGELVL = StratSampleSize[\"AGELVL\"].ix[j]\n",
    "            SampleSize = StratSampleSize[\"StratSampleSize\"].ix[j]\n",
    "            print(SEP, DATECODE, AGELVL, SampleSize)\n",
    "            \n",
    "            SampledOPMStratumDataList.append(OPMDataMerged[(OPMDataMerged[\"SEP\"]==SEP) \n",
    "                                                    & (OPMDataMerged[\"DATECODE\"]==DATECODE) \n",
    "                                                    & (OPMDataMerged[\"AGELVL\"]==AGELVL)].sample(SampleSize,  random_state=SampleSize))\n",
    "        SampledOPMStratumData.append(pd.concat(SampledOPMStratumDataList))\n",
    "        clear_display()\n",
    "    SampledOPMData = pd.concat(SampledOPMStratumData).reset_index()\n",
    "    del SampledOPMData[\"index\"]\n",
    "    pickleObject(SampledOPMData, \"SampledOPMData\")\n",
    "    clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(SampledOPMData))\n",
    "display(SampledOPMData.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "display(pd.DataFrame({'StratCount' : SampledOPMData.groupby([\"SEP\"]).size()}).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#### Analyze Missing Values\n",
    "filtered_msnoData = msno.nullity_sort(msno.nullity_filter(SampledOPMData, filter='bottom', n=15, p=0.999), sort='descending')\n",
    "msno.matrix(filtered_msnoData)\n",
    "\n",
    "del filtered_msnoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "display(SampledOPMData.describe().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "#OPMDataMerged.to_csv(\"OPMDataMerged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os.path.getsize(\"OPMDataMerged.csv\") #Display file size in bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary EDA\n",
    "\n",
    "In terms of data exploration, we first investigate numeric type attributes. Relationships, distributions, and correlation values are reviewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "cols = list(SampledOPMData.select_dtypes(include=['float64', 'int64']))\n",
    "cols.remove('COUNT')\n",
    "cols.append('SEP')\n",
    "\n",
    "plotNumeric = SampledOPMData[cols].dropna()\n",
    "display(plotNumeric.head())\n",
    "print(\"plotNumeric Has {0} Records\".format(len(plotNumeric)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.set()\n",
    "sns.pairplot(plotNumeric, hue = 'SEP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function modified from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n",
    "sns.set()\n",
    "\n",
    "def draw_histograms(df, variables, n_rows, n_cols):\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        df[var_name].hist(bins=20,ax=ax, color='#58D68D')\n",
    "        ax.set_title(var_name+\" Distribution\")\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "draw_histograms(plotNumeric.drop('SEP', axis=1), plotNumeric.drop('SEP', axis=1).columns, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Inspired by http://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "#plt.matshow(plotNumeric.corr())\n",
    "\n",
    "sns.set(style='white')\n",
    "corr = plotNumeric.drop(['SEP'], axis=1).corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.set(font_scale=1.3)\n",
    "heatCorr = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "                       square=True, annot=True, linewidths=1,\n",
    "                       cbar_kws={\"shrink\": .5}, ax=ax, fmt='.1g')\n",
    "#heatCorr.\n",
    "ax.tick_params(labelsize=15)\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.tick_params(labelsize=15)\n",
    "\n",
    "sns.plt.show()\n",
    "#sns.heatmap(corr, annot=True, linewidths=0.01, cmap=cmap, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the distribution of attributes identified above, we have decided to take the log transform of several attributes. \n",
    "- Salary\n",
    "- LOS (augmented by a value of .00001 to adjust for the undefined result of log(0)\n",
    "- SEPCount_EFDATE_OCC\n",
    "- SEPCount_EFDATE_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Log Transform Columns Added\n",
    "SampledOPMData[\"SALARYLog\"] = SampledOPMData[\"SALARY\"].apply(np.log)\n",
    "SampledOPMData[\"LOSLog\"] = (SampledOPMData[\"LOS\"] + .00001).apply(np.log)\n",
    "SampledOPMData[\"SEPCount_EFDATE_OCCLog\"] = SampledOPMData[\"SEPCount_EFDATE_OCC\"].apply(np.log)\n",
    "SampledOPMData[\"SEPCount_EFDATE_LOCLog\"] = SampledOPMData[\"SEPCount_EFDATE_LOC\"].apply(np.log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We next review categorical data to improve our understanding of factor levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(SampledOPMData.select_dtypes(include=['object']))\n",
    "dropCols = [\"LOCTYP\",\n",
    "            \"LOCTYPT\",\n",
    "            \"OCCTYP\",\n",
    "            \"OCCTYPT\",\n",
    "            \"PPTYP\",\n",
    "            \"PPTYPT\",\n",
    "            \"AGYTYP\",\n",
    "            \"OCCFAM\",\n",
    "            \"PPGROUP\",\n",
    "            \"PAYPLAN\",\n",
    "            \"TOATYP\",\n",
    "            \"WSTYP\"]\n",
    "\n",
    "for i in dropCols:\n",
    "    cols.remove(i)\n",
    "\n",
    "plotCat = SampledOPMData[cols].dropna()\n",
    "display(plotCat.head())\n",
    "print(\"plotCat Has {0} Records\".format(len(plotCat)))\n",
    "print(\"Number of colums = \", len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = cols[:2] # for quick visualization debug only; may delete once complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGYSUB\n",
    "High seperation among following:\n",
    "* Veterans Health Administration (VATA)\n",
    "* Forest Service (AG11)\n",
    "\n",
    "### AGELVL\n",
    "High termination due to expired appt/other among following:\n",
    "* B\n",
    "* C\n",
    "\n",
    "Number of Quits peaks at AGELVL D\n",
    "\n",
    "Individual transfer counts mostly trend with Quits\n",
    "\n",
    "Retirement highest at following:\n",
    "* I\n",
    "* J\n",
    "* K\n",
    "\n",
    "### GENDER\n",
    "Similar separation distributions among males and females, except more terminations due to contract expiration among males\n",
    "\n",
    "### GSEGRD\n",
    "High termination due to expired appt/other among following:\n",
    "* 3\n",
    "* 4\n",
    "* 5\n",
    "\n",
    "Bimodal Quit distribution with outlier spike at GSEGRD 9:\n",
    "* Distribution 1 from GSEGRD 3 to 8\n",
    "* Distribution 2 from GSEGRD 11 to 15\n",
    "\n",
    "Individual transfers highest among levels 11, 12, 13\n",
    "\n",
    "### LOSLVL\n",
    "Highest Quit count for LOSLVL A (< 1 year service) which then declines for levels B and C before spiking again at level D (5-9 years service)\n",
    "\n",
    "Same pattern is observed for contract terminations but without any significant spikes with longer service\n",
    "\n",
    "Large individual transfer spike at LOSLVL D (5-9 years service)\n",
    "\n",
    "Retirement starts at LOSLVL D but trends upward to J\n",
    "\n",
    "### STILL ONGOING...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subCountPlot(att1, att2, thresh):\n",
    "    counts = plotCat.groupby([att1, att2]).size().unstack(fill_value=0) # Get att1 sizes by att2\n",
    "    counts = pd.concat([counts,counts.sum(axis=1)], axis=1) # Calculate total for each att1 value and append total as new column\n",
    "    counts.rename(columns={0:\"Total\"}, inplace=True)\n",
    "    top = counts[counts[\"Total\"] > thresh].index.tolist() # Obtain att1 values where total surpasses threshold\n",
    "    \n",
    "    zoom = plotCat[plotCat[att1].isin(top)] # Subset data to only the top att1 values\n",
    "    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 10), sharey=False)\n",
    "    sns.countplot(y=att1, data=zoom, color=\"blue\", ax=ax1); # Dark blue signifies zoomed data\n",
    "    sns.countplot(y=att1, data=zoom, hue=att2, palette=\"hls\", ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in cols:\n",
    "    if i != 'SEP':\n",
    "        plt.figure(i) # Required to create new figure each loop rather than drawing over previous object\n",
    "        f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 10), sharey=False)\n",
    "        sns.countplot(y=i, data=plotCat, color=\"lightblue\", ax=ax1);\n",
    "        sns.countplot(y=i, data=plotCat, hue=\"SEP\", palette=\"hls\", ax=ax2);\n",
    "        \n",
    "    if i == 'AGYSUB':\n",
    "        subCountPlot(i, 'SEP', 10000)\n",
    "    elif i == 'LOC':\n",
    "        subCountPlot(i, 'SEP', 4000)\n",
    "    elif i == 'OCC':\n",
    "        subCountPlot(i, 'SEP', 2000)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotCat.AGYSUB.value_counts()\n",
    "\n",
    "#counts = plotCat.groupby(['AGYSUB', 'SEP']).size().unstack(fill_value=0)\n",
    "#counts = pd.concat([counts,counts.sum(axis=1)], axis=1)\n",
    "#counts.rename(columns={0:\"Total\"}, inplace=True)\n",
    "##counts.sort_values(by=\"Total\", ascending=False)\n",
    "#top = counts[counts[\"Total\"] > 10000].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##zoom = plotCat.query('AGYSUB == \"VATA\" or AGYSUB == \"AG11\"')\n",
    "#zoom = plotCat[plotCat['AGYSUB'].isin(top)]\n",
    "##sns.countplot(y='AGYSUB', data=zoom, hue=\"SEP\", palette=\"hls\");\n",
    "#f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 10), sharey=False)\n",
    "#sns.countplot(y='AGYSUB', data=zoom, color=\"blue\", ax=ax1);\n",
    "#sns.countplot(y='AGYSUB', data=zoom, hue=\"SEP\", palette=\"hls\", ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(plotNumeric, plotCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Attributes, and Remove Description Columns for Analysis Prep\n",
    "\n",
    "Now that we have the dataset sampled, we still have some legwork necessary to convert our categorical attributes into binary integer values. Below we walk through this process for the following Attributes:\n",
    "- AGYSUB\n",
    "- AGELVL\n",
    "- LOSLVL\n",
    "- LOC\n",
    "- OCC\n",
    "- PATCO\n",
    "- PPGRD\n",
    "- SALLVL\n",
    "- TOA\n",
    "- WORKSCH\n",
    "- AGYTYP\n",
    "- AGY\n",
    "- LOCTYP\n",
    "- OCCTYP\n",
    "- OCCFAM\n",
    "- PPTYP\n",
    "- PPGROUP\n",
    "- PAYPLAN\n",
    "- TOATYP\n",
    "- WSTYP\n",
    "\n",
    "Once these attributes have been encoded and description columns removed, we end up with a total of 2446 attributes in our dataset for analysis in our model generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up old objects no longer needed, to clear up memory\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Memory Usage before Cleanup: \", process.memory_info().rss)\n",
    "\n",
    "if 'AGELVL' in dir():\n",
    "    del AGELVL\n",
    "if 'AggIndAvgSalary' in dir():\n",
    "    del AggIndAvgSalary\n",
    "if 'AggIndAvgSalary2' in dir():\n",
    "    del AggIndAvgSalary2\n",
    "if 'AggSEPCount_EFDATE_LOC' in dir():\n",
    "    del AggSEPCount_EFDATE_LOC\n",
    "if 'AggSEPCount_EFDATE_OCC' in dir():\n",
    "    del AggSEPCount_EFDATE_OCC\n",
    "if 'AggStrat' in dir():\n",
    "    del AggStrat\n",
    "if 'DATECODE' in dir():\n",
    "    del DATECODE\n",
    "if 'EMPColList' in dir():\n",
    "    del EMPColList\n",
    "if 'EMPDataOrig4Q' in dir():\n",
    "    del EMPDataOrig4Q\n",
    "if 'MaxSampleSize' in dir():\n",
    "    del MaxSampleSize\n",
    "if 'OPMColList' in dir():\n",
    "    del OPMColList\n",
    "if 'OPMDataFiles' in dir():\n",
    "    del OPMDataFiles\n",
    "if 'OPMDataList' in dir():\n",
    "    del OPMDataList\n",
    "if 'OPMDataMerged' in dir():\n",
    "    del OPMDataMerged\n",
    "if 'OPMDataOrig' in dir():\n",
    "    del OPMDataOrig\n",
    "if 'SEP' in dir():\n",
    "    del SEP\n",
    "if 'SampleSize' in dir():\n",
    "    del SampleSize\n",
    "if 'SampledOPMStratumData' in dir():\n",
    "    del SampledOPMStratumData\n",
    "if 'SampledOPMStratumDataList' in dir():\n",
    "    del SampledOPMStratumDataList\n",
    "if 'StratCountSample' in dir():\n",
    "    del StratCountSample\n",
    "if 'StratSampleSize' in dir():\n",
    "    del StratSampleSize\n",
    "if 'JTL' in dir():\n",
    "    del JTL\n",
    "    \n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Memory Usage after Cleanup: \", process.memory_info().rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(SampledOPMData.head())\n",
    "SampledOPMData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.isfile(PickleJarPath+\"/OPMAnalysisData.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    OPMAnalysisData = unpickleObject(\"OPMAnalysisData\")\n",
    "else:\n",
    "\n",
    "    OPMAnalysisData = SampledOPMData.copy()\n",
    "\n",
    "\n",
    "    cols = [\"GENDER\",\n",
    "            \"DATECODE\",\n",
    "            \"GSEGRD\",\n",
    "            \"COUNT\",\n",
    "            \"AGYTYPT\",\n",
    "            \"AGYT\",\n",
    "            \"AGYSUBT\",\n",
    "            \"QTR\",\n",
    "            \"AGELVLT\",\n",
    "            \"LOSLVLT\",\n",
    "            \"LOCTYPT\",\n",
    "            \"LOCT\",\n",
    "            \"OCCTYPT\",\n",
    "            \"OCCFAMT\",\n",
    "            \"OCCT\",\n",
    "            \"PATCOT\",\n",
    "            \"PPTYPT\",\n",
    "            \"PPGROUPT\",\n",
    "            \"PAYPLANT\",\n",
    "            \"SALLVLT\",\n",
    "            \"TOATYPT\",\n",
    "            \"TOAT\",\n",
    "            \"WSTYPT\",\n",
    "            \"WORKSCHT\",\n",
    "            \"SALARY\",\n",
    "            \"LOS\",\n",
    "            \"SEPCount_EFDATE_OCC\",\n",
    "            \"SEPCount_EFDATE_LOC\"\n",
    "           ]\n",
    "\n",
    "\n",
    "\n",
    "    #delete cols from analysis data\n",
    "    for col in cols:\n",
    "        del OPMAnalysisData[col]\n",
    "\n",
    "    OPMAnalysisData.info()\n",
    "\n",
    "    cols = [\"AGYSUB\",\n",
    "            \"AGELVL\",\n",
    "            \"LOSLVL\",\n",
    "            \"LOC\",\n",
    "            \"OCC\",\n",
    "            \"PATCO\",\n",
    "            \"PPGRD\",\n",
    "            \"SALLVL\",\n",
    "            \"TOA\",\n",
    "            \"WORKSCH\",\n",
    "            \"AGYTYP\",\n",
    "            \"AGY\",\n",
    "            \"LOCTYP\",\n",
    "            \"OCCTYP\",\n",
    "            \"OCCFAM\",\n",
    "            \"PPTYP\",\n",
    "            \"PPGROUP\",\n",
    "            \"PAYPLAN\",\n",
    "            \"TOATYP\",\n",
    "            \"WSTYP\"\n",
    "           ]\n",
    "\n",
    "    #Split Values for cols \n",
    "    for col in cols:\n",
    "        AttSplit = pd.get_dummies(OPMAnalysisData[col],prefix=col)\n",
    "        display(AttSplit.head())\n",
    "        OPMAnalysisData = pd.concat((OPMAnalysisData,AttSplit),axis=1) # add back into the dataframe\n",
    "        del OPMAnalysisData[col]\n",
    "\n",
    "    pickleObject(OPMAnalysisData, \"OPMAnalysisData\")\n",
    "        \n",
    "display(OPMAnalysisData.head())\n",
    "print(\"Number of Columns: \",len(OPMAnalysisData.columns))\n",
    "OPMAnalysisData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a display of all remaining attributes and their corresponding data types for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_type = []\n",
    "for idx, col in enumerate(OPMAnalysisData.columns):\n",
    "    data_type.append(OPMAnalysisData.dtypes[idx])\n",
    "\n",
    "summary_df = {'Attribute Name' : pd.Series(OPMAnalysisData.columns, index = range(len(OPMAnalysisData.columns))), 'Data Type' : pd.Series(data_type, index = range(len(OPMAnalysisData.columns)))}\n",
    "summary_df = pd.DataFrame(summary_df)\n",
    "display(summary_df)\n",
    "\n",
    "del data_type, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction using Principal Component Analysis\n",
    "\n",
    "We also scale the data values to remove bias in our models due to different attribute scales. Without scaling the data, attributes such as SALARY and LOS would carry heavier weights when compared against the binary encoded attributes and BLS data. This would cause unbalanced and improperly analyzed data for model creation. \n",
    "\n",
    "#### need to fix missing values in sepcount attributes above.... then remove the .dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "OPMScaledAnalysisData = OPMAnalysisData.copy().dropna()\n",
    "del OPMScaledAnalysisData[\"SEP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "OPMAnalysisScalerFit = MinMaxScaler().fit(OPMScaledAnalysisData)\n",
    "## Pickle for later re-use if needed\n",
    "pickleObject(OPMAnalysisScalerFit, \"OPMAnalysisScalerFit\")\n",
    "\n",
    "OPMScaledAnalysisData = pd.DataFrame(OPMAnalysisScalerFit.transform(OPMScaledAnalysisData), columns = OPMScaledAnalysisData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(OPMScaledAnalysisData.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Principal Components defined\n",
    "\n",
    "Our objective, is to reduce dimensionality through identification of principal components. We have chosen 100 as the maximum number of components to be produced, given our hopes are to drastically reduce the number of attributes needed for a model. We will review each component's explained variance further to determine the proper number of components to be included later during model generation. Note randomized PCA was chosen in order to use singular value decomposition in our dimensionality reduction efforts due to the large size of our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seed = len(OPMScaledAnalysisData)\n",
    "\n",
    "print(OPMScaledAnalysisData.shape)\n",
    "pca_class = PCA(n_components=100, svd_solver='randomized', random_state=seed)\n",
    "\n",
    "pca_class.fit(OPMScaledAnalysisData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the resulting components have been ordered by eigenvector value and these values portrayed as ratios of variance explained by each component. In order to identify the principal components to be included during model generation, we review the rate at which explained variance decreases in significance from one principal component to the next. Accompanying these proportion values is a scree plot representing these same values in visual form. By plotting the scree plot, it is easier to judge where this rate of decreasing explained variance occurs. Note the rate of change in explained variance among the first 11 principal components, with another less significant change through the 20th component. After the 20th component, the rate of decreasing explained variance begins to somewhat flatten out, reducing to a < 0.03% change or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#The amount of variance that each PC explains\n",
    "var= pca_class.explained_variance_ratio_\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "plt.plot(range(1,101), var*100, marker = '.', color = 'red', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.axis([0, 101, -0.1, 9])\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(var, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now referring to the cumulative variance values and associated plot below, it may be seen that the cumulative variance increases in a fairly consistent parabola curve. In attempts to acheive a cumulative variance explained of greater than 50%, we end at 23 principal components. For this reason, 23 principal components may be selected as being the most appropriate for separation classification modeling given the variables among these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca_class.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "plt.plot(range(1,101), var1, marker = '.', color = 'green', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance (Sum %)')\n",
    "plt.title('Cumulative Variance Plot')\n",
    "plt.axis([0, 101, 10, 101])\n",
    "\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to analyze the first 4 component Feature Loadings more carefully. See below, plots of the top 10 loadings for each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 12)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,4):\n",
    "    components = pd.Series(pca_class.components_[i], index=OPMScaledAnalysisData.columns)\n",
    "\n",
    "    maxcomponent = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(10)).index)\n",
    "\n",
    "    matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(2,2,i + 1)\n",
    "       \n",
    "    weightsplot = pd.Series(components, index=maxcomponent)\n",
    "    weightsplot.plot(title = \"Principal Component \"+ str(i+1), kind='bar', color = 'Tomato', ax = ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MaxPC = 23\n",
    "\n",
    "PCList = []\n",
    "for i in range(0,MaxPC):\n",
    "    components = pd.Series(pca_class.components_[i], index=OPMScaledAnalysisData.columns)\n",
    "\n",
    "    maxcomponent = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(10)).index)\n",
    "    PCList.append(maxcomponent)\n",
    "\n",
    "PCList = pd.concat(PCList).drop_duplicates().sort_values(ascending=True).reset_index(drop = True)\n",
    "print(PCList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total of 83 features of the original 2441 are identified, by taking the top 10 feature loadings within the first 23 components as determined above as the appropriate components to maximize variance explained. We may now, optionally utilize these 83 features identified, or utilize principal component vectors for analysis in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
