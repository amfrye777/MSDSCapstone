{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPM Data Separation Analysis\n",
    "<i><b>\n",
    "Christopher Boomhower<sub>1</sub>, Stacey Fabricant<sub>2</sub>, Alex Frye<sub>1</sub>, David Mumford<sub>2</sub>, Michael Smith<sub>1</sub>, Lindsay Vitovsky<sub>1</sub>\n",
    "\n",
    "<sub>1</sub> Southern Methodis Univeristy, Dallas, TX, US\n",
    "<sub>2</sub> Penn Mutual Life Insurance Co, Horsham PA\n",
    "</i></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "background text...\n",
    "\n",
    "**our intent is to: 1)..2)...3)........**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "Data Source Background Text & citation links\n",
    "\n",
    "Dataset Attribute Descriptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "To begin our analysis, we need to load the data from our 89 source .txt files. Data is separated into two separate groups of files; Separation and Non-Separation, thus data is loaded in two separate phases, then unioned together. Once data is loaded, Steps taken to remove non-US observations or those with no specified occupation, no specified salary, or no specified length of service level.  Of a total 8,423,336 observations, we end with 8,232,375 after removal of these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import pickle\n",
    "import os\n",
    "import psutil\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import missingno as msno\n",
    "import prettytable\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "## Library Options\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pre-defined Functions for use later\n",
    "def pickleObject(objectname, filename, filepath = \"PickleJar/\"):\n",
    "    fullpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create a variable to pickle and open it in write mode\n",
    "    picklefile = open(fullpicklepath, 'wb')\n",
    "    pickle.dump(objectname, picklefile)\n",
    "    picklefile.close()\n",
    "    \n",
    "def unpickleObject(filename, filepath = \"PickleJar/\"):\n",
    "    fullunpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create an variable to pickle and open it in write mode\n",
    "    unpicklefile = open(fullunpicklepath, 'rb')\n",
    "    unpickleObject = pickle.load(unpicklefile)\n",
    "    unpicklefile.close()\n",
    "\n",
    "    return unpickleObject\n",
    "    \n",
    "def clear_display():\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "## Pre-defined variables for use later\n",
    "dataOPMPath = \"dataOPM\"\n",
    "dataEMPPath = \"dataEMP\"\n",
    "PickleJarPath = \"PickleJar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Load OPMSeparation Files\n",
    "\n",
    "OPMDataFiles = glob.glob(os.path.join(dataOPMPath, \"*.txt\"))\n",
    "\n",
    "for i in range(0,len(OPMDataFiles)):\n",
    "    OPMDataFiles[i] = OPMDataFiles[i].replace(\"\\\\\",\"/\")\n",
    "\n",
    "OPMDataList = []\n",
    "\n",
    "for i,j in zip(OPMDataFiles,range(0,len(OPMDataFiles))):\n",
    "    OPMDataList.append(pd.read_csv(i, dtype = 'str'))\n",
    "    display(OPMDataList[j].head())\n",
    "\n",
    "## Load the SEPDATA_FY2015 file into it's own object\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/SEPDATA_FY2015.txt']\n",
    "OPMDataOrig = OPMDataList[indexes[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#print(OPMDataFiles)\n",
    "\n",
    "print(len(OPMDataOrig))\n",
    "\n",
    "##### Merge / Modify Codes / Aggregate Attributes to be more descriptive per the metadata files\n",
    "\n",
    "OPMDataMerged = OPMDataOrig.copy()\n",
    "\n",
    "##AGYSUB - AGYTYP, AGY\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTagy.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'AGYSUB', how = 'left')\n",
    "\n",
    "##EFDate - quarter, month\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTefdate.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'EFDATE', how = 'left')\n",
    "\n",
    "##AGELVL - AGELVLT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTagelvl.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'AGELVL', how = 'left')\n",
    "\n",
    "##LOSLVL - LOSLVLT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTloslvl.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'LOSLVL', how = 'left')\n",
    "\n",
    "##LOC - LocTypeT, LocT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTloc.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'LOC', how = 'left')\n",
    "\n",
    "##OCC - OCCTYPT, OCCFAM\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTocc.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'OCC', how = 'left')\n",
    "\n",
    "##PATCO - PATCOT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTpatco.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'PATCO', how = 'left')\n",
    "\n",
    "##PPGRD - PayPlan, PPGroup, PPTYP\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTppgrd.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'PPGRD', how = 'left')\n",
    "\n",
    "##SALLVL - SALLVLT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTsallvl.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'SALLVL', how = 'left')\n",
    "\n",
    "##TOA - TOATYP\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTtoa.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'TOA', how = 'left')\n",
    "\n",
    "##WORKSCH - WSTYPT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTwrksch.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'WORKSCH', how = 'left')\n",
    "\n",
    "\n",
    "## Modify Data Types for numeric objects\n",
    "OPMDataMerged[\"SALARY\"] = OPMDataMerged[\"SALARY\"].apply(pd.to_numeric)\n",
    "OPMDataMerged[\"COUNT\"]  = OPMDataMerged[\"COUNT\"].apply(pd.to_numeric)\n",
    "OPMDataMerged[\"LOS\"]    = OPMDataMerged[\"LOS\"].apply(pd.to_numeric)\n",
    "\n",
    "print(\"Original SEP data size of: \"+str(len(OPMDataMerged)))\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"LOCTYP\"] != \"1\"]))+\" Non-US observations.\")\n",
    "\n",
    "    ## Remove Non-US Data\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"LOCTYP\"] == \"1\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"OCCTYP\"] == \"3\"]))+\" observations with no specified Occupation.\")\n",
    "\n",
    "   ## Remove Observations with no specified occupation\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"OCCTYP\"] != \"3\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"SALLVL\"] == \"Z\"]))+\" observations with no specified Salary.\")\n",
    "\n",
    "   ## Remove Observations with no specified salary\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"SALLVL\"] != \"Z\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"LOSLVL\"] == \"Z\"]))+\" observations with no specified Length of Service.\")\n",
    "\n",
    "   ## Remove Observations with no specified LOSLVL\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"LOSLVL\"] != \"Z\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"AGELVL\"] == \"A\"]))+\" observations of Age Level A\")\n",
    "\n",
    "## Remove Observations from Age Level A (less than 20 years old)\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"AGELVL\"] != \"A\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"AGELVL\"] == \"Z\"]))+\" observations with no specified Age Level.\")\n",
    "\n",
    "   ## Remove Observations with no specified Age Level\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"AGELVL\"] != \"Z\"]\n",
    "\n",
    "    ## Fix differences in spaces on WORKSCHT Column\n",
    "OPMDataMerged[\"WORKSCHT\"] = np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"F\", 'Full-time Nonseasonal',\n",
    "                                np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"I\", 'Intermittent Nonseasonal',\n",
    "                                         np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"P\", 'Part-time Nonseasonal',\n",
    "                                                  np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"G\", 'Full-time Seasonal',\n",
    "                                                        np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"J\", 'Intermittent Seasonal',\n",
    "                                                                np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"Q\", 'Part-time Seasonal',\n",
    "                                                                        np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"T\", 'Part-time Job Sharer Seasonal',\n",
    "                                                                                np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"S\", 'Part-time Job Sharer Nonseasonal',\n",
    "                                                                                        np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"B\", 'Full-time Nonseasonal Baylor Plan',\n",
    "                                                                                                'NO WORK SCHEDULE REPORTED' ### ELSE case represents Night\n",
    "                                                                                                 )\n",
    "                                                                                         )\n",
    "                                                                                 )\n",
    "                                                                         )\n",
    "                                                                 )\n",
    "                                                          )\n",
    "                                                 )\n",
    "                                        )\n",
    "                               )    \n",
    "\n",
    "display(OPMDataMerged.head())\n",
    "print(\"New SEP data size of: \"+str(len(OPMDataMerged)))\n",
    "display(OPMDataMerged.describe().transpose())\n",
    "#del OPMDataList,OPMDataFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.isfile(PickleJarPath+\"/EMPDataOrig4Q.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    EMPDataOrig4Q = unpickleObject(\"EMPDataOrig4Q\")\n",
    "else:\n",
    "    ## Load EMPData Files\n",
    "\n",
    "    indexes = []\n",
    "    EMPDataFiles = []\n",
    "    EMPDataList = []\n",
    "    EMPDataOrig = []\n",
    "\n",
    "    for i,qtr in enumerate([\"Q1\", \"Q2\", \"Q3\", \"Q4\"]): \n",
    "        EMPDataFiles.append(glob.glob(os.path.join(dataEMPPath, qtr + \"/*.txt\")))\n",
    "\n",
    "        for j in range(0,len(EMPDataFiles[i])):\n",
    "            EMPDataFiles[i][j] = EMPDataFiles[i][j].replace(\"\\\\\",\"/\")\n",
    "\n",
    "        EMPDataList.append([])\n",
    "\n",
    "        for j,file in enumerate(EMPDataFiles[i]):\n",
    "            EMPDataList[i].append(pd.read_csv(file, dtype = 'str'))\n",
    "            if i == 0:\n",
    "                display(EMPDataList[i][j].head())\n",
    "\n",
    "        ## Load the FactData files into it's own object\n",
    "        indexes.append([])\n",
    "            ##[qtr][fileindex from EMPDataList]\n",
    "        indexes[i]=[j for j,x in enumerate(EMPDataFiles[i]) if dataEMPPath + '/' + qtr + '/FACTDATA' in x]   \n",
    "\n",
    "        EMPDataOrig.append([])\n",
    "\n",
    "        EMPDataOrig[i] = pd.concat([EMPDataList[i][indexes[i][j]] for j in range(0,len(indexes[i]))]) \n",
    "        EMPDataOrig[i][\"QTR\"] = str(i+1)\n",
    "\n",
    "            ## modify data type for numerics\n",
    "        EMPDataOrig[i][\"SALARY\"] = EMPDataOrig[i][\"SALARY\"].str.replace(',', '').str.replace('$', '').str.replace(' ', '').apply(pd.to_numeric)\n",
    "      \n",
    "        ## Load Metadata\n",
    "        ##AGYSUB - AGYTYP, AGY\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTagy.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'AGYSUB', how = 'left')\n",
    "\n",
    "        ##AGELVL - AGELVLT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTagelvl.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'AGELVL', how = 'left')\n",
    "\n",
    "        #LOSLVL - LOSLVLT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTloslvl.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'LOSLVL', how = 'left')\n",
    "        EMPDataOrig[i][\"LOS\"] = EMPDataOrig[i][\"LOS\"].apply(pd.to_numeric)\n",
    "        \n",
    "        ##LOC - LocTypeT, LocT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTloc.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'LOC', how = 'left')\n",
    " \n",
    "        ##OCC - OCCTYPT, OCCFAM\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTocc.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'OCC', how = 'left')\n",
    "\n",
    "        ##PATCO - PATCOT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTpatco.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'PATCO', how = 'left')\n",
    "\n",
    "        ##PPGRD - PayPlan, PPGroup, PPTYP\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTppgrd.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'PPGRD', how = 'left')\n",
    "\n",
    "        ##SALLVL - SALLVLT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTsallvl.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'SALLVL', how = 'left')\n",
    "\n",
    "        ##TOA - TOATYP\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTtoa.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'TOA', how = 'left')\n",
    "\n",
    "        ##WORKSCH - WSTYPT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTwrksch.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'WORKSCH', how = 'left')\n",
    "\n",
    "        display(EMPDataOrig[i].head())\n",
    "\n",
    "    EMPDataOrig4Q = pd.concat([EMPDataOrig[j] for j in range(0,len(EMPDataOrig))])\n",
    "    print(\"Original EMP data size of: \"+str(len(EMPDataOrig4Q)))\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"LOCTYP\"] != \"1\"]))+\" Non-US observations.\")\n",
    "    \n",
    "       ## Remove Non-US Data\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"LOCTYP\"] == \"1\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"OCCTYP\"] == \"3\"]))+\" observations with no specified Occupation.\")\n",
    "\n",
    "       ## Remove Observations with no specified occupation\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"OCCTYP\"] != \"3\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"SALLVL\"] == \"Z\"]))+\" observations with no specified Salary.\")\n",
    "\n",
    "       ## Remove Observations with no specified salary\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"SALLVL\"] != \"Z\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"LOSLVL\"] == \"Z\"]))+\" observations with no specified Length of Service.\")\n",
    "\n",
    "       ## Remove Observations with no specified LOSLVL\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"LOSLVL\"] != \"Z\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] == \"A\"]))+\" observations of Age Level A.\")\n",
    "\n",
    "        ## Remove Observations from Age Level A (less than 20 years old)\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] != \"A\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] == \"Z\"]))+\" observations with no specified Age Level.\")\n",
    "\n",
    "        ## Remove Observations with no specified Age Level\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] != \"Z\"]\n",
    "\n",
    "        ## Fix differences in spaces on WORKSCHT Column\n",
    "    EMPDataOrig4Q[\"WORKSCHT\"] = np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"F\", 'Full-time Nonseasonal',\n",
    "                                    np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"I\", 'Intermittent Nonseasonal',\n",
    "                                             np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"P\", 'Part-time Nonseasonal',\n",
    "                                                      np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"G\", 'Full-time Seasonal',\n",
    "                                                            np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"J\", 'Intermittent Seasonal',\n",
    "                                                                    np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"Q\", 'Part-time Seasonal',\n",
    "                                                                            np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"T\", 'Part-time Job Sharer Seasonal',\n",
    "                                                                                    np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"S\", 'Part-time Job Sharer Nonseasonal',\n",
    "                                                                                            np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"B\", 'Full-time Nonseasonal Baylor Plan',\n",
    "                                                                                                    'NO WORK SCHEDULE REPORTED' ### ELSE case represents Night\n",
    "                                                                                                     )\n",
    "                                                                                             )\n",
    "                                                                                     )\n",
    "                                                                             )\n",
    "                                                                     )\n",
    "                                                              )\n",
    "                                                     )\n",
    "                                            )\n",
    "                                   )    \n",
    "\n",
    "    pickleObject(EMPDataOrig4Q, \"EMPDataOrig4Q\")\n",
    "\n",
    "print(\"New EMP data size of: \"+str(len(EMPDataOrig4Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(EMPDataOrig4Q.describe().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#sns.boxplot(y = \"SALARY\", data = EMPDataOrig4Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both our separation and non-separation data loaded, we calculate three new attributes through aggregation or calculation amongst various attributes. \n",
    "\n",
    "1) SEP Count by Date & Occupation – total number of separations (of any type) for a given Date and Occupation; \n",
    "\n",
    "2) SEP Count by Date & Location – total number of separations (of any type) for a given Date and Location; \n",
    "\n",
    "3) Industry Average Salary – Average salary amongst non-separated employees, grouped by quarter, occupation, pay grade, and work schedule; \n",
    "\n",
    "We proceed, by concatenating our Separation and Non-Separation observations, and merge these newly calculated attributes to the concatenated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "##Aggregate Number of Total Separations in current month for given Occ\n",
    "AggSEPCount_EFDATE_OCC= pd.DataFrame({'SEPCount_EFDATE_OCC' : OPMDataMerged.groupby([\"EFDATE\", \"OCC\"]).size()}).reset_index()\n",
    "display(AggSEPCount_EFDATE_OCC.head())\n",
    "\n",
    "\n",
    "##Aggregate Number of Total Separations in current month for given LOC\n",
    "AggSEPCount_EFDATE_LOC = pd.DataFrame({'SEPCount_EFDATE_LOC' : OPMDataMerged.groupby([\"EFDATE\", \"LOC\"]).size()}).reset_index()\n",
    "display(AggSEPCount_EFDATE_LOC.head())\n",
    "\n",
    "##Average Quarterly EMP Salary by occ \n",
    "AggIndAvgSalary = pd.DataFrame({'count' : EMPDataOrig4Q.groupby([\"QTR\", \"OCC\", \"PPGRD\", \"WORKSCHT\"]).size()}).reset_index()\n",
    "AggIndAvgSalary2 = pd.DataFrame({'IndSalarySum' : EMPDataOrig4Q.groupby([\"QTR\", \"OCC\", \"PPGRD\", \"WORKSCHT\"])[\"SALARY\"].sum()}).reset_index()\n",
    "AggIndAvgSalary = AggIndAvgSalary.merge(AggIndAvgSalary2,on=[\"QTR\", \"OCC\", \"PPGRD\", \"WORKSCHT\"])\n",
    "AggIndAvgSalary[\"IndAvgSalary\"] = AggIndAvgSalary[\"IndSalarySum\"]/AggIndAvgSalary[\"count\"]\n",
    "del AggIndAvgSalary[\"count\"]\n",
    "del AggIndAvgSalary[\"IndSalarySum\"]\n",
    "display(AggIndAvgSalary.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Two Datasets\n",
    "### NS SEP code means NonSeparation\n",
    "###add hardcoded null value columns where applicable\n",
    "EMPDataOrig4Q[\"SEP\"] = \"NS\"\n",
    "EMPDataOrig4Q[\"GENDER\"] = np.nan\n",
    "EMPDataOrig4Q[\"COUNT\"] = np.nan\n",
    "\n",
    "OPMDataMerged[\"DATECODE\"] = OPMDataMerged[\"EFDATE\"]\n",
    "\n",
    "OPMColList = [\"AGYSUB\", \"SEP\", \"DATECODE\",   \"AGELVL\", \"GENDER\", \"GSEGRD\", \"LOSLVL\", \"LOC\", \"OCC\", \"PATCO\", \"PPGRD\", \"SALLVL\", \"TOA\", \"WORKSCH\", \"COUNT\", \"SALARY\", \"LOS\", \"AGYTYP\", \"AGYTYPT\", \"AGY\", \"AGYT\", \"AGYSUBT\", \"QTR\", \"AGELVLT\", \"LOSLVLT\", \"LOCTYP\", \"LOCTYPT\", \"LOCT\", \"OCCTYP\", \"OCCTYPT\", \"OCCFAM\", \"OCCFAMT\", \"OCCT\", \"PATCOT\", \"PPTYP\", \"PPTYPT\", \"PPGROUP\", \"PPGROUPT\", \"PAYPLAN\", \"PAYPLANT\", \"SALLVLT\", \"TOATYP\", \"TOATYPT\", \"TOAT\", \"WSTYP\", \"WSTYPT\", \"WORKSCHT\"]\n",
    "EMPColList = [\"AGYSUB\", \"SEP\", \"DATECODE\", \"AGELVL\", \"GENDER\", \"GSEGRD\", \"LOSLVL\", \"LOC\", \"OCC\", \"PATCO\", \"PPGRD\", \"SALLVL\", \"TOA\", \"WORKSCH\", \"COUNT\", \"SALARY\", \"LOS\", \"AGYTYP\", \"AGYTYPT\", \"AGY\", \"AGYT\", \"AGYSUBT\", \"QTR\", \"AGELVLT\", \"LOSLVLT\", \"LOCTYP\", \"LOCTYPT\", \"LOCT\", \"OCCTYP\", \"OCCTYPT\", \"OCCFAM\", \"OCCFAMT\", \"OCCT\", \"PATCOT\", \"PPTYP\", \"PPTYPT\", \"PPGROUP\", \"PPGROUPT\", \"PAYPLAN\", \"PAYPLANT\", \"SALLVLT\", \"TOATYP\", \"TOATYPT\", \"TOAT\", \"WSTYP\", \"WSTYPT\", \"WORKSCHT\"]\n",
    "\n",
    "OPMDataMerged = pd.concat([OPMDataMerged[OPMColList], EMPDataOrig4Q[EMPColList]], ignore_index=True)\n",
    "print(\"Total concatenated data size for SEP and non-SEP: \"+str(len(OPMDataMerged)))\n",
    "\n",
    "OPMDataMerged = OPMDataMerged.merge(AggSEPCount_EFDATE_OCC, left_on = ['DATECODE','OCC'], right_on = ['EFDATE','OCC'], how = 'left')\n",
    "OPMDataMerged = OPMDataMerged.merge(AggSEPCount_EFDATE_LOC, left_on = ['DATECODE','LOC'], right_on = ['EFDATE','LOC'], how = 'left')\n",
    "OPMDataMerged = OPMDataMerged.merge(AggIndAvgSalary, on = ['QTR','OCC', 'PPGRD', 'WORKSCHT'], how = 'left')\n",
    "OPMDataMerged[\"SalaryOverUnderIndAvg\"] = OPMDataMerged[\"SALARY\"] - OPMDataMerged[\"IndAvgSalary\"]\n",
    "\n",
    "del OPMDataMerged[\"EFDATE_x\"]\n",
    "del OPMDataMerged[\"EFDATE_y\"]\n",
    "\n",
    "display(OPMDataMerged.head())\n",
    "display(OPMDataMerged.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].isnull()]))\n",
    "\n",
    "display(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].isnull()][[\"SEP\",\"DATECODE\", \"OCC\"]].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 50993 Non-Separation observations do not have coverage within the Separation Dataset, thus, we will remove these observations as out of scope demographic in our analysis. Any attempt in predicting these values will not have enough data to support a significant response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].notnull()]\n",
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].isnull()]))\n",
    "\n",
    "print(len(OPMDataMerged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_LOC\"].isnull()]))\n",
    "\n",
    "display(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_LOC\"].isnull()][[\"SEP\",\"DATECODE\",\"LOC\"]].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].isnull()]))\n",
    "\n",
    "display(OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].isnull()][[\"QTR\", \"SEP\",\"OCCT\", \"PPGRD\", \"WORKSCHT\"]].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 1293 separation observations do not have coverage within the EMP Dataset, thus, we will remove these observations as out of scope demographic in our analysis. Any attempt in predicting these values will not have enough data to support a significant response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].notnull()]\n",
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].isnull()]))\n",
    "\n",
    "print(len(OPMDataMerged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************************\n",
    "*************************************\n",
    "\n",
    "# Placeholder Chunks for Data Quality check of salary against GS Grade Level Ranges\n",
    "\n",
    "*************************************\n",
    "*************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder Chunks for Data Quality check of salary against GS Grade Level Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are iterested to see how federal pension plans may impact attrition in this dataset. An interesting attribute to complement Length of service, is Years to Retirement. Utilizing a FERS retirement eligibility baseline of 57 years of age for all observations, and the lower limitation of age level ranges we compute a numeric value for length of retirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Column YearsToRetirement\n",
    "\n",
    "\"\"\"\n",
    "    AGELVL,AGELVLT\n",
    "    A,Less than 20\n",
    "    B,20-24\n",
    "    C,25-29\n",
    "    D,30-34\n",
    "    E,35-39\n",
    "    F,40-44\n",
    "    G,45-49\n",
    "    H,50-54\n",
    "    I,55-59\n",
    "    J,60-64\n",
    "    K,65 or more\n",
    "    Z,Unspecified\n",
    "\"\"\"\n",
    "OPMDataMerged[\"LowerLimitAge\"] = np.where(OPMDataMerged[\"AGELVL\"]==\"B\", 20,\n",
    "                                                np.where(OPMDataMerged[\"AGELVL\"]==\"C\", 25,\n",
    "                                                         np.where(OPMDataMerged[\"AGELVL\"]==\"D\", 30,\n",
    "                                                                  np.where(OPMDataMerged[\"AGELVL\"]==\"E\", 35,\n",
    "                                                                           np.where(OPMDataMerged[\"AGELVL\"]==\"F\", 40,\n",
    "                                                                                    np.where(OPMDataMerged[\"AGELVL\"]==\"G\", 45,\n",
    "                                                                                             np.where(OPMDataMerged[\"AGELVL\"]==\"H\", 50,\n",
    "                                                                                                      np.where(OPMDataMerged[\"AGELVL\"]==\"I\", 55,\n",
    "                                                                                                               np.where(OPMDataMerged[\"AGELVL\"]==\"J\", 60,\n",
    "                                                                                                                        np.where(OPMDataMerged[\"AGELVL\"]==\"K\", 65,\n",
    "                                                                                                                                 np.nan\n",
    "                                                                                                                                )\n",
    "                                                                                                                        )\n",
    "                                                                                                               )\n",
    "                                                                                                      )\n",
    "                                                                                            )\n",
    "                                                                                   )\n",
    "                                                                          )\n",
    "                                                                 )\n",
    "                                                        )\n",
    "                                               )  \n",
    "\n",
    "retAge = 57\n",
    "\n",
    "OPMDataMerged[\"YearsToRetirement\"] = np.where(OPMDataMerged[\"AGELVL\"]==\"B\", retAge-20,\n",
    "                                                np.where(OPMDataMerged[\"AGELVL\"]==\"C\", retAge-25,\n",
    "                                                         np.where(OPMDataMerged[\"AGELVL\"]==\"D\", retAge-30,\n",
    "                                                                  np.where(OPMDataMerged[\"AGELVL\"]==\"E\", retAge-35,\n",
    "                                                                           np.where(OPMDataMerged[\"AGELVL\"]==\"F\", retAge-40,\n",
    "                                                                                    np.where(OPMDataMerged[\"AGELVL\"]==\"G\", retAge-45,\n",
    "                                                                                             np.where(OPMDataMerged[\"AGELVL\"]==\"H\", retAge-50,\n",
    "                                                                                                      np.where(OPMDataMerged[\"AGELVL\"]==\"I\", retAge-55,\n",
    "                                                                                                               np.where(OPMDataMerged[\"AGELVL\"]==\"J\", retAge-60,\n",
    "                                                                                                                        np.where(OPMDataMerged[\"AGELVL\"]==\"K\", retAge-65,\n",
    "                                                                                                                                 np.nan\n",
    "                                                                                                                                )\n",
    "                                                                                                                        )\n",
    "                                                                                                               )\n",
    "                                                                                                      )\n",
    "                                                                                            )\n",
    "                                                                                   )\n",
    "                                                                          )\n",
    "                                                                 )\n",
    "                                                        )\n",
    "                                               )  \n",
    "\n",
    "print(\"Null Values for LowerLimitAge: \" + str(len(OPMDataMerged[OPMDataMerged[\"LowerLimitAge\"].isnull()])))\n",
    "print(\"Null Values for YearsToRetirement: \" + str(len(OPMDataMerged[OPMDataMerged[\"YearsToRetirement\"].isnull()])))\n",
    "\n",
    "display(OPMDataMerged.head())\n",
    "display(OPMDataMerged.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Bureau of Labor Statistics data\n",
    "\n",
    "In addition to the OPM data, we merge 10 attributes from the Bureau of Labor Statistics (BLS). Data is sourced from Federal Government industry codes across all regions. Although assumed to be highly correlated, we source both Level (Total number) and Rate (Percentage of Level to total employment and / or job openings) for the following statistics: 1) Job Openings, 2) Layoffs, 3) Quits, 4) Total Separations, and 5) Other Separations. While Rate paints an aggregated, holistic picture for job market trends, Level provides a raw count for total separations alone. Both these statistics were captured by a monthly aggregate and merged to the OPM data by their respective months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def bls(series, start, end):\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    sID   = []\n",
    "    \n",
    "    for i in range(0,len(series)):\n",
    "        sID.append(series[i][0])\n",
    "    \n",
    "    data = json.dumps({\"seriesid\": sID,\n",
    "                       \"startyear\":start,\n",
    "                       \"endyear\":end,\n",
    "                       \"catalog\":False,\n",
    "                       \"calculations\":False,\n",
    "                       \"annualaverage\":False,\n",
    "                       \"registrationkey\":\"7a89c8d7979349fba8914b8be16a1646\"})\n",
    "    \n",
    "    p = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=data, headers=headers)\n",
    "    json_data = json.loads(p.text)\n",
    "    bls = []\n",
    "    for series in json_data['Results']['series']:\n",
    "        #x=prettytable.PrettyTable([\"series id\",\"year\",\"period\",\"value\",\"footnotes\"])\n",
    "        result = pd.DataFrame(columns=[\"series id\",\"year\",\"period\",\"value\",\"footnotes\"])\n",
    "        seriesId = series['seriesID']\n",
    "        for item in series['data']:\n",
    "            year = item['year']\n",
    "            period = item['period']\n",
    "            value = item['value']\n",
    "            footnotes=\"\"\n",
    "            for footnote in item['footnotes']:\n",
    "                if footnote:\n",
    "                    footnotes = footnotes + footnote['text'] + ','\n",
    "            if 'M01' <= period <= 'M12':\n",
    "                #x.add_row([seriesId,year,period,value,footnotes[0:-1]])\n",
    "                y = pd.DataFrame({\"series id\" : seriesId,\n",
    "                                  \"year\" : year,\n",
    "                                  \"period\" : period,\n",
    "                                  \"value\" : value,\n",
    "                                  \"footnotes\" : footnotes}, index = [0])\n",
    "                result = result.append(y, ignore_index = True)\n",
    "        bls.append(result)\n",
    "    return(bls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seriesList = [\n",
    "              ['JTU91000000JOL','BLS_FEDERAL_JobOpenings_Level'],\n",
    "              ['JTU91000000LDL','BLS_FEDERAL_Layoffs_Level'],\n",
    "              ['JTU91000000OSL','BLS_FEDERAL_OtherSep_Level'],\n",
    "              ['JTU91000000QUL','BLS_FEDERAL_Quits_Level'],\n",
    "              ['JTU91000000TSL','BLS_FEDERAL_TotalSep_Level'],\n",
    "              ['JTU91000000JOR','BLS_FEDERAL_JobOpenings_Rate'],\n",
    "              ['JTU91000000LDR','BLS_FEDERAL_Layoffs_Rate'],\n",
    "              ['JTU91000000OSR','BLS_FEDERAL_OtherSep_Rate'],\n",
    "              ['JTU91000000QUR','BLS_FEDERAL_Quits_Rate'],\n",
    "              ['JTU91000000TSR','BLS_FEDERAL_TotalSep_Rate']\n",
    "             ]\n",
    "\n",
    "# Pull job openings and labor turnover data\n",
    "JTL = bls(seriesList, \"2014\", \"2015\")\n",
    "\n",
    "seriesList = pd.DataFrame(seriesList, columns = [\"series id\",\"sName\"])\n",
    "\n",
    "##We need to replace these with actual Descriptor Column Names\n",
    "\n",
    "for i in range(0,len(seriesList)):\n",
    "    \n",
    "    JTL[i] = JTL[i].merge(seriesList, on = \"series id\", how = 'inner')\n",
    "\n",
    "    if len(JTL[i]) >0:\n",
    "        name = JTL[i][\"sName\"].drop_duplicates().values[0]\n",
    "    else:\n",
    "        name = str(i)\n",
    "\n",
    "    JTL[i][name] = JTL[i][\"value\"].apply(pd.to_numeric)\n",
    "    JTL[i][\"DATECODE\"] = JTL[i][\"year\"] + JTL[i][\"period\"].str[-2:]\n",
    "    del JTL[i][\"value\"]\n",
    "    del JTL[i][\"year\"]\n",
    "    del JTL[i][\"period\"]\n",
    "    del JTL[i][\"series id\"]\n",
    "    del JTL[i][\"footnotes\"]\n",
    "    del JTL[i][\"sName\"]\n",
    "    \n",
    "    \n",
    "    OPMDataMerged = OPMDataMerged.merge(JTL[i], on = \"DATECODE\", how = 'left')\n",
    "    display(JTL[i].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(OPMDataMerged.head())\n",
    "display(OPMDataMerged.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame({'StratCount' : OPMDataMerged.groupby([\"SEP\"]).size()}).reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several separation types we would like to either roll up, or remove altogether.\n",
    "\n",
    "**Roll-Up**\n",
    "\n",
    "We have chosen to roll up all retirement separation together. Separation categories of 1) SD,Retirement - Voluntary; 2)  SE,Retirement - Early Out; 3) SF,Retirement - Disability; 4) SG,Retirement - Other are consolidated into one category \"SD\".\n",
    "\n",
    "**Removal**\n",
    "\n",
    "We have chosen to remove the following. 1) SB,Transfer Out - Mass Transfer; 2) SK,Death; 3) SL,Other Separation. 4) SJ,Termination (Expired Appt/Other)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OPMDataMerged = OPMDataMerged[(OPMDataMerged[\"SEP\"] != \"SB\") & (OPMDataMerged[\"SEP\"] != \"SK\") & (OPMDataMerged[\"SEP\"] != \"SL\") & (OPMDataMerged[\"SEP\"] != \"SJ\")]\n",
    "\n",
    "OPMDataMerged.loc[(OPMDataMerged[\"SEP\"] == \"SD\") | (OPMDataMerged[\"SEP\"] == \"SE\") | (OPMDataMerged[\"SEP\"] == \"SF\") | (OPMDataMerged[\"SEP\"] == \"SG\"), \"SEP\"]=\"SD\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary EDA\n",
    "\n",
    "In terms of data exploration, we first investigate numeric type attributes. Relationships, distributions, and correlation values are reviewed.\n",
    "\n",
    "**A new binary separation attribute is created to indicate whether non-sep or sep for EDA correlation purposes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#\n",
    "#cols = list(SampledOPMData.select_dtypes(include=['float64', 'int64']))\n",
    "#cols.remove('COUNT')\n",
    "#cols.remove('BLS_FEDERAL_OtherSep_Rate')\n",
    "#cols.remove('BLS_FEDERAL_Quits_Rate')\n",
    "#cols.remove('BLS_FEDERAL_TotalSep_Level')\n",
    "#cols.remove('BLS_FEDERAL_JobOpenings_Rate')\n",
    "#cols.remove('BLS_FEDERAL_OtherSep_Level')\n",
    "#cols.remove('BLS_FEDERAL_Quits_Level')\n",
    "#cols.remove('BLS_FEDERAL_JobOpenings_Level')\n",
    "#cols.remove('BLS_FEDERAL_Layoffs_Rate')\n",
    "#cols.remove('BLS_FEDERAL_Layoffs_Level')\n",
    "#cols.remove('BLS_FEDERAL_TotalSep_Rate')\n",
    "#cols.append('SEP')\n",
    "#display(cols)\n",
    "#\n",
    "#plotNumeric = SampledOPMData[cols]\n",
    "#\n",
    "## Create binary separation attribute for EDA correlation review\n",
    "##plotNumeric[\"SEP_bin\"] = plotNumeric.SEP.replace(\"NS\", 1)\n",
    "##plotNumeric.loc[plotNumeric['SEP_bin'] != 1, 'SEP_bin'] = 0\n",
    "##plotNumeric.SEP_bin = plotNumeric.SEP_bin.apply(pd.to_numeric)\n",
    "#AttSplit = pd.get_dummies(plotNumeric['SEP'],prefix='SEP')\n",
    "#display(AttSplit.head())\n",
    "#plotNumeric = pd.concat((plotNumeric,AttSplit),axis=1) # add back into the dataframe\n",
    "#\n",
    "#display(plotNumeric.head())\n",
    "#print(\"plotNumeric has {0} Records\".format(len(plotNumeric)))\n",
    "##print(plotNumeric.SEP_bin.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "cols = list(OPMDataMerged.select_dtypes(include=['float64', 'int64']))\n",
    "cols.remove('COUNT')\n",
    "cols.remove('BLS_FEDERAL_OtherSep_Rate')\n",
    "cols.remove('BLS_FEDERAL_Quits_Rate')\n",
    "cols.remove('BLS_FEDERAL_TotalSep_Level')\n",
    "cols.remove('BLS_FEDERAL_JobOpenings_Rate')\n",
    "cols.remove('BLS_FEDERAL_OtherSep_Level')\n",
    "cols.remove('BLS_FEDERAL_Quits_Level')\n",
    "cols.remove('BLS_FEDERAL_JobOpenings_Level')\n",
    "cols.remove('BLS_FEDERAL_Layoffs_Rate')\n",
    "cols.remove('BLS_FEDERAL_Layoffs_Level')\n",
    "cols.remove('BLS_FEDERAL_TotalSep_Rate')\n",
    "cols.append('SEP')\n",
    "display(cols)\n",
    "\n",
    "plotNumeric = OPMDataMerged[cols]\n",
    "\n",
    "# Create binary separation attribute for EDA correlation review\n",
    "#plotNumeric[\"SEP_bin\"] = plotNumeric.SEP.replace(\"NS\", 1)\n",
    "#plotNumeric.loc[plotNumeric['SEP_bin'] != 1, 'SEP_bin'] = 0\n",
    "#plotNumeric.SEP_bin = plotNumeric.SEP_bin.apply(pd.to_numeric)\n",
    "AttSplit = pd.get_dummies(plotNumeric['SEP'],prefix='SEP')\n",
    "display(AttSplit.head())\n",
    "plotNumeric = pd.concat((plotNumeric,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "display(plotNumeric.head())\n",
    "print(\"plotNumeric has {0} Records\".format(len(plotNumeric)))\n",
    "#print(plotNumeric.SEP_bin.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#sns.set(font_scale=1)\n",
    "#sns.pairplot(plotNumeric.drop(['SEP_NS',\n",
    "#                               'SEP_SA',\n",
    "#                               'SEP_SC',\n",
    "#                               'SEP_SD',\n",
    "#                               'SEP_SH', \n",
    "#                               'SEP_SI'], axis=1), hue = 'SEP', palette=\"hls\", plot_kws={\"s\": 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/amfrye777/MSDSCapstone/blob/master/OPMSeparationsCode/Visualizations/PairPlotCorrelations.png?raw=true\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function modified from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n",
    "sns.set()\n",
    "\n",
    "def draw_histograms(df, variables, n_rows, n_cols):\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        df[var_name].hist(bins=20,ax=ax, color='#58D68D')\n",
    "        ax.set_title(var_name+\" Distribution\")\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "draw_histograms(plotNumeric.drop(['SEP',\n",
    "                                  'SEP_NS',\n",
    "                                  'SEP_SA',\n",
    "                                  'SEP_SC',\n",
    "                                  'SEP_SD',\n",
    "                                  'SEP_SH', \n",
    "                                  'SEP_SI'], axis=1),\n",
    "                plotNumeric.drop(['SEP',\n",
    "                                  'SEP_NS',\n",
    "                                  'SEP_SA',\n",
    "                                  'SEP_SC',\n",
    "                                  'SEP_SD',\n",
    "                                  'SEP_SH',\n",
    "                                  'SEP_SI'], axis=1).columns, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Inspired by http://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "#plt.matshow(plotNumeric.corr())\n",
    "\n",
    "sns.set(style='white')\n",
    "corr = plotNumeric.drop(['SEP'], axis=1).corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.set(font_scale=0.95)\n",
    "heatCorr = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "                       square=True, annot=True, linewidths=1,\n",
    "                       cbar_kws={\"shrink\": .5}, ax=ax, fmt='.1g')\n",
    "#heatCorr.\n",
    "ax.tick_params(labelsize=15)\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.tick_params(labelsize=15)\n",
    "\n",
    "sns.plt.show()\n",
    "#sns.heatmap(corr, annot=True, linewidths=0.01, cmap=cmap, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the distribution of attributes identified above, we have decided to take the log transform of several attributes. \n",
    "- Salary\n",
    "- LOS (augmented by a value of .00001 to adjust for the undefined result of log(0)\n",
    "- SEPCount_EFDATE_OCC\n",
    "- SEPCount_EFDATE_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Log Transform Columns Added\n",
    "OPMDataMerged[\"SALARYLog\"] = OPMDataMerged[\"SALARY\"].apply(np.log)\n",
    "OPMDataMerged[\"LOSLog\"] = (OPMDataMerged[\"LOS\"] + .00001).apply(np.log)\n",
    "OPMDataMerged[\"SEPCount_EFDATE_OCCLog\"] = OPMDataMerged[\"SEPCount_EFDATE_OCC\"].apply(np.log)\n",
    "OPMDataMerged[\"SEPCount_EFDATE_LOCLog\"] = OPMDataMerged[\"SEPCount_EFDATE_LOC\"].apply(np.log)\n",
    "OPMDataMerged[\"IndAvgSalaryLog\"] = OPMDataMerged[\"IndAvgSalary\"].apply(np.log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We next review categorical data to improve our understanding of factor levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#cols = list(SampledOPMData.select_dtypes(include=['object']))\n",
    "#dropCols = [\"LOCTYP\",\n",
    "#            \"LOCTYPT\",\n",
    "#            \"OCCTYP\",\n",
    "#            \"OCCTYPT\",\n",
    "#            \"PPTYP\",\n",
    "#            \"PPTYPT\",\n",
    "#            \"AGYTYP\",\n",
    "#            \"OCCFAM\",\n",
    "#            \"PPGROUP\",\n",
    "#            \"PAYPLAN\",\n",
    "#            \"TOATYP\",\n",
    "#            \"WSTYP\",\n",
    "#            \"AGYSUBT\",\n",
    "#            \"AGELVL\",\n",
    "#            \"LOSLVL\",\n",
    "#            \"LOC\",\n",
    "#            \"OCC\",\n",
    "#            \"PATCO\",\n",
    "#            \"SALLVL\",\n",
    "#            \"TOA\",\n",
    "#            \"WORKSCH\"]\n",
    "#\n",
    "#for i in dropCols:\n",
    "#    cols.remove(i)\n",
    "#\n",
    "#plotCat = SampledOPMData[cols]\n",
    "#display(plotCat.head())\n",
    "#print(\"plotCat Has {0} Records\".format(len(plotCat)))\n",
    "#print(\"Number of colums = \", len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cols = list(OPMDataMerged.select_dtypes(include=['object']))\n",
    "dropCols = [\"LOCTYP\",\n",
    "            \"LOCTYPT\",\n",
    "            \"OCCTYP\",\n",
    "            \"OCCTYPT\",\n",
    "            \"PPTYP\",\n",
    "            \"PPTYPT\",\n",
    "            \"AGYTYP\",\n",
    "            \"OCCFAM\",\n",
    "            \"PPGROUP\",\n",
    "            \"PAYPLAN\",\n",
    "            \"TOATYP\",\n",
    "            \"WSTYP\",\n",
    "            \"AGYSUBT\",\n",
    "            \"AGELVL\",\n",
    "            \"LOSLVL\",\n",
    "            \"LOC\",\n",
    "            \"OCC\",\n",
    "            \"PATCO\",\n",
    "            \"SALLVL\",\n",
    "            \"TOA\",\n",
    "            \"WORKSCH\"]\n",
    "\n",
    "for i in dropCols:\n",
    "    cols.remove(i)\n",
    "\n",
    "plotCat = OPMDataMerged[cols]\n",
    "display(plotCat.head())\n",
    "print(\"plotCat Has {0} Records\".format(len(plotCat)))\n",
    "print(\"Number of colums = \", len(cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGYSUB\n",
    "High seperation among following:\n",
    "* Veterans Health Administration (VATA)\n",
    "* Forest Service (AG11)\n",
    "\n",
    "### GENDER\n",
    "Similar separation distributions among males and females, except more terminations due to contract expiration among males\n",
    "\n",
    "### GSEGRD\n",
    "High termination due to expired appt/other among following:\n",
    "* 3\n",
    "* 4\n",
    "* 5\n",
    "\n",
    "Bimodal Quit distribution with outlier spike at GSEGRD 9:\n",
    "* Distribution 1 from GSEGRD 3 to 8\n",
    "* Distribution 2 from GSEGRD 11 to 15\n",
    "\n",
    "Individual transfers highest among levels 11, 12, 13\n",
    "\n",
    "### PPGRD\n",
    "Majority of distribution resides in GS values per the GSEGRD observations described above.... <font color=\"red\">Are other PPGRD values of any significance? What are corporate grades all about?</font>\n",
    "\n",
    "### AGYT\n",
    "Top three Agencies with separation:\n",
    "1. AR-Department of the Army\n",
    "2. AG-Department of Agriculture\n",
    "3. VA-Department of Veteran Affairs\n",
    "\n",
    "High contract termination in:\n",
    "* AG-Department of Agriculture\n",
    "* IN-Department of the Interior\n",
    "\n",
    "While Veteran Affairs and Army both have many quits and many retirees, the Army has significantly more individual transfers (on par with retirements)\n",
    "\n",
    "### QTR\n",
    "Most contract terminations in 1st and 4th quarters\n",
    "\n",
    "Retirement peaks in 2nd quarter\n",
    "\n",
    "Number of quits increases from one quarter to the next\n",
    "\n",
    "<font color=\"purple\">*Bear in mind these are quarters from single year only so time-sensitive trends may not be applicable*</font>\n",
    "\n",
    "### AGELVLT\n",
    "High termination due to expired appt/other among following:\n",
    "* B\n",
    "* C\n",
    "\n",
    "Number of Quits peaks at AGELVL D\n",
    "\n",
    "Individual transfer counts mostly trend with Quits\n",
    "\n",
    "Retirement highest at following:\n",
    "* I\n",
    "* J\n",
    "* K\n",
    "\n",
    "### LOSLVLT\n",
    "Highest Quit count for LOSLVL A (< 1 year service) which then declines for levels B and C before spiking again at level D (5-9 years service)\n",
    "\n",
    "Same pattern is observed for contract terminations but without any significant spikes with longer service\n",
    "\n",
    "Large individual transfer spike at LOSLVL D (5-9 years service)\n",
    "\n",
    "Retirement starts at LOSLVL D but trends upward to J\n",
    "\n",
    "### LOCT\n",
    "Contract terminations comprise most California terminations among top total separation states\n",
    "\n",
    "East Coast locations may possibly have most individual transfers, the most being in Washington DC\n",
    "\n",
    "### OCCFAMT\n",
    "03xx-General Admin, clerical, and office svcs highest separation by far but indicates both high number of Quits and Retirements\n",
    "\n",
    "Many quits in 06xx-Medical\n",
    "\n",
    "04xx-Natural Resources again indicates high number of contract terminations\n",
    "\n",
    "01xx-Social Science has even number of Quits and retirements\n",
    "\n",
    "### OCCT\n",
    "\n",
    "### PATCOT\n",
    "\n",
    "### PAYPLANT\n",
    "Results skewed by GS\n",
    "\n",
    "### TOAT\n",
    "\n",
    "### WORKSCHT\n",
    "Should model full time only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subCountPlot(att1, att2, thresh):\n",
    "    counts = plotCat.groupby([att1, att2]).size().unstack(fill_value=0) # Get att1 sizes by att2\n",
    "    counts = pd.concat([counts,counts.sum(axis=1)], axis=1) # Calculate total for each att1 value and append total as new column\n",
    "    counts.rename(columns={0:\"Total\"}, inplace=True)\n",
    "    top = counts[counts[\"Total\"] > thresh].index.tolist() # Obtain att1 values where total surpasses threshold\n",
    "    \n",
    "    zoom = plotCat[plotCat[att1].isin(top)] # Subset data to only the top att1 values\n",
    "    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10), sharey=False)\n",
    "    sns.countplot(y=att1, data=zoom, color=\"blue\", ax=ax1); # Dark blue signifies zoomed data\n",
    "    sns.countplot(y=att1, data=zoom, hue=att2, palette=\"hls\", ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percBarPlot(att1, att2, numColors):\n",
    "    # Create count by att1 and att2\n",
    "    counts = plotCat.groupby([att1, att2]).size().unstack(fill_value=0) # Get att1 sizes by att2\n",
    "    counts = pd.concat([counts,counts.sum(axis=1)], axis=1) # Calculate total for each att1 value and append total as new column\n",
    "    counts.rename(columns={0:\"Total\"}, inplace=True)\n",
    "    #counts.drop('Total', axis=1).plot(kind='bar', stacked=True)\n",
    "    \n",
    "    # create cmap from sns color palette\n",
    "    my_cmap = ListedColormap(sns.color_palette('hls', numColors).as_hex())\n",
    "\n",
    "    # Create and plot percentage by att1 and att2\n",
    "    nest1 = []\n",
    "    for i in counts.values:\n",
    "        nest2 = []\n",
    "        for j in i:\n",
    "            nest2.append(float(j/(i[len(i)-1:]))*100)\n",
    "        nest1.append(nest2)\n",
    "    perc = pd.DataFrame(nest1)\n",
    "    perc = perc.set_index(counts.index.values)\n",
    "    perc.columns = counts.columns\n",
    "    perc.drop('Total', axis=1).plot(kind='bar', stacked=True, ylim=(0,100), figsize={13,6}, title=att1+' Percentage Plot', colormap=my_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = cols[:4] # for quick visualization debug only; may delete once complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in cols:\n",
    "    if i != 'SEP':\n",
    "        plt.figure(i) # Required to create new figure each loop rather than drawing over previous object\n",
    "        f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10), sharey=False)\n",
    "        sns.countplot(y=i, data=plotCat, color=\"lightblue\", ax=ax1);\n",
    "        sns.countplot(y=i, data=plotCat, hue=\"SEP\", palette=\"hls\", ax=ax2);\n",
    "        \n",
    "    if i == 'AGYSUB':\n",
    "        subCountPlot(i, 'SEP', 10000)\n",
    "    elif i == 'LOCT':\n",
    "        subCountPlot(i, 'SEP', 4000)\n",
    "    elif i == 'OCCT':\n",
    "        subCountPlot(i, 'SEP', 2000)\n",
    "    elif i == 'PPGRD':\n",
    "        subCountPlot(i, 'SEP', 6000)\n",
    "    elif i == 'AGYT':\n",
    "        subCountPlot(i, 'SEP', 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in cols:\n",
    "    if i != 'SEP':\n",
    "        percBarPlot(i, 'SEP', len(plotCat.SEP.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percBarPlot('GSEGRD', 'SALLVLT', len(plotCat.SALLVLT.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percBarPlot('PATCOT', 'SALLVLT', len(plotCat.SALLVLT.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.violinplot(x=\"PATCOT\", y=\"SALARY\", hue=\"GENDER\", data=OPMDataMerged[OPMDataMerged.GENDER != 'Z'], split=True,\n",
    "               inner=\"quart\", palette={\"M\": \"b\", \"F\": \"pink\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"SEP\", y=\"SALARY\", hue=\"GENDER\", data=OPMDataMerged[OPMDataMerged.GENDER != 'Z'], split=True,\n",
    "               inner=\"quart\", palette={\"M\": \"b\", \"F\": \"pink\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.factorplot(x=\"SEP\", y=\"SALARY\", hue=\"GENDER\", col=\"PATCOT\",\n",
    "               data=OPMDataMerged[OPMDataMerged.GENDER != 'Z'],\n",
    "               kind=\"violin\", split=True, aspect=.4, size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#sns.factorplot(x=\"SEP\", y=\"SALARY\", col=\"PATCOT\", data=OPMDataMerged,\n",
    "#               kind=\"violin\", split=True, aspect=.4, size=10, palette = \"hls\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/amfrye777/MSDSCapstone/blob/master/OPMSeparationsCode/Visualizations/FactorPlotViolins.png?raw=true\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#g = sns.PairGrid(data=OPMDataMerged,\n",
    "#                 x_vars=[\"SEP\",\"PATCOT\"],\n",
    "#                 y_vars=[\"SALARY\", \"LOS\", \"LowerLimitAge\", \"YearsToRetirement\"],\n",
    "#                 aspect=1, size=10)\n",
    "#g.map(sns.violinplot, palette=\"pastel\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/amfrye777/MSDSCapstone/blob/master/OPMSeparationsCode/Visualizations/PairGridViolins.png?raw=true\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(plotNumeric, plotCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focusing in on our Target Demographic\n",
    "\n",
    "After analyzing the above plots for our categorical data, we have decided to narrow our focus due to the large variability in the dataset. We take the below actions on our dataset:\n",
    "- Keep only Full-time Nonseasonal observations\n",
    "- Remove the location US-SUPPRESSED (SEE DATA DEFINITIONS) due to apparent bias towards unknowns in the non-separation data\n",
    "- Keep only General Schedele Grades above 7.\n",
    "- Focus model generation on White Collar Jobs only\n",
    "- Create a Training set for the Professional PATCO value, and a Testing set for Administration\n",
    "\n",
    "In addition, we have opted to remove the below attributes for model generation:\n",
    "- Datecode, QTR; Although very relevant for merging data from alternate sources, we do not have several years of data so this does not bring us much value\n",
    "- All Agency Attributes(AGYTYP,AGYTYPT,AGY,AGYT,AGYSUB,AGYSUBT); We are not concerned with agencies\n",
    "- Gender; Missing values for Non-Separation observations\n",
    "- Count; Missing values for Non-Separation observations; Also, all values = 1 so not very useful\n",
    "- PAYPLAN,PAYPLANT,PPGRD; Much too granular than we care for\n",
    "- LOSLVL,LOSLVLT; we have a numerical version of this attribute\n",
    "- OCC,OCCT; Much too granular than we care for\n",
    "\n",
    "Our goal is to limit our focus to Professional occupations, build a model, then test that generated model on the Administration segment of the population.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Removing Attributes\n",
    "cols = list(OPMDataMerged.columns)\n",
    "dropCols = [\"QTR\",\n",
    "            \"AGYTYP\",\n",
    "            \"AGYTYPT\",\n",
    "            \"AGY\",\n",
    "            \"AGYT\",\n",
    "            \"AGYSUB\",\n",
    "            \"AGYSUBT\",\n",
    "            \"GENDER\",\n",
    "            \"COUNT\",\n",
    "            \"PAYPLAN\",\n",
    "            \"PAYPLANT\",\n",
    "            \"PPGRD\",\n",
    "            \"LOSLVL\",\n",
    "            \"LOSLVLT\",\n",
    "            \"SALLVL\",\n",
    "            \"SALLVLT\",\n",
    "            \"OCC\",\n",
    "            \"OCCT\"]\n",
    "\n",
    "for i in dropCols:\n",
    "    if i in cols:\n",
    "        cols.remove(i)\n",
    "\n",
    "OPMDataMerged = OPMDataMerged[cols]\n",
    "\n",
    "# Keep only Full-time Nonseasonal observations\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"WORKSCH\"] == \"F\"]\n",
    "\n",
    "#Remove the location US-SUPPRESSED (SEE DATA DEFINITIONS)\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"LOC\"] != \"US\"]\n",
    "\n",
    "#Keep only General Schedele Grades above 7.\n",
    "OPMDataMerged[\"GSEGRD\"] = OPMDataMerged[\"GSEGRD\"].apply(pd.to_numeric)\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"GSEGRD\"] >= 7]\n",
    "\n",
    "#Focus model generation on White Collar Jobs only\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"OCCTYP\"] == \"1\"]\n",
    "\n",
    "#Create a Training set for the Professional PATCO value, and a Testing set for Administration\n",
    "OPMDataMergedProf = OPMDataMerged[OPMDataMerged[\"PATCO\"] == \"1\"]\n",
    "OPMDataMergedAdmin = OPMDataMerged[OPMDataMerged[\"PATCO\"] == \"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(OPMDataMergedProf.head())\n",
    "print(len(OPMDataMergedProf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(OPMDataMergedAdmin.head())\n",
    "print(len(OPMDataMergedAdmin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curious on stratum SEP counts for full remaining data\n",
    "stratum = pd.DataFrame({'StratCount' : OPMDataMerged.groupby([\"SEP\"]).size()}).reset_index()\n",
    "\n",
    "display(stratum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess Stratum SEP Counts for Prof, for use in sampling\n",
    "maxSize=4000\n",
    "stratumProf = pd.DataFrame({'StratCount' : OPMDataMergedProf.groupby([\"SEP\"]).size()}).reset_index()\n",
    "\n",
    "stratumProf.loc[stratumProf[\"StratCount\"]>maxSize,\"StratCountSample\"] = maxSize\n",
    "stratumProf.loc[stratumProf[\"StratCount\"]<=maxSize,\"StratCountSample\"] = stratumProf[\"StratCount\"]\n",
    "#else: stratum[\"StratCountSample\"] = stratum[\"StratCount\"]\n",
    "\n",
    "display(stratumProf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess Stratum SEP Counts for Admin, for use in sampling\n",
    "maxSize=4000\n",
    "stratumAdmin = pd.DataFrame({'StratCount' : OPMDataMergedAdmin.groupby([\"SEP\"]).size()}).reset_index()\n",
    "\n",
    "stratumAdmin.loc[stratumAdmin[\"StratCount\"]>maxSize,\"StratCountSample\"] = maxSize\n",
    "stratumAdmin.loc[stratumAdmin[\"StratCount\"]<=maxSize,\"StratCountSample\"] = stratumAdmin[\"StratCount\"]\n",
    "#else: stratum[\"StratCountSample\"] = stratum[\"StratCount\"]\n",
    "\n",
    "display(stratumAdmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def aggStratPop(stratum, OPMDataMerged):\n",
    "    AggStrat = []\n",
    "\n",
    "    for i in range(0,len(stratum)):\n",
    "        sep = stratum[\"SEP\"].ix[i]\n",
    "        StratCountSample = stratum[\"StratCountSample\"].ix[i]\n",
    "        print(\"Stratum Sample Size Calculations for SEP: {}\".format(sep))   \n",
    "        AggStrat.append(pd.DataFrame({'StratCount' : OPMDataMerged[OPMDataMerged[\"SEP\"]==sep].groupby([\"DATECODE\", \"AGELVL\"]).size()}).reset_index())\n",
    "        AggStrat[i][\"SEP\"] = sep\n",
    "        AggStrat[i][\"TotalCount\"] = len(OPMDataMerged[OPMDataMerged[\"SEP\"]==sep])\n",
    "        AggStrat[i][\"p\"] = AggStrat[i][\"StratCount\"] / AggStrat[i][\"TotalCount\"]\n",
    "        AggStrat[i][\"StratCountSample\"] = StratCountSample\n",
    "        AggStrat[i][\"StratSampleSize\"] = round(AggStrat[i][\"p\"] * StratCountSample).apply(int)\n",
    "\n",
    "        display(AggStrat[i].head())\n",
    "        print(\"totalStratumSampleSize: \", AggStrat[i][\"StratSampleSize\"].sum())\n",
    "        #print(len(AggStrat[i]))\n",
    "    return AggStrat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SampleStrata(stratum, OPMDataMerged, FileName):\n",
    "    AggStrat = aggStratPop(stratum, OPMDataMerged)\n",
    "\n",
    "    SampledOPMStratumDataList = []\n",
    "\n",
    "    for i,StratSampleSize in enumerate(AggStrat):\n",
    "        SampledOPMStratumData = []\n",
    "        for j in range(0,len(StratSampleSize)):\n",
    "            SEP = StratSampleSize[\"SEP\"].ix[j]\n",
    "            DATECODE = StratSampleSize[\"DATECODE\"].ix[j]\n",
    "            AGELVL = StratSampleSize[\"AGELVL\"].ix[j]\n",
    "            SampleSize = StratSampleSize[\"StratSampleSize\"].ix[j]\n",
    "            print(SEP, DATECODE, AGELVL, SampleSize)\n",
    "\n",
    "            SampledOPMStratumDataList.append(OPMDataMerged[(OPMDataMerged[\"SEP\"]==SEP) \n",
    "                                                    & (OPMDataMerged[\"DATECODE\"]==DATECODE) \n",
    "                                                    & (OPMDataMerged[\"AGELVL\"]==AGELVL)].sample(SampleSize,  random_state=SampleSize))\n",
    "        SampledOPMStratumData.append(pd.concat(SampledOPMStratumDataList))\n",
    "        clear_display()\n",
    "    SampledOPMData = pd.concat(SampledOPMStratumData).reset_index()\n",
    "    del SampledOPMData[\"index\"]\n",
    "    pickleObject(SampledOPMData, FileName)\n",
    "    clear_display()\n",
    "\n",
    "    return SampledOPMData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a seed value equal to each strata sample size, we take random samples according to the computed sizes above. We loop through each Separation Type's Aggregated Strata Sample Sizes; Identify all observations matching on Datecode, Separation Type, and AgeLevel; and finally sample those observations with the computed sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "##Prof Data Sampling\n",
    "if os.path.isfile(PickleJarPath+\"/SampledOPMDataProf.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    SampledOPMDataProf = unpickleObject(\"SampledOPMDataProf\")\n",
    "else:\n",
    "    SampledOPMDataProf= SampleStrata(stratumProf, OPMDataMergedProf, \"SampledOPMDataProf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(SampledOPMDataProf))\n",
    "display(SampledOPMDataProf.head())\n",
    "display(pd.DataFrame({'StratCount' : SampledOPMDataProf.groupby([\"SEP\"]).size()}).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Analyze Missing Values\n",
    "filtered_msnoData = msno.nullity_sort(msno.nullity_filter(SampledOPMDataProf, filter='bottom', n=15, p=0.999), sort='descending')\n",
    "msno.matrix(filtered_msnoData)\n",
    "\n",
    "del filtered_msnoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "##Admin Data Sampling\n",
    "if os.path.isfile(PickleJarPath+\"/SampledOPMDataAdmin.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    SampledOPMDataAdmin = unpickleObject(\"SampledOPMDataAdmin\")\n",
    "else:\n",
    "    SampledOPMDataAdmin= SampleStrata(stratumAdmin, OPMDataMergedAdmin, \"SampledOPMDataAdmin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(SampledOPMDataAdmin))\n",
    "display(SampledOPMDataAdmin.head())\n",
    "display(pd.DataFrame({'StratCount' : SampledOPMDataAdmin.groupby([\"SEP\"]).size()}).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Analyze Missing Values\n",
    "filtered_msnoData = msno.nullity_sort(msno.nullity_filter(SampledOPMDataAdmin, filter='bottom', n=15, p=0.999), sort='descending')\n",
    "msno.matrix(filtered_msnoData)\n",
    "\n",
    "del filtered_msnoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Describe Summary for our Model Professional Subgroup for Modeling\n",
    "display(SampledOPMDataProf.describe().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "#OPMDataMerged.to_csv(\"OPMDataMerged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os.path.getsize(\"OPMDataMerged.csv\") #Display file size in bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Visualizations post-Data removal and sampling\n",
    "\n",
    "Chris... can you use the SampledOPMDataProf dataset, and re-run the Visuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SampledOPMDataProf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "cols = list(SampledOPMDataProf.select_dtypes(include=['float64', 'int64']))\n",
    "cols.remove('BLS_FEDERAL_OtherSep_Rate')\n",
    "cols.remove('BLS_FEDERAL_Quits_Rate')\n",
    "cols.remove('BLS_FEDERAL_TotalSep_Level')\n",
    "cols.remove('BLS_FEDERAL_JobOpenings_Rate')\n",
    "cols.remove('BLS_FEDERAL_OtherSep_Level')\n",
    "cols.remove('BLS_FEDERAL_Quits_Level')\n",
    "cols.remove('BLS_FEDERAL_JobOpenings_Level')\n",
    "cols.remove('BLS_FEDERAL_Layoffs_Rate')\n",
    "cols.remove('BLS_FEDERAL_Layoffs_Level')\n",
    "cols.remove('BLS_FEDERAL_TotalSep_Rate')\n",
    "cols.append('SEP')\n",
    "display(cols)\n",
    "\n",
    "plotNumeric = SampledOPMDataProf[cols]\n",
    "\n",
    "# Create binary separation attribute for EDA correlation review\n",
    "#plotNumeric[\"SEP_bin\"] = plotNumeric.SEP.replace(\"NS\", 1)\n",
    "#plotNumeric.loc[plotNumeric['SEP_bin'] != 1, 'SEP_bin'] = 0\n",
    "#plotNumeric.SEP_bin = plotNumeric.SEP_bin.apply(pd.to_numeric)\n",
    "AttSplit = pd.get_dummies(plotNumeric['SEP'],prefix='SEP')\n",
    "display(AttSplit.head())\n",
    "plotNumeric = pd.concat((plotNumeric,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "display(plotNumeric.head())\n",
    "print(\"plotNumeric has {0} Records\".format(len(plotNumeric)))\n",
    "#print(plotNumeric.SEP_bin.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "sns.pairplot(plotNumeric.drop(['SEP_NS',\n",
    "                               'SEP_SA',\n",
    "                               'SEP_SC',\n",
    "                               'SEP_SD',\n",
    "                               'SEP_SH', \n",
    "                               'SEP_SI'], axis=1), hue = 'SEP', palette=\"hls\", plot_kws={\"s\": 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function modified from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n",
    "sns.set()\n",
    "\n",
    "def draw_histograms(df, variables, n_rows, n_cols):\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        df[var_name].hist(bins=20,ax=ax, color='#58D68D')\n",
    "        ax.set_title(var_name+\" Distribution\")\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "draw_histograms(plotNumeric.drop(['SEP',\n",
    "                                  'SEP_NS',\n",
    "                                  'SEP_SA',\n",
    "                                  'SEP_SC',\n",
    "                                  'SEP_SD',\n",
    "                                  'SEP_SH', \n",
    "                                  'SEP_SI'], axis=1),\n",
    "                plotNumeric.drop(['SEP',\n",
    "                                  'SEP_NS',\n",
    "                                  'SEP_SA',\n",
    "                                  'SEP_SC',\n",
    "                                  'SEP_SD',\n",
    "                                  'SEP_SH',\n",
    "                                  'SEP_SI'], axis=1).columns, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Inspired by http://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "#plt.matshow(plotNumeric.corr())\n",
    "\n",
    "sns.set(style='white')\n",
    "corr = plotNumeric.drop(['SEP'], axis=1).corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.set(font_scale=0.95)\n",
    "heatCorr = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "                       square=True, annot=True, linewidths=1,\n",
    "                       cbar_kws={\"shrink\": .5}, ax=ax, fmt='.1g')\n",
    "#heatCorr.\n",
    "ax.tick_params(labelsize=15)\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.tick_params(labelsize=15)\n",
    "\n",
    "sns.plt.show()\n",
    "#sns.heatmap(corr, annot=True, linewidths=0.01, cmap=cmap, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cols = list(SampledOPMDataProf.select_dtypes(include=['object']))\n",
    "dropCols = [\"LOCTYP\",\n",
    "            \"LOCTYPT\",\n",
    "            \"OCCTYP\",\n",
    "            \"OCCTYPT\",\n",
    "            \"PPTYP\",\n",
    "            \"PPTYPT\",\n",
    "            \"AGYTYP\",\n",
    "            \"OCCFAM\",\n",
    "            \"PPGROUP\",\n",
    "            \"PAYPLAN\",\n",
    "            \"TOATYP\",\n",
    "            \"WSTYP\",\n",
    "            \"AGYSUBT\",\n",
    "            \"AGELVL\",\n",
    "            \"LOSLVL\",\n",
    "            \"LOC\",\n",
    "            \"OCC\",\n",
    "            \"PATCO\",\n",
    "            \"SALLVL\",\n",
    "            \"TOA\",\n",
    "            \"WORKSCH\"]\n",
    "\n",
    "for i in dropCols:\n",
    "    if(i in list(SampledOPMDataProf.columns)): cols.remove(i)\n",
    "\n",
    "plotCat = SampledOPMDataProf[cols]\n",
    "display(plotCat.head())\n",
    "print(\"plotCat Has {0} Records\".format(len(plotCat)))\n",
    "print(\"Number of colums = \", len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in cols:\n",
    "    if i != 'SEP':\n",
    "        plt.figure(i) # Required to create new figure each loop rather than drawing over previous object\n",
    "        f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10), sharey=False)\n",
    "        sns.countplot(y=i, data=plotCat, color=\"lightblue\", ax=ax1);\n",
    "        sns.countplot(y=i, data=plotCat, hue=\"SEP\", palette=\"hls\", ax=ax2);\n",
    "        \n",
    "    if i == 'AGYSUB':\n",
    "        subCountPlot(i, 'SEP', 10000)\n",
    "    elif i == 'LOCT':\n",
    "        subCountPlot(i, 'SEP', 1000)\n",
    "    elif i == 'OCCT':\n",
    "        subCountPlot(i, 'SEP', 2000)\n",
    "    elif i == 'PPGRD':\n",
    "        subCountPlot(i, 'SEP', 6000)\n",
    "    elif i == 'AGYT':\n",
    "        subCountPlot(i, 'SEP', 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in cols:\n",
    "    if i != 'SEP':\n",
    "        percBarPlot(i, 'SEP', len(plotCat.SEP.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "\n",
    "sns.violinplot(x=\"PATCOT\", y=\"SALARY\", data=SampledOPMDataProf, split=True,\n",
    "               inner=\"quart\")\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"SEP\", y=\"SALARY\", data=SampledOPMDataProf, split=True,\n",
    "               inner=\"quart\")\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#sns.factorplot(x=\"SEP\", y=\"SALARY\", col=\"PATCOT\",\n",
    "#               data=SampledOPMDataProf,\n",
    "#               kind=\"violin\", split=True, aspect=.5, size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#sns.factorplot(x=\"SEP\", y=\"SALARY\", col=\"PATCOT\", data=SampledOPMDataProf,\n",
    "#               kind=\"violin\", split=True, aspect=.4, size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "g = sns.PairGrid(data=SampledOPMDataProf,\n",
    "                 x_vars=[\"SEP\",\"PATCOT\"],\n",
    "                 y_vars=[\"SALARY\", \"LOS\", \"LowerLimitAge\", \"YearsToRetirement\"],\n",
    "                 aspect=1, size=10)\n",
    "g.map(sns.violinplot, palette=\"pastel\", inner=\"quart\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(plotNumeric, plotCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Attributes, and Remove Description Columns for Analysis Prep\n",
    "\n",
    "Now that we have the dataset sampled, we still have some legwork necessary to convert our categorical attributes into binary integer values. Below we walk through this process for the following Attributes:\n",
    "- AGELVL\n",
    "- LOC\n",
    "- SALLVL\n",
    "- TOA\n",
    "- OCCTYP\n",
    "- OCCFAM\n",
    "- PPTYP\n",
    "- PPGROUP\n",
    "- TOATYP\n",
    "\n",
    "Once these attributes have been encoded and description columns removed, we end up with a total of 2446 attributes in our dataset for analysis in our model generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up old objects no longer needed, to clear up memory\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Memory Usage before Cleanup: \", process.memory_info().rss)\n",
    "\n",
    "if 'AGELVL' in dir():\n",
    "    del AGELVL\n",
    "if 'AggIndAvgSalary' in dir():\n",
    "    del AggIndAvgSalary\n",
    "if 'AggIndAvgSalary2' in dir():\n",
    "    del AggIndAvgSalary2\n",
    "if 'AggSEPCount_EFDATE_LOC' in dir():\n",
    "    del AggSEPCount_EFDATE_LOC\n",
    "if 'AggSEPCount_EFDATE_OCC' in dir():\n",
    "    del AggSEPCount_EFDATE_OCC\n",
    "if 'AggStrat' in dir():\n",
    "    del AggStrat\n",
    "if 'DATECODE' in dir():\n",
    "    del DATECODE\n",
    "if 'EMPColList' in dir():\n",
    "    del EMPColList\n",
    "if 'EMPDataOrig4Q' in dir():\n",
    "    del EMPDataOrig4Q\n",
    "if 'maxSize' in dir():\n",
    "    del maxSize\n",
    "if 'OPMColList' in dir():\n",
    "    del OPMColList\n",
    "if 'OPMDataFiles' in dir():\n",
    "    del OPMDataFiles\n",
    "if 'OPMDataList' in dir():\n",
    "    del OPMDataList\n",
    "if 'OPMDataMerged' in dir():\n",
    "    del OPMDataMerged\n",
    "if 'OPMDataOrig' in dir():\n",
    "    del OPMDataOrig\n",
    "if 'SEP' in dir():\n",
    "    del SEP\n",
    "if 'SampleSize' in dir():\n",
    "    del SampleSize\n",
    "if 'SampledOPMStratumData' in dir():\n",
    "    del SampledOPMStratumData\n",
    "if 'SampledOPMStratumDataList' in dir():\n",
    "    del SampledOPMStratumDataList\n",
    "if 'StratCountSample' in dir():\n",
    "    del StratCountSample\n",
    "if 'StratSampleSize' in dir():\n",
    "    del StratSampleSize\n",
    "if 'JTL' in dir():\n",
    "    del JTL\n",
    "    \n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Memory Usage after Cleanup: \", process.memory_info().rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SampledOPMDataProf.head())\n",
    "SampledOPMDataProf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.isfile(PickleJarPath+\"/OPMAnalysisDataNoFam.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    OPMAnalysisDataNoFam = unpickleObject(\"OPMAnalysisDataNoFam\")\n",
    "else:\n",
    "\n",
    "    OPMAnalysisDataNoFam = SampledOPMDataProf.copy()\n",
    "\n",
    "    cols = [\"GENDER\",\n",
    "            \"DATECODE\",\n",
    "            \"QTR\",\n",
    "            \"COUNT\",\n",
    "            \"AGYTYPT\",\n",
    "            \"AGYT\",\n",
    "            \"AGYSUB\",\n",
    "            \"AGYSUBT\",\n",
    "            \"QTR\",\n",
    "            \"AGELVLT\",\n",
    "            \"LOSLVL\",\n",
    "            \"LOSLVLT\",\n",
    "            \"LOCTYPT\",\n",
    "            \"LOCT\",\n",
    "            \"OCCTYP\",\n",
    "            \"OCCTYPT\",\n",
    "            \"OCCFAM\",\n",
    "            \"OCCFAMT\",\n",
    "            \"OCC\",\n",
    "            \"OCCT\",\n",
    "            \"PATCO\",\n",
    "            \"PPGRD\",\n",
    "            \"PATCOT\",\n",
    "            \"PPTYPT\",\n",
    "            \"PPGROUPT\",\n",
    "            \"PAYPLAN\",\n",
    "            \"PAYPLANT\",\n",
    "            \"SALLVLT\",\n",
    "            \"TOATYPT\",\n",
    "            \"TOAT\",\n",
    "            \"WSTYP\",\n",
    "            \"WSTYPT\",\n",
    "            \"WORKSCH\",\n",
    "            \"WORKSCHT\",\n",
    "            \"SALARY\",\n",
    "            \"LOS\",\n",
    "            \"SEPCount_EFDATE_OCC\",\n",
    "            \"SEPCount_EFDATE_LOC\"\n",
    "           ]\n",
    "\n",
    "\n",
    "\n",
    "    #delete cols from analysis data\n",
    "    for col in cols:\n",
    "        if col in list(OPMAnalysisDataNoFam.columns):\n",
    "            del OPMAnalysisDataNoFam[col]\n",
    "\n",
    "    OPMAnalysisDataNoFam.info()\n",
    "\n",
    "    cols = [\"AGELVL\",\n",
    "            \"LOC\",\n",
    "            \"SALLVL\",\n",
    "            \"TOA\",\n",
    "            \"AGYTYP\",\n",
    "            \"AGY\",\n",
    "            \"LOCTYP\",\n",
    "            \"PPTYP\",\n",
    "            \"PPGROUP\",\n",
    "            \"TOATYP\"\n",
    "           ]\n",
    "\n",
    "    #Split Values for cols \n",
    "    for col in cols:\n",
    "        if col in list(OPMAnalysisDataNoFam.columns):\n",
    "            AttSplit = pd.get_dummies(OPMAnalysisDataNoFam[col],prefix=col)\n",
    "            display(AttSplit.head())\n",
    "            OPMAnalysisDataNoFam = pd.concat((OPMAnalysisDataNoFam,AttSplit),axis=1) # add back into the dataframe\n",
    "            del OPMAnalysisDataNoFam[col]\n",
    "\n",
    "    pickleObject(OPMAnalysisDataNoFam, \"OPMAnalysisData\")\n",
    "        \n",
    "display(OPMAnalysisDataNoFam.head())\n",
    "print(\"Number of Columns: \",len(OPMAnalysisDataNoFam.columns))\n",
    "OPMAnalysisDataNoFam.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a display of all remaining attributes and their corresponding data types for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_type = []\n",
    "for idx, col in enumerate(OPMAnalysisDataNoFam.columns):\n",
    "    data_type.append(OPMAnalysisDataNoFam.dtypes[idx])\n",
    "\n",
    "summary_df = {'Attribute Name' : pd.Series(OPMAnalysisDataNoFam.columns, index = range(len(OPMAnalysisDataNoFam.columns))), 'Data Type' : pd.Series(data_type, index = range(len(OPMAnalysisDataNoFam.columns)))}\n",
    "summary_df = pd.DataFrame(summary_df)\n",
    "display(summary_df)\n",
    "\n",
    "del data_type, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction using Principal Component Analysis\n",
    "\n",
    "We also scale the data values to remove bias in our models due to different attribute scales. Without scaling the data, attributes such as SALARY and LOS would carry heavier weights when compared against the binary encoded attributes and BLS data. This would cause unbalanced and improperly analyzed data for model creation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "OPMScaledAnalysisData = OPMAnalysisDataNoFam.copy()\n",
    "del OPMScaledAnalysisData[\"SEP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "OPMAnalysisScalerFit = MinMaxScaler().fit(OPMScaledAnalysisData)\n",
    "## Pickle for later re-use if needed\n",
    "pickleObject(OPMAnalysisScalerFit, \"OPMAnalysisScalerFit\")\n",
    "\n",
    "OPMScaledAnalysisData = pd.DataFrame(OPMAnalysisScalerFit.transform(OPMScaledAnalysisData), columns = OPMScaledAnalysisData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(OPMScaledAnalysisData.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Principal Components defined\n",
    "\n",
    "Our objective, is to reduce dimensionality through identification of principal components. We have chosen 100 as the maximum number of components to be produced, given our hopes are to reduce the number of attributes needed for a model. We will review each component's explained variance further to determine the proper number of components to be included later during model generation. Note randomized PCA was chosen in order to use singular value decomposition in our dimensionality reduction efforts due to the large size of our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seed = len(OPMScaledAnalysisData)\n",
    "\n",
    "print(OPMScaledAnalysisData.shape)\n",
    "pca_class = PCA(n_components=len(OPMScaledAnalysisData.columns), svd_solver='randomized', random_state=seed)\n",
    "\n",
    "pca_class.fit(OPMScaledAnalysisData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the resulting components have been ordered by eigenvector value and these values portrayed as ratios of variance explained by each component. In order to identify the principal components to be included during model generation, we review the rate at which explained variance decreases in significance from one principal component to the next. Accompanying these proportion values is a scree plot representing these same values in visual form. By plotting the scree plot, it is easier to judge where this rate of decreasing explained variance occurs. Note the rate of change in explained variance among the first 8 principal components, with another less significant change through the 22th component. After the 22th component, the rate of decreasing explained variance begins to somewhat flatten out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#The amount of variance that each PC explains\n",
    "var= pca_class.explained_variance_ratio_\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "plt.plot(range(1,len(OPMScaledAnalysisData.columns)+1), var*100, marker = '.', color = 'red', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.axis([0, len(OPMScaledAnalysisData.columns)+1, -0.1, 9])\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(var, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now referring to the cumulative variance values and associated plot below, it may be seen that the cumulative variance increases in a fairly consistent parabola curve. In attempts to acheive a cumulative variance explained of greater than 80%, we end at 22 principal components. For this reason, 22 principal components may be selected as being the most appropriate for separation classification modeling given the variables among these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca_class.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "plt.plot(range(1,len(OPMScaledAnalysisData.columns)+1), var1, marker = '.', color = 'green', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance (Sum %)')\n",
    "plt.title('Cumulative Variance Plot')\n",
    "plt.axis([0, len(OPMScaledAnalysisData.columns)+1, 10, len(OPMScaledAnalysisData.columns)+1])\n",
    "\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to analyze the first 4 component Feature Loadings more carefully. See below, plots of the top 10 loadings for each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 12)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,4):\n",
    "    components = pd.Series(pca_class.components_[i], index=OPMScaledAnalysisData.columns)\n",
    "\n",
    "    maxcomponent = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(10)).index)\n",
    "\n",
    "    matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(2,2,i + 1)\n",
    "       \n",
    "    weightsplot = pd.Series(components, index=maxcomponent)\n",
    "    weightsplot.plot(title = \"Principal Component \"+ str(i+1), kind='bar', color = 'Tomato', ax = ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxPC = 22\n",
    "\n",
    "PCList = []\n",
    "for i in range(0,MaxPC):\n",
    "    components = pd.Series(pca_class.components_[i], index=OPMScaledAnalysisData.columns)\n",
    "\n",
    "    maxcomponent = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "    PCList.append(maxcomponent)\n",
    "\n",
    "PCList = pd.concat(PCList).drop_duplicates().sort_values(ascending=True).reset_index(drop = True)\n",
    "print(PCList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total of 48 features of the original 98 are identified, by taking the top 15 feature loadings within the first 22 components as determined above as the appropriate components to maximize variance explained. We may now, optionally utilize these 48 features identified, or utilize principal component vectors for analysis in the next steps.\n",
    "\n",
    "We take this list of columns, and add SEP to the list for use in our model analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCList = list(PCList.append(pd.Series(\"SEP\")).sort_values(ascending=True).reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Separation Response Weights\n",
    "Due to the unproportional number of observations in each separation type in our dataset, we need to create weightings. using SciKit's class_weight algorithm, we compute an array of weights to be used downstream in our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPMClassWeights = class_weight.compute_class_weight(\"balanced\", OPMAnalysisDataNoFam[\"SEP\"].drop_duplicates(), OPMAnalysisDataNoFam[\"SEP\"])\n",
    "\n",
    "display(stratumProf)\n",
    "display(pd.DataFrame({\"Weight\": OPMClassWeights, \"SEP\": OPMAnalysisDataNoFam[\"SEP\"].drop_duplicates()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Separation\n",
    "We have chosen to utilize Stratified KFold Cross Validation for our classification analysis, with 10 folds. This means, that from our original sample size of 16,638, each \"fold\" will save off approximately 10% as test observations utilizing the rest as training observations all while keeping the ratio of classes equal amongst customers and subscribers. This process will occur through 10 iterations, or folds, to allow us to cross validate our results amongst different test/train combinations. We have utilized a random_state seed equal to the length of the original sampled dataset to ensure reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = len(OPMAnalysisDataNoFam)\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 10, random_state = seed)\n",
    "print(OPMAnalysisDataNoFam.shape)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classification\n",
    "\n",
    "**Max Depth**\n",
    "The maximum depth (levels) in the tree. When a value is set, the tree may not split further once this level has been met regardless of how many nodes are in the leaf. \n",
    "\n",
    "**Max Features**\n",
    "Number of features to consider when looking for a split. \n",
    "\n",
    "**Minimum Samples in Leaf**\n",
    "Minimum number of samples required to be in a leaf node. Splits may not occur which cause the number of samples in a leaf to be less than this value. Too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**Minimum Samples to Split**\n",
    "Minimum number fo samples required to split a node. Care was taken during parameter tests to keep the ratio between Min Samples in Leaf and Min Samples to Split equal to that of the default values (1:2). This was done to allow an even 50/50 split on nodes which match the lowest granularity split criteria. similar to the min samples in leaf, too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**n_estimators**\n",
    "Number of Trees generated in the forest. Increasing the number of trees, in our models increased accuracy while decreasing performance. We tuned to provide output that completed all 10 iterations in under 10 minutes.\n",
    "\n",
    "###Not Complete#### After 13 iterations of modifying the above parameters, we land on a final winner based on the highest average Accuracy value across all iterations. Average Accuracy values in our 10 test/train iterations ranged from 70.2668 % from default inputs of the random forest classification model to a value of 72.5192 % in the best tuned model fit. Although the run-time of this model parameter choice is the largest performed, we decided to remain with these inputs due to the amount increase in accuracy. As mentioned previously, we tuned the n_estimators parameter to ensure we stayed under 10 minutes execution. Parameter inputs for the final Random Forest Classification model with the KD Tree Algorithm are as follows: ###Not Complete#### \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>n_estimators</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>TBD</th>\n",
    "      <td>TBD</td>\n",
    "      <td>TBD</td>\n",
    "      <td>TBD</td>\n",
    "      <td>TBD</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def rfc_explor(n_estimators,\n",
    "               max_features,\n",
    "               max_depth, \n",
    "               min_samples_split,\n",
    "               min_samples_leaf,\n",
    "               Data        = OPMAnalysisDataNoFam,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data['SEP'].values # get the labels we want    \n",
    "    \n",
    "    X = ScaledData\n",
    "    \n",
    "    rfc_clf = RandomForestClassifier(n_estimators=n_estimators, max_features = max_features, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, n_jobs=-1, random_state = seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler', MinMaxScaler()),\n",
    "         ('CLF',rfc_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10  , 5    , 15   ]  \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "\n",
    "for i in range(0,len(n_estimators)):\n",
    "    acclist.append(rfc_explor(n_estimators      = n_estimators[i],\n",
    "                              max_features      = max_features[i],\n",
    "                              max_depth         = max_depth[i],\n",
    "                              min_samples_split = min_samples_split[i],\n",
    "                              min_samples_leaf  = min_samples_leaf[i]\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"n_estimators\": n_estimators,          \n",
    "                                                \"max_features\": max_features,         \n",
    "                                                \"max_depth\": max_depth,        \n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
