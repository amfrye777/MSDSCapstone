{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPM Data Separation Analysis\n",
    "<i><b>\n",
    "Christopher Boomhower<sub>1</sub>, Stacey Fabricant<sub>2</sub>, Alex Frye<sub>1</sub>, David Mumford<sub>2</sub>, Michael Smith<sub>1</sub>, Lindsay Vitovsky<sub>1</sub>\n",
    "\n",
    "<sub>1</sub> Southern Methodist University, Dallas, TX, US\n",
    "<sub>2</sub> Penn Mutual Life Insurance Co, Horsham PA\n",
    "</i></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "background text...\n",
    "\n",
    "**our intent is to: 1)..2)...3)........**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "Data Source Background Text & citation links\n",
    "\n",
    "Dataset Attribute Descriptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "To begin our analysis, we need to load the data from our 89 source .txt files. Data is separated into two separate groups of files; Separation and Non-Separation, thus data is loaded in two separate phases, then unioned together. Once data is loaded, Steps taken to remove non-US observations or those with no specified occupation, no specified salary, or no specified length of service level.  Of a total 8,423,336 observations, we end with 8,232,375 after removal of these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import pickle\n",
    "import os\n",
    "import psutil\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import missingno as msno\n",
    "import prettytable\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy import interp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from itertools import cycle\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.feature_selection import chi2\n",
    "import itertools\n",
    "\n",
    "#Need to make sure you install the rpy2 package via following command in the Putty genuse41 console:\n",
    "#python3 /usr/bin/pip install --user rpy2\n",
    "#NOTE: If the above pip install does not work, try the following instead:\n",
    "#python3 /usr/local/es7/lib/python3.5/site-packages/pip install --user rpy2\n",
    "%load_ext rpy2.ipython\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "\n",
    "## Library Options\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pre-defined Functions for use later\n",
    "def pickleObject(objectname, filename, filepath = \"PickleJar/\"):\n",
    "    fullpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create a variable to pickle and open it in write mode\n",
    "    picklefile = open(fullpicklepath, 'wb')\n",
    "    pickle.dump(objectname, picklefile)\n",
    "    picklefile.close()\n",
    "    \n",
    "def unpickleObject(filename, filepath = \"PickleJar/\"):\n",
    "    fullunpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create an variable to pickle and open it in write mode\n",
    "    unpicklefile = open(fullunpicklepath, 'rb')\n",
    "    unpickleObject = pickle.load(unpicklefile)\n",
    "    unpicklefile.close()\n",
    "\n",
    "    return unpickleObject\n",
    "    \n",
    "def clear_display():\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "## Pre-defined variables for use later\n",
    "dataOPMPath = \"dataOPM\"\n",
    "dataEMPPath = \"dataEMP\"\n",
    "PickleJarPath = \"PickleJar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Load OPMSeparation Files\n",
    "\n",
    "OPMDataFiles = glob.glob(os.path.join(dataOPMPath, \"*.txt\"))\n",
    "\n",
    "for i in range(0,len(OPMDataFiles)):\n",
    "    OPMDataFiles[i] = OPMDataFiles[i].replace(\"\\\\\",\"/\")\n",
    "\n",
    "OPMDataList = []\n",
    "\n",
    "for i,j in zip(OPMDataFiles,range(0,len(OPMDataFiles))):\n",
    "    OPMDataList.append(pd.read_csv(i, dtype = 'str'))\n",
    "    display(OPMDataList[j].head())\n",
    "\n",
    "## Load the SEPDATA_FY2015 file into it's own object\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/SEPDATA_FY2015.txt']\n",
    "OPMDataOrig = OPMDataList[indexes[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#print(OPMDataFiles)\n",
    "\n",
    "print(len(OPMDataOrig))\n",
    "\n",
    "##### Merge / Modify Codes / Aggregate Attributes to be more descriptive per the metadata files\n",
    "\n",
    "OPMDataMerged = OPMDataOrig.copy()\n",
    "\n",
    "##AGYSUB - AGYTYP, AGY\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTagy.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'AGYSUB', how = 'left')\n",
    "\n",
    "##EFDate - quarter, month\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTefdate.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'EFDATE', how = 'left')\n",
    "\n",
    "##AGELVL - AGELVLT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTagelvl.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'AGELVL', how = 'left')\n",
    "\n",
    "##LOSLVL - LOSLVLT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTloslvl.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'LOSLVL', how = 'left')\n",
    "\n",
    "##LOC - LocTypeT, LocT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTloc.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'LOC', how = 'left')\n",
    "\n",
    "##OCC - OCCTYPT, OCCFAM\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTocc.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'OCC', how = 'left')\n",
    "\n",
    "##PATCO - PATCOT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTpatco.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'PATCO', how = 'left')\n",
    "\n",
    "##PPGRD - PayPlan, PPGroup, PPTYP\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTppgrd.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'PPGRD', how = 'left')\n",
    "\n",
    "##SALLVL - SALLVLT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTsallvl.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'SALLVL', how = 'left')\n",
    "\n",
    "##TOA - TOATYP\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTtoa.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'TOA', how = 'left')\n",
    "\n",
    "##WORKSCH - WSTYPT\n",
    "indexes = [i for i,x in enumerate(OPMDataFiles) if x == 'dataOPM/DTwrksch.txt']\n",
    "OPMDataMerged = OPMDataMerged.merge(OPMDataList[indexes[0]], on = 'WORKSCH', how = 'left')\n",
    "\n",
    "\n",
    "## Modify Data Types for numeric objects\n",
    "OPMDataMerged[\"SALARY\"] = OPMDataMerged[\"SALARY\"].apply(pd.to_numeric)\n",
    "OPMDataMerged[\"COUNT\"]  = OPMDataMerged[\"COUNT\"].apply(pd.to_numeric)\n",
    "OPMDataMerged[\"LOS\"]    = OPMDataMerged[\"LOS\"].apply(pd.to_numeric)\n",
    "\n",
    "print(\"Original SEP data size of: \"+str(len(OPMDataMerged)))\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"LOCTYP\"] != \"1\"]))+\" Non-US observations.\")\n",
    "\n",
    "## Remove Non-US Data\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"LOCTYP\"] == \"1\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"OCCTYP\"] == \"3\"]))+\" observations with no specified Occupation.\")\n",
    "\n",
    "   ## Remove Observations with no specified occupation\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"OCCTYP\"] != \"3\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"SALLVL\"] == \"Z\"]))+\" observations with no specified Salary.\")\n",
    "\n",
    "   ## Remove Observations with no specified salary\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"SALLVL\"] != \"Z\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"LOSLVL\"] == \"Z\"]))+\" observations with no specified Length of Service.\")\n",
    "\n",
    "   ## Remove Observations with no specified LOSLVL\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"LOSLVL\"] != \"Z\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"AGELVL\"] == \"A\"]))+\" observations of Age Level A\")\n",
    "\n",
    "## Remove Observations from Age Level A (less than 20 years old)\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"AGELVL\"] != \"A\"]\n",
    "\n",
    "print(\"Removing \"+str(len(OPMDataMerged[OPMDataMerged[\"AGELVL\"] == \"Z\"]))+\" observations with no specified Age Level.\")\n",
    "\n",
    "   ## Remove Observations with no specified Age Level\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"AGELVL\"] != \"Z\"]\n",
    "\n",
    "    ## Fix differences in spaces on WORKSCHT Column\n",
    "OPMDataMerged[\"WORKSCHT\"] = np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"F\", 'Full-time Nonseasonal',\n",
    "                                np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"I\", 'Intermittent Nonseasonal',\n",
    "                                         np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"P\", 'Part-time Nonseasonal',\n",
    "                                                  np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"G\", 'Full-time Seasonal',\n",
    "                                                        np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"J\", 'Intermittent Seasonal',\n",
    "                                                                np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"Q\", 'Part-time Seasonal',\n",
    "                                                                        np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"T\", 'Part-time Job Sharer Seasonal',\n",
    "                                                                                np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"S\", 'Part-time Job Sharer Nonseasonal',\n",
    "                                                                                        np.where(OPMDataMerged[\"WORKSCHT\"].str[0]==\"B\", 'Full-time Nonseasonal Baylor Plan',\n",
    "                                                                                                'NO WORK SCHEDULE REPORTED' ### ELSE case represents Night\n",
    "                                                                                                 )\n",
    "                                                                                         )\n",
    "                                                                                 )\n",
    "                                                                         )\n",
    "                                                                 )\n",
    "                                                          )\n",
    "                                                 )\n",
    "                                        )\n",
    "                               )    \n",
    "\n",
    "display(OPMDataMerged.head())\n",
    "print(\"New SEP data size of: \"+str(len(OPMDataMerged)))\n",
    "display(OPMDataMerged.describe().transpose())\n",
    "#del OPMDataList,OPMDataFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.isfile(PickleJarPath+\"/EMPDataOrig4Q.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    EMPDataOrig4Q = unpickleObject(\"EMPDataOrig4Q\")\n",
    "else:\n",
    "    ## Load EMPData Files\n",
    "\n",
    "    indexes = []\n",
    "    EMPDataFiles = []\n",
    "    EMPDataList = []\n",
    "    EMPDataOrig = []\n",
    "\n",
    "    for i,qtr in enumerate([\"Q1\", \"Q2\", \"Q3\", \"Q4\"]): \n",
    "        EMPDataFiles.append(glob.glob(os.path.join(dataEMPPath, qtr + \"/*.txt\")))\n",
    "\n",
    "        for j in range(0,len(EMPDataFiles[i])):\n",
    "            EMPDataFiles[i][j] = EMPDataFiles[i][j].replace(\"\\\\\",\"/\")\n",
    "\n",
    "        EMPDataList.append([])\n",
    "\n",
    "        for j,file in enumerate(EMPDataFiles[i]):\n",
    "            EMPDataList[i].append(pd.read_csv(file, dtype = 'str'))\n",
    "            if i == 0:\n",
    "                display(EMPDataList[i][j].head())\n",
    "\n",
    "        ## Load the FactData files into it's own object\n",
    "        indexes.append([])\n",
    "            ##[qtr][fileindex from EMPDataList]\n",
    "        indexes[i]=[j for j,x in enumerate(EMPDataFiles[i]) if dataEMPPath + '/' + qtr + '/FACTDATA' in x]   \n",
    "\n",
    "        EMPDataOrig.append([])\n",
    "\n",
    "        EMPDataOrig[i] = pd.concat([EMPDataList[i][indexes[i][j]] for j in range(0,len(indexes[i]))]) \n",
    "        EMPDataOrig[i][\"QTR\"] = str(i+1)\n",
    "\n",
    "            ## modify data type for numerics\n",
    "        EMPDataOrig[i][\"SALARY\"] = EMPDataOrig[i][\"SALARY\"].str.replace(',', '').str.replace('$', '').str.replace(' ', '').apply(pd.to_numeric)\n",
    "      \n",
    "        ## Load Metadata\n",
    "        ##AGYSUB - AGYTYP, AGY\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTagy.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'AGYSUB', how = 'left')\n",
    "\n",
    "        ##AGELVL - AGELVLT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTagelvl.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'AGELVL', how = 'left')\n",
    "\n",
    "        #LOSLVL - LOSLVLT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTloslvl.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'LOSLVL', how = 'left')\n",
    "        EMPDataOrig[i][\"LOS\"] = EMPDataOrig[i][\"LOS\"].apply(pd.to_numeric)\n",
    "        \n",
    "        ##LOC - LocTypeT, LocT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTloc.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'LOC', how = 'left')\n",
    " \n",
    "        ##OCC - OCCTYPT, OCCFAM\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTocc.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'OCC', how = 'left')\n",
    "\n",
    "        ##PATCO - PATCOT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTpatco.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'PATCO', how = 'left')\n",
    "\n",
    "        ##PPGRD - PayPlan, PPGroup, PPTYP\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTppgrd.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'PPGRD', how = 'left')\n",
    "\n",
    "        ##SALLVL - SALLVLT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTsallvl.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'SALLVL', how = 'left')\n",
    "\n",
    "        ##TOA - TOATYP\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTtoa.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'TOA', how = 'left')\n",
    "\n",
    "        ##WORKSCH - WSTYPT\n",
    "        ind2 = [i for i,x in enumerate(EMPDataFiles[i]) if x == dataEMPPath + '/' + qtr + '/DTwrksch.txt']\n",
    "        EMPDataOrig[i] = EMPDataOrig[i].merge(EMPDataList[i][ind2[0]], on = 'WORKSCH', how = 'left')\n",
    "\n",
    "        display(EMPDataOrig[i].head())\n",
    "\n",
    "    EMPDataOrig4Q = pd.concat([EMPDataOrig[j] for j in range(0,len(EMPDataOrig))])\n",
    "    print(\"Original EMP data size of: \"+str(len(EMPDataOrig4Q)))\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"LOCTYP\"] != \"1\"]))+\" Non-US observations.\")\n",
    "    \n",
    "       ## Remove Non-US Data\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"LOCTYP\"] == \"1\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"OCCTYP\"] == \"3\"]))+\" observations with no specified Occupation.\")\n",
    "\n",
    "       ## Remove Observations with no specified occupation\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"OCCTYP\"] != \"3\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"SALLVL\"] == \"Z\"]))+\" observations with no specified Salary.\")\n",
    "\n",
    "       ## Remove Observations with no specified salary\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"SALLVL\"] != \"Z\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"LOSLVL\"] == \"Z\"]))+\" observations with no specified Length of Service.\")\n",
    "\n",
    "       ## Remove Observations with no specified LOSLVL\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"LOSLVL\"] != \"Z\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] == \"A\"]))+\" observations of Age Level A.\")\n",
    "\n",
    "        ## Remove Observations from Age Level A (less than 20 years old)\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] != \"A\"]\n",
    "\n",
    "    print(\"Removing \"+str(len(EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] == \"Z\"]))+\" observations with no specified Age Level.\")\n",
    "\n",
    "        ## Remove Observations with no specified Age Level\n",
    "    EMPDataOrig4Q = EMPDataOrig4Q[EMPDataOrig4Q[\"AGELVL\"] != \"Z\"]\n",
    "\n",
    "        ## Fix differences in spaces on WORKSCHT Column\n",
    "    EMPDataOrig4Q[\"WORKSCHT\"] = np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"F\", 'Full-time Nonseasonal',\n",
    "                                    np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"I\", 'Intermittent Nonseasonal',\n",
    "                                             np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"P\", 'Part-time Nonseasonal',\n",
    "                                                      np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"G\", 'Full-time Seasonal',\n",
    "                                                            np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"J\", 'Intermittent Seasonal',\n",
    "                                                                    np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"Q\", 'Part-time Seasonal',\n",
    "                                                                            np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"T\", 'Part-time Job Sharer Seasonal',\n",
    "                                                                                    np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"S\", 'Part-time Job Sharer Nonseasonal',\n",
    "                                                                                            np.where(EMPDataOrig4Q[\"WORKSCHT\"].str[0]==\"B\", 'Full-time Nonseasonal Baylor Plan',\n",
    "                                                                                                    'NO WORK SCHEDULE REPORTED' ### ELSE case represents Night\n",
    "                                                                                                     )\n",
    "                                                                                             )\n",
    "                                                                                     )\n",
    "                                                                             )\n",
    "                                                                     )\n",
    "                                                              )\n",
    "                                                     )\n",
    "                                            )\n",
    "                                   )    \n",
    "\n",
    "    pickleObject(EMPDataOrig4Q, \"EMPDataOrig4Q\")\n",
    "\n",
    "print(\"New EMP data size of: \"+str(len(EMPDataOrig4Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(EMPDataOrig4Q.describe().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#sns.boxplot(y = \"SALARY\", data = EMPDataOrig4Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both our separation and non-separation data loaded, we calculate three new attributes through aggregation or calculation amongst various attributes. \n",
    "\n",
    "1) SEP Count by Date & Occupation – total number of separations (of any type) for a given Date and Occupation; \n",
    "\n",
    "2) SEP Count by Date & Location – total number of separations (of any type) for a given Date and Location; \n",
    "\n",
    "3) Industry Average Salary – Average salary amongst non-separated employees, grouped by quarter, occupation, pay grade, and work schedule; \n",
    "\n",
    "We proceed, by concatenating our Separation and Non-Separation observations, and merge these newly calculated attributes to the concatenated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "##Aggregate Number of Total Separations in current month for given Occ\n",
    "AggSEPCount_EFDATE_OCC= pd.DataFrame({'SEPCount_EFDATE_OCC' : OPMDataMerged.groupby([\"EFDATE\", \"OCC\"]).size()}).reset_index()\n",
    "display(AggSEPCount_EFDATE_OCC.head())\n",
    "\n",
    "\n",
    "##Aggregate Number of Total Separations in current month for given LOC\n",
    "AggSEPCount_EFDATE_LOC = pd.DataFrame({'SEPCount_EFDATE_LOC' : OPMDataMerged.groupby([\"EFDATE\", \"LOC\"]).size()}).reset_index()\n",
    "display(AggSEPCount_EFDATE_LOC.head())\n",
    "\n",
    "##Average Quarterly EMP Salary by occ \n",
    "AggIndAvgSalary = pd.DataFrame({'count' : EMPDataOrig4Q.groupby([\"QTR\", \"OCC\", \"PPGRD\", \"WORKSCHT\"]).size()}).reset_index()\n",
    "AggIndAvgSalary2 = pd.DataFrame({'IndSalarySum' : EMPDataOrig4Q.groupby([\"QTR\", \"OCC\", \"PPGRD\", \"WORKSCHT\"])[\"SALARY\"].sum()}).reset_index()\n",
    "AggIndAvgSalary = AggIndAvgSalary.merge(AggIndAvgSalary2,on=[\"QTR\", \"OCC\", \"PPGRD\", \"WORKSCHT\"])\n",
    "AggIndAvgSalary[\"IndAvgSalary\"] = AggIndAvgSalary[\"IndSalarySum\"]/AggIndAvgSalary[\"count\"]\n",
    "del AggIndAvgSalary[\"count\"]\n",
    "del AggIndAvgSalary[\"IndSalarySum\"]\n",
    "display(AggIndAvgSalary.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Two Datasets\n",
    "### NS SEP code means NonSeparation\n",
    "###add hardcoded null value columns where applicable\n",
    "EMPDataOrig4Q[\"SEP\"] = \"NS\"\n",
    "EMPDataOrig4Q[\"GENDER\"] = np.nan\n",
    "EMPDataOrig4Q[\"COUNT\"] = np.nan\n",
    "\n",
    "OPMDataMerged[\"DATECODE\"] = OPMDataMerged[\"EFDATE\"]\n",
    "\n",
    "OPMColList = [\"AGYSUB\", \"SEP\", \"DATECODE\",   \"AGELVL\", \"GENDER\", \"GSEGRD\", \"LOSLVL\", \"LOC\", \"OCC\", \"PATCO\", \"PPGRD\", \"SALLVL\", \"TOA\", \"WORKSCH\", \"COUNT\", \"SALARY\", \"LOS\", \"AGYTYP\", \"AGYTYPT\", \"AGY\", \"AGYT\", \"AGYSUBT\", \"QTR\", \"AGELVLT\", \"LOSLVLT\", \"LOCTYP\", \"LOCTYPT\", \"LOCT\", \"OCCTYP\", \"OCCTYPT\", \"OCCFAM\", \"OCCFAMT\", \"OCCT\", \"PATCOT\", \"PPTYP\", \"PPTYPT\", \"PPGROUP\", \"PPGROUPT\", \"PAYPLAN\", \"PAYPLANT\", \"SALLVLT\", \"TOATYP\", \"TOATYPT\", \"TOAT\", \"WSTYP\", \"WSTYPT\", \"WORKSCHT\"]\n",
    "EMPColList = [\"AGYSUB\", \"SEP\", \"DATECODE\", \"AGELVL\", \"GENDER\", \"GSEGRD\", \"LOSLVL\", \"LOC\", \"OCC\", \"PATCO\", \"PPGRD\", \"SALLVL\", \"TOA\", \"WORKSCH\", \"COUNT\", \"SALARY\", \"LOS\", \"AGYTYP\", \"AGYTYPT\", \"AGY\", \"AGYT\", \"AGYSUBT\", \"QTR\", \"AGELVLT\", \"LOSLVLT\", \"LOCTYP\", \"LOCTYPT\", \"LOCT\", \"OCCTYP\", \"OCCTYPT\", \"OCCFAM\", \"OCCFAMT\", \"OCCT\", \"PATCOT\", \"PPTYP\", \"PPTYPT\", \"PPGROUP\", \"PPGROUPT\", \"PAYPLAN\", \"PAYPLANT\", \"SALLVLT\", \"TOATYP\", \"TOATYPT\", \"TOAT\", \"WSTYP\", \"WSTYPT\", \"WORKSCHT\"]\n",
    "\n",
    "OPMDataMerged = pd.concat([OPMDataMerged[OPMColList], EMPDataOrig4Q[EMPColList]], ignore_index=True)\n",
    "print(\"Total concatenated data size for SEP and non-SEP: \"+str(len(OPMDataMerged)))\n",
    "\n",
    "OPMDataMerged = OPMDataMerged.merge(AggSEPCount_EFDATE_OCC, left_on = ['DATECODE','OCC'], right_on = ['EFDATE','OCC'], how = 'left')\n",
    "OPMDataMerged = OPMDataMerged.merge(AggSEPCount_EFDATE_LOC, left_on = ['DATECODE','LOC'], right_on = ['EFDATE','LOC'], how = 'left')\n",
    "OPMDataMerged = OPMDataMerged.merge(AggIndAvgSalary, on = ['QTR','OCC', 'PPGRD', 'WORKSCHT'], how = 'left')\n",
    "OPMDataMerged[\"SalaryOverUnderIndAvg\"] = OPMDataMerged[\"SALARY\"] - OPMDataMerged[\"IndAvgSalary\"]\n",
    "\n",
    "del OPMDataMerged[\"EFDATE_x\"]\n",
    "del OPMDataMerged[\"EFDATE_y\"]\n",
    "\n",
    "display(OPMDataMerged.head())\n",
    "display(OPMDataMerged.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].isnull()]))\n",
    "\n",
    "display(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].isnull()][[\"SEP\",\"DATECODE\", \"OCC\"]].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 50993 Non-Separation observations do not have coverage within the Separation Dataset, thus, we will remove these observations as out of scope demographic in our analysis. Any attempt in predicting these values will not have enough data to support a significant response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].notnull()]\n",
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_OCC\"].isnull()]))\n",
    "\n",
    "print(len(OPMDataMerged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_LOC\"].isnull()]))\n",
    "\n",
    "display(OPMDataMerged[OPMDataMerged[\"SEPCount_EFDATE_LOC\"].isnull()][[\"SEP\",\"DATECODE\",\"LOC\"]].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].isnull()]))\n",
    "\n",
    "display(OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].isnull()][[\"QTR\", \"SEP\",\"OCCT\", \"PPGRD\", \"WORKSCHT\"]].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 1293 separation observations do not have coverage within the EMP Dataset, thus, we will remove these observations as out of scope demographic in our analysis. Any attempt in predicting these values will not have enough data to support a significant response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].notnull()]\n",
    "\n",
    "print(len(OPMDataMerged[OPMDataMerged[\"IndAvgSalary\"].isnull()]))\n",
    "\n",
    "print(len(OPMDataMerged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************************\n",
    "*************************************\n",
    "\n",
    "# Placeholder Chunks for Data Quality check of salary against GS Grade Level Ranges\n",
    "\n",
    "*************************************\n",
    "*************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder Chunks for Data Quality check of salary against GS Grade Level Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are iterested to see how federal pension plans may impact attrition in this dataset. An interesting attribute to complement Length of service, is Years to Retirement. Utilizing a FERS retirement eligibility baseline of 57 years of age for all observations, and the lower limitation of age level ranges we compute a numeric value for length of retirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Column YearsToRetirement\n",
    "\n",
    "\"\"\"\n",
    "    AGELVL,AGELVLT\n",
    "    A,Less than 20\n",
    "    B,20-24\n",
    "    C,25-29\n",
    "    D,30-34\n",
    "    E,35-39\n",
    "    F,40-44\n",
    "    G,45-49\n",
    "    H,50-54\n",
    "    I,55-59\n",
    "    J,60-64\n",
    "    K,65 or more\n",
    "    Z,Unspecified\n",
    "\"\"\"\n",
    "OPMDataMerged[\"LowerLimitAge\"] = np.where(OPMDataMerged[\"AGELVL\"]==\"B\", 20,\n",
    "                                                np.where(OPMDataMerged[\"AGELVL\"]==\"C\", 25,\n",
    "                                                         np.where(OPMDataMerged[\"AGELVL\"]==\"D\", 30,\n",
    "                                                                  np.where(OPMDataMerged[\"AGELVL\"]==\"E\", 35,\n",
    "                                                                           np.where(OPMDataMerged[\"AGELVL\"]==\"F\", 40,\n",
    "                                                                                    np.where(OPMDataMerged[\"AGELVL\"]==\"G\", 45,\n",
    "                                                                                             np.where(OPMDataMerged[\"AGELVL\"]==\"H\", 50,\n",
    "                                                                                                      np.where(OPMDataMerged[\"AGELVL\"]==\"I\", 55,\n",
    "                                                                                                               np.where(OPMDataMerged[\"AGELVL\"]==\"J\", 60,\n",
    "                                                                                                                        np.where(OPMDataMerged[\"AGELVL\"]==\"K\", 65,\n",
    "                                                                                                                                 np.nan\n",
    "                                                                                                                                )\n",
    "                                                                                                                        )\n",
    "                                                                                                               )\n",
    "                                                                                                      )\n",
    "                                                                                            )\n",
    "                                                                                   )\n",
    "                                                                          )\n",
    "                                                                 )\n",
    "                                                        )\n",
    "                                               )  \n",
    "\n",
    "retAge = 57\n",
    "\n",
    "OPMDataMerged[\"YearsToRetirement\"] = np.where(OPMDataMerged[\"AGELVL\"]==\"B\", retAge-20,\n",
    "                                                np.where(OPMDataMerged[\"AGELVL\"]==\"C\", retAge-25,\n",
    "                                                         np.where(OPMDataMerged[\"AGELVL\"]==\"D\", retAge-30,\n",
    "                                                                  np.where(OPMDataMerged[\"AGELVL\"]==\"E\", retAge-35,\n",
    "                                                                           np.where(OPMDataMerged[\"AGELVL\"]==\"F\", retAge-40,\n",
    "                                                                                    np.where(OPMDataMerged[\"AGELVL\"]==\"G\", retAge-45,\n",
    "                                                                                             np.where(OPMDataMerged[\"AGELVL\"]==\"H\", retAge-50,\n",
    "                                                                                                      np.where(OPMDataMerged[\"AGELVL\"]==\"I\", retAge-55,\n",
    "                                                                                                               np.where(OPMDataMerged[\"AGELVL\"]==\"J\", retAge-60,\n",
    "                                                                                                                        np.where(OPMDataMerged[\"AGELVL\"]==\"K\", retAge-65,\n",
    "                                                                                                                                 np.nan\n",
    "                                                                                                                                )\n",
    "                                                                                                                        )\n",
    "                                                                                                               )\n",
    "                                                                                                      )\n",
    "                                                                                            )\n",
    "                                                                                   )\n",
    "                                                                          )\n",
    "                                                                 )\n",
    "                                                        )\n",
    "                                               )  \n",
    "\n",
    "print(\"Null Values for LowerLimitAge: \" + str(len(OPMDataMerged[OPMDataMerged[\"LowerLimitAge\"].isnull()])))\n",
    "print(\"Null Values for YearsToRetirement: \" + str(len(OPMDataMerged[OPMDataMerged[\"YearsToRetirement\"].isnull()])))\n",
    "\n",
    "display(OPMDataMerged.head())\n",
    "display(OPMDataMerged.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Bureau of Labor Statistics data\n",
    "\n",
    "In addition to the OPM data, we merge 10 attributes from the Bureau of Labor Statistics (BLS). Data is sourced from Federal Government industry codes across all regions. Although assumed to be highly correlated, we source both Level (Total number) and Rate (Percentage of Level to total employment and / or job openings) for the following statistics: 1) Job Openings, 2) Layoffs, 3) Quits, 4) Total Separations, and 5) Other Separations. While Rate paints an aggregated, holistic picture for job market trends, Level provides a raw count for total separations alone. Both these statistics were captured by a monthly aggregate and merged to the OPM data by their respective months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def bls(series, start, end):\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    sID   = []\n",
    "    \n",
    "    for i in range(0,len(series)):\n",
    "        sID.append(series[i][0])\n",
    "    \n",
    "    data = json.dumps({\"seriesid\": sID,\n",
    "                       \"startyear\":start,\n",
    "                       \"endyear\":end,\n",
    "                       \"catalog\":False,\n",
    "                       \"calculations\":False,\n",
    "                       \"annualaverage\":False,\n",
    "                       \"registrationkey\":\"7a89c8d7979349fba8914b8be16a1646\"})\n",
    "    \n",
    "    p = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=data, headers=headers)\n",
    "    json_data = json.loads(p.text)\n",
    "    bls = []\n",
    "    for series in json_data['Results']['series']:\n",
    "        #x=prettytable.PrettyTable([\"series id\",\"year\",\"period\",\"value\",\"footnotes\"])\n",
    "        result = pd.DataFrame(columns=[\"series id\",\"year\",\"period\",\"value\",\"footnotes\"])\n",
    "        seriesId = series['seriesID']\n",
    "        for item in series['data']:\n",
    "            year = item['year']\n",
    "            period = item['period']\n",
    "            value = item['value']\n",
    "            footnotes=\"\"\n",
    "            for footnote in item['footnotes']:\n",
    "                if footnote:\n",
    "                    footnotes = footnotes + footnote['text'] + ','\n",
    "            if 'M01' <= period <= 'M12':\n",
    "                #x.add_row([seriesId,year,period,value,footnotes[0:-1]])\n",
    "                y = pd.DataFrame({\"series id\" : seriesId,\n",
    "                                  \"year\" : year,\n",
    "                                  \"period\" : period,\n",
    "                                  \"value\" : value,\n",
    "                                  \"footnotes\" : footnotes}, index = [0])\n",
    "                result = result.append(y, ignore_index = True)\n",
    "        bls.append(result)\n",
    "    return(bls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seriesList = [\n",
    "              ['JTU91000000JOL','BLS_FEDERAL_JobOpenings_Level'],\n",
    "              ['JTU91000000LDL','BLS_FEDERAL_Layoffs_Level'],\n",
    "              ['JTU91000000OSL','BLS_FEDERAL_OtherSep_Level'],\n",
    "              ['JTU91000000QUL','BLS_FEDERAL_Quits_Level'],\n",
    "              ['JTU91000000TSL','BLS_FEDERAL_TotalSep_Level'],\n",
    "              ['JTU91000000JOR','BLS_FEDERAL_JobOpenings_Rate'],\n",
    "              ['JTU91000000LDR','BLS_FEDERAL_Layoffs_Rate'],\n",
    "              ['JTU91000000OSR','BLS_FEDERAL_OtherSep_Rate'],\n",
    "              ['JTU91000000QUR','BLS_FEDERAL_Quits_Rate'],\n",
    "              ['JTU91000000TSR','BLS_FEDERAL_TotalSep_Rate']\n",
    "             ]\n",
    "\n",
    "# Pull job openings and labor turnover data\n",
    "JTL = bls(seriesList, \"2014\", \"2015\")\n",
    "\n",
    "seriesList = pd.DataFrame(seriesList, columns = [\"series id\",\"sName\"])\n",
    "\n",
    "##We need to replace these with actual Descriptor Column Names\n",
    "\n",
    "for i in range(0,len(seriesList)):\n",
    "    \n",
    "    JTL[i] = JTL[i].merge(seriesList, on = \"series id\", how = 'inner')\n",
    "\n",
    "    if len(JTL[i]) >0:\n",
    "        name = JTL[i][\"sName\"].drop_duplicates().values[0]\n",
    "    else:\n",
    "        name = str(i)\n",
    "\n",
    "    JTL[i][name] = JTL[i][\"value\"].apply(pd.to_numeric)\n",
    "    JTL[i][\"DATECODE\"] = JTL[i][\"year\"] + JTL[i][\"period\"].str[-2:]\n",
    "    del JTL[i][\"value\"]\n",
    "    del JTL[i][\"year\"]\n",
    "    del JTL[i][\"period\"]\n",
    "    del JTL[i][\"series id\"]\n",
    "    del JTL[i][\"footnotes\"]\n",
    "    del JTL[i][\"sName\"]\n",
    "    \n",
    "    \n",
    "    OPMDataMerged = OPMDataMerged.merge(JTL[i], on = \"DATECODE\", how = 'left')\n",
    "    display(JTL[i].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(OPMDataMerged.head())\n",
    "display(OPMDataMerged.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame({'StratCount' : OPMDataMerged.groupby([\"SEP\"]).size()}).reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary EDA\n",
    "\n",
    "In terms of data exploration, we first investigate numeric type attributes. Relationships, distributions, and correlation values are reviewed.\n",
    "\n",
    "**A new binary separation attribute is created to indicate whether non-sep or sep for EDA correlation purposes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#\n",
    "#cols = list(SampledOPMData.select_dtypes(include=['float64', 'int64']))\n",
    "#cols.remove('COUNT')\n",
    "#cols.remove('BLS_FEDERAL_OtherSep_Rate')\n",
    "#cols.remove('BLS_FEDERAL_Quits_Rate')\n",
    "#cols.remove('BLS_FEDERAL_TotalSep_Level')\n",
    "#cols.remove('BLS_FEDERAL_JobOpenings_Rate')\n",
    "#cols.remove('BLS_FEDERAL_OtherSep_Level')\n",
    "#cols.remove('BLS_FEDERAL_Quits_Level')\n",
    "#cols.remove('BLS_FEDERAL_JobOpenings_Level')\n",
    "#cols.remove('BLS_FEDERAL_Layoffs_Rate')\n",
    "#cols.remove('BLS_FEDERAL_Layoffs_Level')\n",
    "#cols.remove('BLS_FEDERAL_TotalSep_Rate')\n",
    "#cols.append('SEP')\n",
    "#display(cols)\n",
    "#\n",
    "#plotNumeric = SampledOPMData[cols]\n",
    "#\n",
    "## Create binary separation attribute for EDA correlation review\n",
    "##plotNumeric[\"SEP_bin\"] = plotNumeric.SEP.replace(\"NS\", 1)\n",
    "##plotNumeric.loc[plotNumeric['SEP_bin'] != 1, 'SEP_bin'] = 0\n",
    "##plotNumeric.SEP_bin = plotNumeric.SEP_bin.apply(pd.to_numeric)\n",
    "#AttSplit = pd.get_dummies(plotNumeric['SEP'],prefix='SEP')\n",
    "#display(AttSplit.head())\n",
    "#plotNumeric = pd.concat((plotNumeric,AttSplit),axis=1) # add back into the dataframe\n",
    "#\n",
    "#display(plotNumeric.head())\n",
    "#print(\"plotNumeric has {0} Records\".format(len(plotNumeric)))\n",
    "##print(plotNumeric.SEP_bin.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "cols = list(OPMDataMerged.select_dtypes(include=['float64', 'int64']))\n",
    "cols.remove('COUNT')\n",
    "cols.remove('BLS_FEDERAL_OtherSep_Rate')\n",
    "cols.remove('BLS_FEDERAL_Quits_Rate')\n",
    "cols.remove('BLS_FEDERAL_TotalSep_Level')\n",
    "cols.remove('BLS_FEDERAL_JobOpenings_Rate')\n",
    "cols.remove('BLS_FEDERAL_OtherSep_Level')\n",
    "cols.remove('BLS_FEDERAL_Quits_Level')\n",
    "cols.remove('BLS_FEDERAL_JobOpenings_Level')\n",
    "cols.remove('BLS_FEDERAL_Layoffs_Rate')\n",
    "cols.remove('BLS_FEDERAL_Layoffs_Level')\n",
    "cols.remove('BLS_FEDERAL_TotalSep_Rate')\n",
    "cols.append('SEP')\n",
    "display(cols)\n",
    "\n",
    "plotNumeric = OPMDataMerged[cols]\n",
    "\n",
    "# Create binary separation attribute for EDA correlation review\n",
    "#plotNumeric[\"SEP_bin\"] = plotNumeric.SEP.replace(\"NS\", 1)\n",
    "#plotNumeric.loc[plotNumeric['SEP_bin'] != 1, 'SEP_bin'] = 0\n",
    "#plotNumeric.SEP_bin = plotNumeric.SEP_bin.apply(pd.to_numeric)\n",
    "AttSplit = pd.get_dummies(plotNumeric['SEP'],prefix='SEP')\n",
    "display(AttSplit.head())\n",
    "plotNumeric = pd.concat((plotNumeric,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "display(plotNumeric.head())\n",
    "print(\"plotNumeric has {0} Records\".format(len(plotNumeric)))\n",
    "#print(plotNumeric.SEP_bin.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#sns.set(font_scale=1)\n",
    "#sns.pairplot(plotNumeric.drop([\"SEP_NS\", \n",
    "#                               \"SEP_SA\", \n",
    "#                               \"SEP_SB\", \n",
    "#                               \"SEP_SC\", \n",
    "#                               \"SEP_SD\", \n",
    "#                               \"SEP_SE\", \n",
    "#                               \"SEP_SF\", \n",
    "#                               \"SEP_SG\", \n",
    "#                               \"SEP_SH\", \n",
    "#                               \"SEP_SI\", \n",
    "#                               \"SEP_SJ\", \n",
    "#                               \"SEP_SK\", \n",
    "#                               \"SEP_SL\"\n",
    "#                              ], axis=1), hue = 'SEP', palette=\"hls\", plot_kws={\"s\": 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/amfrye777/MSDSCapstone/blob/master/OPMSeparationsCode/Visualizations/PairPlotCorrelations.png?raw=true\\\" width=\\\"1200\\\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function modified from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n",
    "sns.set()\n",
    "\n",
    "def draw_histograms(df, variables, n_rows, n_cols):\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        df[var_name].hist(bins=20,ax=ax, color='#58D68D')\n",
    "        ax.set_title(var_name+\" Distribution\")\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "draw_histograms(plotNumeric.drop(['SEP',\n",
    "                                  \"SEP_NS\", \n",
    "                               \"SEP_SA\", \n",
    "                               \"SEP_SB\", \n",
    "                               \"SEP_SC\", \n",
    "                               \"SEP_SD\", \n",
    "                               \"SEP_SE\", \n",
    "                               \"SEP_SF\", \n",
    "                               \"SEP_SG\", \n",
    "                               \"SEP_SH\", \n",
    "                               \"SEP_SI\", \n",
    "                               \"SEP_SJ\", \n",
    "                               \"SEP_SK\", \n",
    "                               \"SEP_SL\"\n",
    "                              ], axis=1),\n",
    "                plotNumeric.drop(['SEP',\n",
    "                                  \"SEP_NS\", \n",
    "                               \"SEP_SA\", \n",
    "                               \"SEP_SB\", \n",
    "                               \"SEP_SC\", \n",
    "                               \"SEP_SD\", \n",
    "                               \"SEP_SE\", \n",
    "                               \"SEP_SF\", \n",
    "                               \"SEP_SG\", \n",
    "                               \"SEP_SH\", \n",
    "                               \"SEP_SI\", \n",
    "                               \"SEP_SJ\", \n",
    "                               \"SEP_SK\", \n",
    "                               \"SEP_SL\"\n",
    "                              ], axis=1).columns, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Inspired by http://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "#plt.matshow(plotNumeric.corr())\n",
    "\n",
    "sns.set(style='white')\n",
    "corr = plotNumeric.drop(['SEP'], axis=1).corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.set(font_scale=0.95)\n",
    "heatCorr = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "                       square=True, annot=True, linewidths=1,\n",
    "                       cbar_kws={\"shrink\": .5}, ax=ax, fmt='.1g')\n",
    "#heatCorr.\n",
    "ax.tick_params(labelsize=15)\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.tick_params(labelsize=15)\n",
    "\n",
    "sns.plt.show()\n",
    "#sns.heatmap(corr, annot=True, linewidths=0.01, cmap=cmap, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the distribution of attributes identified above, we have decided to take the log transform of several attributes. \n",
    "- Salary\n",
    "- LOS (augmented by a value of .00001 to adjust for the undefined result of log(0)\n",
    "- SEPCount_EFDATE_OCC\n",
    "- SEPCount_EFDATE_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Log Transform Columns Added\n",
    "OPMDataMerged[\"SALARYLog\"] = OPMDataMerged[\"SALARY\"].apply(np.log)\n",
    "#OPMDataMerged[\"LOSLog\"] = (OPMDataMerged[\"LOS\"] + .00001).apply(np.log)\n",
    "OPMDataMerged[\"LOSSqrt\"] = (OPMDataMerged[\"LOS\"]).apply(np.sqrt)\n",
    "OPMDataMerged[\"SEPCount_EFDATE_OCCLog\"] = OPMDataMerged[\"SEPCount_EFDATE_OCC\"].apply(np.log)\n",
    "OPMDataMerged[\"SEPCount_EFDATE_LOCLog\"] = OPMDataMerged[\"SEPCount_EFDATE_LOC\"].apply(np.log)\n",
    "OPMDataMerged[\"IndAvgSalaryLog\"] = OPMDataMerged[\"IndAvgSalary\"].apply(np.log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We next review categorical data to improve our understanding of factor levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#            \"LOCTYPT\",\n",
    "#            \"OCCTYP\",\n",
    "#            \"OCCTYPT\",\n",
    "#            \"PPTYP\",\n",
    "#            \"PPTYPT\",\n",
    "#            \"AGYTYP\",\n",
    "#            \"OCCFAM\",\n",
    "#            \"PPGROUP\",\n",
    "#            \"PAYPLAN\",\n",
    "#            \"TOATYP\",\n",
    "#            \"WSTYP\",\n",
    "#            \"AGYSUBT\",\n",
    "#            \"AGELVL\",\n",
    "#            \"LOSLVL\",\n",
    "#            \"LOC\",\n",
    "#            \"OCC\",\n",
    "#            \"PATCO\",\n",
    "#            \"SALLVL\",\n",
    "#            \"TOA\",\n",
    "#            \"WORKSCH\"]\n",
    "#\n",
    "#for i in dropCols:\n",
    "#    cols.remove(i)\n",
    "#\n",
    "#plotCat = SampledOPMData[cols]\n",
    "#display(plotCat.head())\n",
    "#print(\"plotCat Has {0} Records\".format(len(plotCat)))\n",
    "#print(\"Number of colums = \", len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cols = list(OPMDataMerged.select_dtypes(include=['object']))\n",
    "dropCols = [\"LOCTYP\",\n",
    "            \"LOCTYPT\",\n",
    "            \"OCCTYP\",\n",
    "            \"OCCTYPT\",\n",
    "            \"PPTYP\",\n",
    "            \"PPTYPT\",\n",
    "            \"AGYTYP\",\n",
    "            \"OCCFAM\",\n",
    "            \"PPGROUP\",\n",
    "            \"PAYPLAN\",\n",
    "            \"TOATYP\",\n",
    "            \"WSTYP\",\n",
    "            \"AGYSUBT\",\n",
    "            \"AGELVL\",\n",
    "            \"LOSLVL\",\n",
    "            \"LOC\",\n",
    "            \"OCC\",\n",
    "            \"PATCO\",\n",
    "            \"SALLVL\",\n",
    "            \"TOA\",\n",
    "            \"WORKSCH\"]\n",
    "\n",
    "for i in dropCols:\n",
    "    cols.remove(i)\n",
    "\n",
    "plotCat = OPMDataMerged[cols]\n",
    "display(plotCat.head())\n",
    "print(\"plotCat Has {0} Records\".format(len(plotCat)))\n",
    "print(\"Number of colums = \", len(cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGYSUB\n",
    "High seperation among following:\n",
    "* Veterans Health Administration (VATA)\n",
    "* Forest Service (AG11)\n",
    "\n",
    "### GENDER\n",
    "Similar separation distributions among males and females, except more terminations due to contract expiration among males\n",
    "\n",
    "### GSEGRD\n",
    "High termination due to expired appt/other among following:\n",
    "* 3\n",
    "* 4\n",
    "* 5\n",
    "\n",
    "Bimodal Quit distribution with outlier spike at GSEGRD 9:\n",
    "* Distribution 1 from GSEGRD 3 to 8\n",
    "* Distribution 2 from GSEGRD 11 to 15\n",
    "\n",
    "Individual transfers highest among levels 11, 12, 13\n",
    "\n",
    "### PPGRD\n",
    "Majority of distribution resides in GS values per the GSEGRD observations described above.... <font color=\"red\">Are other PPGRD values of any significance? What are corporate grades all about?</font>\n",
    "\n",
    "### AGYT\n",
    "Top three Agencies with separation:\n",
    "1. AR-Department of the Army\n",
    "2. AG-Department of Agriculture\n",
    "3. VA-Department of Veteran Affairs\n",
    "\n",
    "High contract termination in:\n",
    "* AG-Department of Agriculture\n",
    "* IN-Department of the Interior\n",
    "\n",
    "While Veteran Affairs and Army both have many quits and many retirees, the Army has significantly more individual transfers (on par with retirements)\n",
    "\n",
    "### QTR\n",
    "Most contract terminations in 1st and 4th quarters\n",
    "\n",
    "Retirement peaks in 2nd quarter\n",
    "\n",
    "Number of quits increases from one quarter to the next\n",
    "\n",
    "<font color=\"purple\">*Bear in mind these are quarters from single year only so time-sensitive trends may not be applicable*</font>\n",
    "\n",
    "### AGELVLT\n",
    "High termination due to expired appt/other among following:\n",
    "* B\n",
    "* C\n",
    "\n",
    "Number of Quits peaks at AGELVL D\n",
    "\n",
    "Individual transfer counts mostly trend with Quits\n",
    "\n",
    "Retirement highest at following:\n",
    "* I\n",
    "* J\n",
    "* K\n",
    "\n",
    "### LOSLVLT\n",
    "Highest Quit count for LOSLVL A (< 1 year service) which then declines for levels B and C before spiking again at level D (5-9 years service)\n",
    "\n",
    "Same pattern is observed for contract terminations but without any significant spikes with longer service\n",
    "\n",
    "Large individual transfer spike at LOSLVL D (5-9 years service)\n",
    "\n",
    "Retirement starts at LOSLVL D but trends upward to J\n",
    "\n",
    "### LOCT\n",
    "Contract terminations comprise most California terminations among top total separation states\n",
    "\n",
    "East Coast locations may possibly have most individual transfers, the most being in Washington DC\n",
    "\n",
    "### OCCFAMT\n",
    "03xx-General Admin, clerical, and office svcs highest separation by far but indicates both high number of Quits and Retirements\n",
    "\n",
    "Many quits in 06xx-Medical\n",
    "\n",
    "04xx-Natural Resources again indicates high number of contract terminations\n",
    "\n",
    "01xx-Social Science has even number of Quits and retirements\n",
    "\n",
    "### OCCT\n",
    "\n",
    "### PATCOT\n",
    "\n",
    "### PAYPLANT\n",
    "Results skewed by GS\n",
    "\n",
    "### TOAT\n",
    "\n",
    "### WORKSCHT\n",
    "Should model full time only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subCountPlot(att1, att2, thresh):\n",
    "    counts = plotCat.groupby([att1, att2]).size().unstack(fill_value=0) # Get att1 sizes by att2\n",
    "    counts = pd.concat([counts,counts.sum(axis=1)], axis=1) # Calculate total for each att1 value and append total as new column\n",
    "    counts.rename(columns={0:\"Total\"}, inplace=True)\n",
    "    top = counts[counts[\"Total\"] > thresh].index.tolist() # Obtain att1 values where total surpasses threshold\n",
    "    \n",
    "    zoom = plotCat[plotCat[att1].isin(top)] # Subset data to only the top att1 values\n",
    "    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10), sharey=False)\n",
    "    sns.countplot(y=att1, data=zoom, color=\"blue\", ax=ax1); # Dark blue signifies zoomed data\n",
    "    sns.countplot(y=att1, data=zoom, hue=att2, palette=\"hls\", ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percBarPlot(att1, att2, numColors):\n",
    "    # Create count by att1 and att2\n",
    "    counts = plotCat.groupby([att1, att2]).size().unstack(fill_value=0) # Get att1 sizes by att2\n",
    "    counts = pd.concat([counts,counts.sum(axis=1)], axis=1) # Calculate total for each att1 value and append total as new column\n",
    "    counts.rename(columns={0:\"Total\"}, inplace=True)\n",
    "    #counts.drop('Total', axis=1).plot(kind='bar', stacked=True)\n",
    "    \n",
    "    # create cmap from sns color palette\n",
    "    my_cmap = ListedColormap(sns.color_palette('hls', numColors).as_hex())\n",
    "\n",
    "    # Create and plot percentage by att1 and att2\n",
    "    nest1 = []\n",
    "    for i in counts.values:\n",
    "        nest2 = []\n",
    "        for j in i:\n",
    "            nest2.append(float(j/(i[len(i)-1:]))*100)\n",
    "        nest1.append(nest2)\n",
    "    perc = pd.DataFrame(nest1)\n",
    "    perc = perc.set_index(counts.index.values)\n",
    "    perc.columns = counts.columns\n",
    "    perc.drop('Total', axis=1).plot(kind='bar', stacked=True, ylim=(0,100), figsize={13,6}, title=att1+' Percentage Plot', colormap=my_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = cols[:4] # for quick visualization debug only; may delete once complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in cols:\n",
    "    if i != 'SEP':\n",
    "        plt.figure(i) # Required to create new figure each loop rather than drawing over previous object\n",
    "        f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10), sharey=False)\n",
    "        sns.countplot(y=i, data=plotCat, color=\"lightblue\", ax=ax1);\n",
    "        sns.countplot(y=i, data=plotCat, hue=\"SEP\", palette=\"hls\", ax=ax2);\n",
    "        \n",
    "    if i == 'AGYSUB':\n",
    "        subCountPlot(i, 'SEP', 10000)\n",
    "    elif i == 'LOCT':\n",
    "        subCountPlot(i, 'SEP', 4000)\n",
    "    elif i == 'OCCT':\n",
    "        subCountPlot(i, 'SEP', 2000)\n",
    "    elif i == 'PPGRD':\n",
    "        subCountPlot(i, 'SEP', 6000)\n",
    "    elif i == 'AGYT':\n",
    "        subCountPlot(i, 'SEP', 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in cols:\n",
    "    if i != 'SEP':\n",
    "        percBarPlot(i, 'SEP', len(plotCat.SEP.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percBarPlot('GSEGRD', 'SALLVLT', len(plotCat.SALLVLT.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percBarPlot('PATCOT', 'SALLVLT', len(plotCat.SALLVLT.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "\n",
    "sns.violinplot(x=\"PATCOT\", y=\"SALARY\", hue=\"GENDER\", data=OPMDataMerged[OPMDataMerged.GENDER != 'Z'], split=True,\n",
    "               inner=\"quart\", palette={\"M\": \"b\", \"F\": \"pink\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"SEP\", y=\"SALARY\", hue=\"GENDER\", data=OPMDataMerged[OPMDataMerged.GENDER != 'Z'], split=True,\n",
    "               inner=\"quart\", palette={\"M\": \"b\", \"F\": \"pink\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.factorplot(x=\"SEP\", y=\"SALARY\", hue=\"GENDER\", col=\"PATCOT\",\n",
    "               data=OPMDataMerged[OPMDataMerged.GENDER != 'Z'],\n",
    "               kind=\"violin\", split=True, aspect=.4, size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#sns.factorplot(x=\"SEP\", y=\"SALARY\", col=\"PATCOT\", data=OPMDataMerged,\n",
    "#               kind=\"violin\", split=True, aspect=.4, size=10, palette = \"hls\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/amfrye777/MSDSCapstone/blob/master/OPMSeparationsCode/Visualizations/FactorPlotViolins.png?raw=true\\\" width=\\\"1200\\\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#g = sns.PairGrid(data=OPMDataMerged,\n",
    "#                 x_vars=[\"SEP\",\"PATCOT\"],\n",
    "#                 y_vars=[\"SALARY\", \"LOS\", \"LowerLimitAge\", \"YearsToRetirement\"],\n",
    "#                 aspect=1, size=10)\n",
    "#g.map(sns.violinplot, palette=\"pastel\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/amfrye777/MSDSCapstone/blob/master/OPMSeparationsCode/Visualizations/PairGridViolins.png?raw=true\\\" width=\\\"1200\\\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(plotNumeric, plotCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focusing in on our Target Demographic\n",
    "\n",
    "After analyzing the above plots for our categorical data, we have decided to narrow our focus due to the large variability in the dataset. We take the below actions on our dataset:\n",
    "- Keep only Full-time Nonseasonal observations\n",
    "- Remove the location US-SUPPRESSED (SEE DATA DEFINITIONS) due to apparent bias towards unknowns in the non-separation data\n",
    "- Keep only General Schedele Grades above 7.\n",
    "- Focus model generation on White Collar Jobs only\n",
    "- Create a Training set for the Professional PATCO value, and a Testing set for Administration\n",
    "\n",
    "In addition, we have opted to remove the below attributes for model generation:\n",
    "- Datecode, QTR; Although very relevant for merging data from alternate sources, we do not have several years of data so this does not bring us much value\n",
    "- All Agency Attributes(AGYTYP,AGYTYPT,AGY,AGYT,AGYSUB,AGYSUBT); We are not concerned with agencies\n",
    "- Gender; Missing values for Non-Separation observations\n",
    "- Count; Missing values for Non-Separation observations; Also, all values = 1 so not very useful\n",
    "- PAYPLAN,PAYPLANT,PPGRD; Much too granular than we care for\n",
    "- LOSLVL,LOSLVLT; we have a numerical version of this attribute\n",
    "- OCC,OCCT; Much too granular than we care for\n",
    "\n",
    "Our goal is to limit our focus to Professional occupations, build a model, then test that generated model on the Administration segment of the population.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(OPMDataMerged))\n",
    "#Removing Attributes\n",
    "cols = list(OPMDataMerged.columns)\n",
    "dropCols = [\"QTR\",\n",
    "            \"AGYTYP\",\n",
    "            \"AGYTYPT\",\n",
    "            \"AGY\",\n",
    "            \"AGYT\",\n",
    "            \"AGYSUB\",\n",
    "            \"AGYSUBT\",\n",
    "            \"GENDER\",\n",
    "            \"COUNT\",\n",
    "            \"PAYPLAN\",\n",
    "            \"PAYPLANT\",\n",
    "            \"PPGRD\",\n",
    "            \"LOSLVL\",\n",
    "            \"LOSLVLT\",\n",
    "            \"SALLVL\",\n",
    "            \"SALLVLT\",\n",
    "            \"OCC\",\n",
    "            \"OCCT\"]\n",
    "\n",
    "for i in dropCols:\n",
    "    if i in cols:\n",
    "        cols.remove(i)\n",
    "\n",
    "OPMDataMerged = OPMDataMerged[cols]\n",
    "\n",
    "# Keep only Full-time Nonseasonal observations\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"WORKSCH\"] == \"F\"]\n",
    "\n",
    "#Remove the location US-SUPPRESSED (SEE DATA DEFINITIONS)\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"LOC\"] != \"US\"]\n",
    "\n",
    "#Keep only General Schedele Grades above 7.\n",
    "OPMDataMerged[\"GSEGRD\"] = OPMDataMerged[\"GSEGRD\"].apply(pd.to_numeric)\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"GSEGRD\"] >= 7]\n",
    "\n",
    "#Focus model generation on White Collar Jobs only\n",
    "OPMDataMerged = OPMDataMerged[OPMDataMerged[\"OCCTYP\"] == \"1\"]\n",
    "\n",
    "#Create a Training set for the Professional PATCO value, and a Testing set for Administration\n",
    "OPMDataMergedProf = OPMDataMerged[OPMDataMerged[\"PATCO\"] == \"1\"]\n",
    "OPMDataMergedAdmin = OPMDataMerged[OPMDataMerged[\"PATCO\"] == \"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(OPMDataMergedProf.head())\n",
    "print(len(OPMDataMergedProf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(OPMDataMergedAdmin.head())\n",
    "print(len(OPMDataMergedAdmin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curious on stratum SEP counts for full remaining data\n",
    "stratum = pd.DataFrame({'StratCount' : OPMDataMerged.groupby([\"SEP\"]).size()}).reset_index()\n",
    "\n",
    "display(stratum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess Stratum SEP Counts for Prof, for use in sampling\n",
    "maxSize=7500\n",
    "stratumProf = pd.DataFrame({'StratCount' : OPMDataMergedProf.groupby([\"SEP\"]).size()}).reset_index()\n",
    "\n",
    "stratumProf.loc[stratumProf[\"StratCount\"]>maxSize,\"StratCountSample\"] = maxSize\n",
    "stratumProf.loc[stratumProf[\"StratCount\"]<=maxSize,\"StratCountSample\"] = stratumProf[\"StratCount\"]\n",
    "#else: stratum[\"StratCountSample\"] = stratum[\"StratCount\"]\n",
    "\n",
    "display(stratumProf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess Stratum SEP Counts for Admin, for use in sampling\n",
    "maxSize=7500\n",
    "stratumAdmin = pd.DataFrame({'StratCount' : OPMDataMergedAdmin.groupby([\"SEP\"]).size()}).reset_index()\n",
    "\n",
    "stratumAdmin.loc[stratumAdmin[\"StratCount\"]>maxSize,\"StratCountSample\"] = maxSize\n",
    "stratumAdmin.loc[stratumAdmin[\"StratCount\"]<=maxSize,\"StratCountSample\"] = stratumAdmin[\"StratCount\"]\n",
    "#else: stratum[\"StratCountSample\"] = stratum[\"StratCount\"]\n",
    "\n",
    "display(stratumAdmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def aggStratPop(stratum, OPMDataMerged):\n",
    "    AggStrat = []\n",
    "\n",
    "    for i in range(0,len(stratum)):\n",
    "        sep = stratum[\"SEP\"].ix[i]\n",
    "        StratCountSample = stratum[\"StratCountSample\"].ix[i]\n",
    "        print(\"Stratum Sample Size Calculations for SEP: {}\".format(sep))   \n",
    "        AggStrat.append(pd.DataFrame({'StratCount' : OPMDataMerged[OPMDataMerged[\"SEP\"]==sep].groupby([\"DATECODE\", \"AGELVL\"]).size()}).reset_index())\n",
    "        AggStrat[i][\"SEP\"] = sep\n",
    "        AggStrat[i][\"TotalCount\"] = len(OPMDataMerged[OPMDataMerged[\"SEP\"]==sep])\n",
    "        AggStrat[i][\"p\"] = AggStrat[i][\"StratCount\"] / AggStrat[i][\"TotalCount\"]\n",
    "        AggStrat[i][\"StratCountSample\"] = StratCountSample\n",
    "        AggStrat[i][\"StratSampleSize\"] = round(AggStrat[i][\"p\"] * StratCountSample).apply(int)\n",
    "\n",
    "        display(AggStrat[i].head())\n",
    "        print(\"totalStratumSampleSize: \", AggStrat[i][\"StratSampleSize\"].sum())\n",
    "        #print(len(AggStrat[i]))\n",
    "    return AggStrat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SampleStrata(stratum, OPMDataMerged, FileName):\n",
    "    AggStrat = aggStratPop(stratum, OPMDataMerged)\n",
    "\n",
    "    SampledOPMStratumDataList = []\n",
    "\n",
    "    for i,StratSampleSize in enumerate(AggStrat):\n",
    "        SampledOPMStratumData = []\n",
    "        for j in range(0,len(StratSampleSize)):\n",
    "            SEP = StratSampleSize[\"SEP\"].ix[j]\n",
    "            DATECODE = StratSampleSize[\"DATECODE\"].ix[j]\n",
    "            AGELVL = StratSampleSize[\"AGELVL\"].ix[j]\n",
    "            SampleSize = StratSampleSize[\"StratSampleSize\"].ix[j]\n",
    "            print(SEP, DATECODE, AGELVL, SampleSize)\n",
    "\n",
    "            SampledOPMStratumDataList.append(OPMDataMerged[(OPMDataMerged[\"SEP\"]==SEP) \n",
    "                                                    & (OPMDataMerged[\"DATECODE\"]==DATECODE) \n",
    "                                                    & (OPMDataMerged[\"AGELVL\"]==AGELVL)].sample(SampleSize,  random_state=SampleSize))\n",
    "        SampledOPMStratumData.append(pd.concat(SampledOPMStratumDataList))\n",
    "        clear_display()\n",
    "    SampledOPMData = pd.concat(SampledOPMStratumData).reset_index()\n",
    "    del SampledOPMData[\"index\"]\n",
    "    pickleObject(SampledOPMData, FileName)\n",
    "    clear_display()\n",
    "\n",
    "    return SampledOPMData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a seed value equal to each strata sample size, we take random samples according to the computed sizes above. We loop through each Separation Type's Aggregated Strata Sample Sizes; Identify all observations matching on Datecode, Separation Type, and AgeLevel; and finally sample those observations with the computed sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "##Prof Data Sampling\n",
    "if os.path.isfile(PickleJarPath+\"/SampledOPMDataProf.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    SampledOPMDataProf = unpickleObject(\"SampledOPMDataProf\")\n",
    "else:\n",
    "    SampledOPMDataProf= SampleStrata(stratumProf, OPMDataMergedProf, \"SampledOPMDataProf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(SampledOPMDataProf))\n",
    "display(SampledOPMDataProf.head())\n",
    "display(pd.DataFrame({'StratCount' : SampledOPMDataProf.groupby([\"SEP\"]).size()}).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Analyze Missing Values\n",
    "filtered_msnoData = msno.nullity_sort(msno.nullity_filter(SampledOPMDataProf, filter='bottom', n=15, p=0.999), sort='descending')\n",
    "msno.matrix(filtered_msnoData)\n",
    "\n",
    "del filtered_msnoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "##Admin Data Sampling\n",
    "if os.path.isfile(PickleJarPath+\"/SampledOPMDataAdmin.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    SampledOPMDataAdmin = unpickleObject(\"SampledOPMDataAdmin\")\n",
    "else:\n",
    "    SampledOPMDataAdmin= SampleStrata(stratumAdmin, OPMDataMergedAdmin, \"SampledOPMDataAdmin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(SampledOPMDataAdmin))\n",
    "display(SampledOPMDataAdmin.head())\n",
    "display(pd.DataFrame({'StratCount' : SampledOPMDataAdmin.groupby([\"SEP\"]).size()}).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Analyze Missing Values\n",
    "filtered_msnoData = msno.nullity_sort(msno.nullity_filter(SampledOPMDataAdmin, filter='bottom', n=15, p=0.999), sort='descending')\n",
    "msno.matrix(filtered_msnoData)\n",
    "\n",
    "del filtered_msnoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Describe Summary for our Model Professional Subgroup for Modeling\n",
    "display(SampledOPMDataProf.describe().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "#OPMDataMerged.to_csv(\"OPMDataMerged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os.path.getsize(\"OPMDataMerged.csv\") #Display file size in bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Visualizations post-Data removal and sampling\n",
    "\n",
    "Chris... can you use the SampledOPMDataProf dataset, and re-run the Visuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "cols = list(SampledOPMDataProf.select_dtypes(include=['float64', 'int64']))\n",
    "cols.remove('BLS_FEDERAL_OtherSep_Rate')\n",
    "cols.remove('BLS_FEDERAL_Quits_Rate')\n",
    "cols.remove('BLS_FEDERAL_TotalSep_Level')\n",
    "cols.remove('BLS_FEDERAL_JobOpenings_Rate')\n",
    "cols.remove('BLS_FEDERAL_OtherSep_Level')\n",
    "cols.remove('BLS_FEDERAL_Quits_Level')\n",
    "cols.remove('BLS_FEDERAL_JobOpenings_Level')\n",
    "cols.remove('BLS_FEDERAL_Layoffs_Rate')\n",
    "cols.remove('BLS_FEDERAL_Layoffs_Level')\n",
    "cols.remove('BLS_FEDERAL_TotalSep_Rate')\n",
    "cols.append('SEP')\n",
    "display(cols)\n",
    "\n",
    "plotNumeric = SampledOPMDataProf[cols]\n",
    "\n",
    "# Create binary separation attribute for EDA correlation review\n",
    "#plotNumeric[\"SEP_bin\"] = plotNumeric.SEP.replace(\"NS\", 1)\n",
    "#plotNumeric.loc[plotNumeric['SEP_bin'] != 1, 'SEP_bin'] = 0\n",
    "#plotNumeric.SEP_bin = plotNumeric.SEP_bin.apply(pd.to_numeric)\n",
    "AttSplit = pd.get_dummies(plotNumeric['SEP'],prefix='SEP')\n",
    "display(AttSplit.head())\n",
    "plotNumeric = pd.concat((plotNumeric,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "display(plotNumeric.head())\n",
    "print(\"plotNumeric has {0} Records\".format(len(plotNumeric)))\n",
    "#print(plotNumeric.SEP_bin.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "sns.pairplot(plotNumeric.drop([\"SEP_NS\", \n",
    "                               \"SEP_SA\", \n",
    "                               \"SEP_SB\", \n",
    "                               \"SEP_SC\", \n",
    "                               \"SEP_SD\", \n",
    "                               \"SEP_SE\", \n",
    "                               \"SEP_SF\", \n",
    "                               \"SEP_SG\", \n",
    "                               \"SEP_SH\", \n",
    "                               \"SEP_SI\", \n",
    "                               \"SEP_SJ\", \n",
    "                               \"SEP_SK\", \n",
    "                               \"SEP_SL\"\n",
    "                              ], axis=1), hue = 'SEP', palette=\"hls\", plot_kws={\"s\": 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function modified from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n",
    "sns.set()\n",
    "\n",
    "def draw_histograms(df, variables, n_rows, n_cols):\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        df[var_name].hist(bins=20,ax=ax, color='#58D68D')\n",
    "        ax.set_title(var_name+\" Distribution\")\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "draw_histograms(plotNumeric.drop(['SEP',\n",
    "                                  \"SEP_NS\", \n",
    "                               \"SEP_SA\", \n",
    "                               \"SEP_SB\", \n",
    "                               \"SEP_SC\", \n",
    "                               \"SEP_SD\", \n",
    "                               \"SEP_SE\", \n",
    "                               \"SEP_SF\", \n",
    "                               \"SEP_SG\", \n",
    "                               \"SEP_SH\", \n",
    "                               \"SEP_SI\", \n",
    "                               \"SEP_SJ\", \n",
    "                               \"SEP_SK\", \n",
    "                               \"SEP_SL\"\n",
    "                              ], axis=1),\n",
    "                plotNumeric.drop(['SEP',\n",
    "                                  \"SEP_NS\", \n",
    "                               \"SEP_SA\", \n",
    "                               \"SEP_SB\", \n",
    "                               \"SEP_SC\", \n",
    "                               \"SEP_SD\", \n",
    "                               \"SEP_SE\", \n",
    "                               \"SEP_SF\", \n",
    "                               \"SEP_SG\", \n",
    "                               \"SEP_SH\", \n",
    "                               \"SEP_SI\", \n",
    "                               \"SEP_SJ\", \n",
    "                               \"SEP_SK\", \n",
    "                               \"SEP_SL\"\n",
    "                              ], axis=1).columns, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Inspired by http://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "#plt.matshow(plotNumeric.corr())\n",
    "\n",
    "sns.set(style='white')\n",
    "corr = plotNumeric.drop(['SEP'], axis=1).corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.set(font_scale=0.95)\n",
    "heatCorr = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "                       square=True, annot=True, linewidths=1,\n",
    "                       cbar_kws={\"shrink\": .5}, ax=ax, fmt='.1g')\n",
    "#heatCorr.\n",
    "ax.tick_params(labelsize=15)\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.tick_params(labelsize=15)\n",
    "\n",
    "sns.plt.show()\n",
    "#sns.heatmap(corr, annot=True, linewidths=0.01, cmap=cmap, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cols = list(SampledOPMDataProf.select_dtypes(include=['object']))\n",
    "dropCols = [\"LOCTYP\",\n",
    "            \"LOCTYPT\",\n",
    "            \"OCCTYP\",\n",
    "            \"OCCTYPT\",\n",
    "            \"PPTYP\",\n",
    "            \"PPTYPT\",\n",
    "            \"AGYTYP\",\n",
    "            \"OCCFAM\",\n",
    "            \"PPGROUP\",\n",
    "            \"PAYPLAN\",\n",
    "            \"TOATYP\",\n",
    "            \"WSTYP\",\n",
    "            \"AGYSUBT\",\n",
    "            \"AGELVL\",\n",
    "            \"LOSLVL\",\n",
    "            \"LOC\",\n",
    "            \"OCC\",\n",
    "            \"PATCO\",\n",
    "            \"SALLVL\",\n",
    "            \"TOA\",\n",
    "            \"WORKSCH\"]\n",
    "\n",
    "for i in dropCols:\n",
    "    if(i in list(SampledOPMDataProf.columns)): cols.remove(i)\n",
    "\n",
    "plotCat = SampledOPMDataProf[cols]\n",
    "display(plotCat.head())\n",
    "print(\"plotCat Has {0} Records\".format(len(plotCat)))\n",
    "print(\"Number of colums = \", len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in cols:\n",
    "    if i != 'SEP':\n",
    "        plt.figure(i) # Required to create new figure each loop rather than drawing over previous object\n",
    "        f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10), sharey=False)\n",
    "        sns.countplot(y=i, data=plotCat, color=\"lightblue\", ax=ax1);\n",
    "        sns.countplot(y=i, data=plotCat, hue=\"SEP\", palette=\"hls\", ax=ax2);\n",
    "        \n",
    "    if i == 'AGYSUB':\n",
    "        subCountPlot(i, 'SEP', 10000)\n",
    "    elif i == 'LOCT':\n",
    "        subCountPlot(i, 'SEP', 1000)\n",
    "    elif i == 'OCCT':\n",
    "        subCountPlot(i, 'SEP', 2000)\n",
    "    elif i == 'PPGRD':\n",
    "        subCountPlot(i, 'SEP', 6000)\n",
    "    elif i == 'AGYT':\n",
    "        subCountPlot(i, 'SEP', 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in cols:\n",
    "    if i != 'SEP':\n",
    "        percBarPlot(i, 'SEP', len(plotCat.SEP.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "\n",
    "sns.violinplot(x=\"PATCOT\", y=\"SALARY\", data=SampledOPMDataProf, split=True,\n",
    "               inner=\"quart\")\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"SEP\", y=\"SALARY\", data=SampledOPMDataProf, split=True,\n",
    "               inner=\"box\", scale=\"area\", cut=0)\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#sns.factorplot(x=\"SEP\", y=\"SALARY\", col=\"PATCOT\",\n",
    "#               data=SampledOPMDataProf,\n",
    "#               kind=\"violin\", split=True, aspect=.5, size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#sns.factorplot(x=\"SEP\", y=\"SALARY\", col=\"PATCOT\", data=SampledOPMDataProf,\n",
    "#               kind=\"violin\", split=True, aspect=.4, size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "g = sns.PairGrid(data=SampledOPMDataProf,\n",
    "                 x_vars=[\"SEP\",\"PATCOT\"],\n",
    "                 y_vars=[\"SALARY\", \"LOS\", \"LowerLimitAge\", \"YearsToRetirement\"],\n",
    "                 aspect=1, size=10)\n",
    "g.map(sns.violinplot, palette=\"pastel\", inner=\"quart\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several separation types we would like to either roll up, or remove altogether.\n",
    "\n",
    "We have chosen to roll separation into two Binary Categories. \n",
    "\n",
    "1) NS, Non-Separation comprised of:\n",
    "    a) NS, Non-Separation\n",
    "    b) SA,Transfer Out - Individual Transfer\n",
    "    c) SB,Transfer Out - Mass Transfer\n",
    "    d) SD,Retirement - Voluntary\n",
    "    e) SE,Retirement - Early Out\n",
    "    f) SF,Retirement - Disability\n",
    "    g) SG,Retirement - Other\n",
    "\n",
    "\n",
    "2) SC, Quit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SampledOPMDataProf = SampledOPMDataProf[((SampledOPMDataProf[\"SEP\"] == \"NS\") | (SampledOPMDataProf[\"SEP\"] == \"SD\") | (SampledOPMDataProf[\"SEP\"] == \"SE\") | (SampledOPMDataProf[\"SEP\"] == \"SF\") | (SampledOPMDataProf[\"SEP\"] == \"SH\") | (SampledOPMDataProf[\"SEP\"] == \"SA\") | (SampledOPMDataProf[\"SEP\"] == \"SB\") | (SampledOPMDataProf[\"SEP\"] == \"SC\"))]\n",
    "\n",
    "SampledOPMDataProf.loc[(SampledOPMDataProf[\"SEP\"] != \"SC\") , \"SEP\"]=\"NS\"\n",
    "\n",
    "SampledOPMDataAdmin = SampledOPMDataAdmin[((SampledOPMDataAdmin[\"SEP\"] == \"NS\") | (SampledOPMDataAdmin[\"SEP\"] == \"SD\") | (SampledOPMDataAdmin[\"SEP\"] == \"SE\") | (SampledOPMDataAdmin[\"SEP\"] == \"SF\") | (SampledOPMDataAdmin[\"SEP\"] == \"SH\") | (SampledOPMDataAdmin[\"SEP\"] == \"SA\") | (SampledOPMDataAdmin[\"SEP\"] == \"SB\") | (SampledOPMDataAdmin[\"SEP\"] == \"SC\"))]\n",
    "\n",
    "SampledOPMDataAdmin.loc[(SampledOPMDataAdmin[\"SEP\"] != \"SC\") , \"SEP\"]=\"NS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess Stratum SEP Counts for Prof, for use in sampling\n",
    "maxSize=7500\n",
    "stratumProf = pd.DataFrame({'StratCount' : SampledOPMDataProf.groupby([\"SEP\"]).size()}).reset_index()\n",
    "\n",
    "stratumProf.loc[stratumProf[\"StratCount\"]>maxSize,\"StratCountSample\"] = maxSize\n",
    "stratumProf.loc[stratumProf[\"StratCount\"]<=maxSize,\"StratCountSample\"] = stratumProf[\"StratCount\"]\n",
    "#else: stratum[\"StratCountSample\"] = stratum[\"StratCount\"]\n",
    "\n",
    "display(stratumProf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess Stratum SEP Counts for Admin, for use in sampling\n",
    "maxSize=7500\n",
    "stratumAdmin = pd.DataFrame({'StratCount' : SampledOPMDataAdmin.groupby([\"SEP\"]).size()}).reset_index()\n",
    "\n",
    "stratumAdmin.loc[stratumAdmin[\"StratCount\"]>maxSize,\"StratCountSample\"] = maxSize\n",
    "stratumAdmin.loc[stratumAdmin[\"StratCount\"]<=maxSize,\"StratCountSample\"] = stratumAdmin[\"StratCount\"]\n",
    "#else: stratum[\"StratCountSample\"] = stratum[\"StratCount\"]\n",
    "\n",
    "display(stratumAdmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SampledOPMDataProf= SampleStrata(stratumProf, SampledOPMDataProf, \"SampledOPMDataProfBinary\")\n",
    "SampledOPMDataAdmin= SampleStrata(stratumProf, SampledOPMDataAdmin, \"SampledOPMDataAdminBinary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Visualizations post-Data removal and second round of sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "cols = list(SampledOPMDataProf.select_dtypes(include=['float64', 'int64']))\n",
    "cols.remove('BLS_FEDERAL_OtherSep_Rate')\n",
    "cols.remove('BLS_FEDERAL_Quits_Rate')\n",
    "cols.remove('BLS_FEDERAL_TotalSep_Level')\n",
    "cols.remove('BLS_FEDERAL_JobOpenings_Rate')\n",
    "cols.remove('BLS_FEDERAL_OtherSep_Level')\n",
    "cols.remove('BLS_FEDERAL_Quits_Level')\n",
    "cols.remove('BLS_FEDERAL_JobOpenings_Level')\n",
    "cols.remove('BLS_FEDERAL_Layoffs_Rate')\n",
    "cols.remove('BLS_FEDERAL_Layoffs_Level')\n",
    "cols.remove('BLS_FEDERAL_TotalSep_Rate')\n",
    "cols.append('SEP')\n",
    "display(cols)\n",
    "\n",
    "plotNumeric = SampledOPMDataProf[cols]\n",
    "\n",
    "# Create binary separation attribute for EDA correlation review\n",
    "#plotNumeric[\"SEP_bin\"] = plotNumeric.SEP.replace(\"NS\", 1)\n",
    "#plotNumeric.loc[plotNumeric['SEP_bin'] != 1, 'SEP_bin'] = 0\n",
    "#plotNumeric.SEP_bin = plotNumeric.SEP_bin.apply(pd.to_numeric)\n",
    "AttSplit = pd.get_dummies(plotNumeric['SEP'],prefix='SEP')\n",
    "display(AttSplit.head())\n",
    "plotNumeric = pd.concat((plotNumeric,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "display(plotNumeric.head())\n",
    "print(\"plotNumeric has {0} Records\".format(len(plotNumeric)))\n",
    "#print(plotNumeric.SEP_bin.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "sns.pairplot(plotNumeric.drop(['SEP_NS',\n",
    "                               'SEP_SC'], axis=1), hue = 'SEP', palette=\"hls\", plot_kws={\"s\": 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function modified from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n",
    "sns.set()\n",
    "\n",
    "def draw_histograms(df, variables, n_rows, n_cols):\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        df[var_name].hist(bins=20,ax=ax, color='#58D68D')\n",
    "        ax.set_title(var_name+\" Distribution\")\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "draw_histograms(plotNumeric.drop(['SEP',\n",
    "                                  'SEP_NS',\n",
    "                                  'SEP_SC'\n",
    "                                 ], axis=1),\n",
    "                plotNumeric.drop(['SEP',\n",
    "                                  'SEP_NS',\n",
    "                                  'SEP_SC'\n",
    "                                 ], axis=1).columns, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Inspired by http://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "#plt.matshow(plotNumeric.corr())\n",
    "\n",
    "sns.set(style='white')\n",
    "corr = plotNumeric.drop(['SEP'], axis=1).corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.set(font_scale=0.95)\n",
    "heatCorr = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "                       square=True, annot=True, linewidths=1,\n",
    "                       cbar_kws={\"shrink\": .5}, ax=ax, fmt='.1g')\n",
    "#heatCorr.\n",
    "ax.tick_params(labelsize=15)\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.tick_params(labelsize=15)\n",
    "\n",
    "sns.plt.show()\n",
    "#sns.heatmap(corr, annot=True, linewidths=0.01, cmap=cmap, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cols = list(SampledOPMDataProf.select_dtypes(include=['object']))\n",
    "dropCols = [\"LOCTYP\",\n",
    "            \"LOCTYPT\",\n",
    "            \"OCCTYP\",\n",
    "            \"OCCTYPT\",\n",
    "            \"PPTYP\",\n",
    "            \"PPTYPT\",\n",
    "            \"AGYTYP\",\n",
    "            \"OCCFAM\",\n",
    "            \"PPGROUP\",\n",
    "            \"PAYPLAN\",\n",
    "            \"TOATYP\",\n",
    "            \"WSTYP\",\n",
    "            \"AGYSUBT\",\n",
    "            \"AGELVL\",\n",
    "            \"LOSLVL\",\n",
    "            \"LOC\",\n",
    "            \"OCC\",\n",
    "            \"PATCO\",\n",
    "            \"SALLVL\",\n",
    "            \"TOA\",\n",
    "            \"WORKSCH\"]\n",
    "\n",
    "for i in dropCols:\n",
    "    if(i in list(SampledOPMDataProf.columns)): cols.remove(i)\n",
    "\n",
    "plotCat = SampledOPMDataProf[cols]\n",
    "display(plotCat.head())\n",
    "print(\"plotCat Has {0} Records\".format(len(plotCat)))\n",
    "print(\"Number of colums = \", len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in cols:\n",
    "    if i != 'SEP':\n",
    "        plt.figure(i) # Required to create new figure each loop rather than drawing over previous object\n",
    "        f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10), sharey=False)\n",
    "        sns.countplot(y=i, data=plotCat, color=\"lightblue\", ax=ax1);\n",
    "        sns.countplot(y=i, data=plotCat, hue=\"SEP\", palette=\"hls\", ax=ax2);\n",
    "        \n",
    "    if i == 'AGYSUB':\n",
    "        subCountPlot(i, 'SEP', 10000)\n",
    "    elif i == 'LOCT':\n",
    "        subCountPlot(i, 'SEP', 1000)\n",
    "    elif i == 'OCCT':\n",
    "        subCountPlot(i, 'SEP', 2000)\n",
    "    elif i == 'PPGRD':\n",
    "        subCountPlot(i, 'SEP', 6000)\n",
    "    elif i == 'AGYT':\n",
    "        subCountPlot(i, 'SEP', 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in cols:\n",
    "    if i != 'SEP':\n",
    "        percBarPlot(i, 'SEP', len(plotCat.SEP.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "\n",
    "sns.violinplot(x=\"PATCOT\", y=\"SALARY\", data=SampledOPMDataProf, split=True,\n",
    "               inner=\"quart\")\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"SEP\", y=\"SALARY\", data=SampledOPMDataProf, split=True,\n",
    "               inner=\"box\", scale=\"area\", cut=0)\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#sns.factorplot(x=\"SEP\", y=\"SALARY\", col=\"PATCOT\",\n",
    "#               data=SampledOPMDataProf,\n",
    "#               kind=\"violin\", split=True, aspect=.5, size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#sns.factorplot(x=\"SEP\", y=\"SALARY\", col=\"PATCOT\", data=SampledOPMDataProf,\n",
    "#               kind=\"violin\", split=True, aspect=.4, size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "g = sns.PairGrid(data=SampledOPMDataProf,\n",
    "                 x_vars=[\"SEP\",\"PATCOT\"],\n",
    "                 y_vars=[\"SALARY\", \"LOS\", \"LowerLimitAge\", \"YearsToRetirement\"],\n",
    "                 aspect=1, size=10)\n",
    "g.map(sns.violinplot, palette=\"pastel\", inner=\"quart\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Attributes, and Remove Description Columns for Analysis Prep\n",
    "\n",
    "Now that we have the dataset sampled, we still have some legwork necessary to convert our categorical attributes into binary integer values. Below we walk through this process for the following Attributes:\n",
    "- AGELVL\n",
    "- LOC\n",
    "- SALLVL\n",
    "- TOA\n",
    "- OCCTYP\n",
    "- OCCFAM\n",
    "- PPTYP\n",
    "- PPGROUP\n",
    "- TOATYP\n",
    "\n",
    "Once these attributes have been encoded and description columns removed, we end up with a total of 2446 attributes in our dataset for analysis in our model generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up old objects no longer needed, to clear up memory\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Memory Usage before Cleanup: \", process.memory_info().rss)\n",
    "\n",
    "if 'AGELVL' in dir():\n",
    "    del AGELVL\n",
    "if 'AggIndAvgSalary' in dir():\n",
    "    del AggIndAvgSalary\n",
    "if 'AggIndAvgSalary2' in dir():\n",
    "    del AggIndAvgSalary2\n",
    "if 'AggSEPCount_EFDATE_LOC' in dir():\n",
    "    del AggSEPCount_EFDATE_LOC\n",
    "if 'AggSEPCount_EFDATE_OCC' in dir():\n",
    "    del AggSEPCount_EFDATE_OCC\n",
    "if 'AggStrat' in dir():\n",
    "    del AggStrat\n",
    "if 'DATECODE' in dir():\n",
    "    del DATECODE\n",
    "if 'EMPColList' in dir():\n",
    "    del EMPColList\n",
    "if 'EMPDataOrig4Q' in dir():\n",
    "    del EMPDataOrig4Q\n",
    "if 'maxSize' in dir():\n",
    "    del maxSize\n",
    "if 'OPMColList' in dir():\n",
    "    del OPMColList\n",
    "if 'OPMDataFiles' in dir():\n",
    "    del OPMDataFiles\n",
    "if 'OPMDataList' in dir():\n",
    "    del OPMDataList\n",
    "if 'OPMDataMerged' in dir():\n",
    "    del OPMDataMerged\n",
    "if 'OPMDataOrig' in dir():\n",
    "    del OPMDataOrig\n",
    "if 'SEP' in dir():\n",
    "    del SEP\n",
    "if 'SampleSize' in dir():\n",
    "    del SampleSize\n",
    "if 'SampledOPMStratumData' in dir():\n",
    "    del SampledOPMStratumData\n",
    "if 'SampledOPMStratumDataList' in dir():\n",
    "    del SampledOPMStratumDataList\n",
    "if 'StratCountSample' in dir():\n",
    "    del StratCountSample\n",
    "if 'StratSampleSize' in dir():\n",
    "    del StratSampleSize\n",
    "if 'JTL' in dir():\n",
    "    del JTL\n",
    "    \n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Memory Usage after Cleanup: \", process.memory_info().rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SampledOPMDataProf.head())\n",
    "SampledOPMDataProf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.isfile(PickleJarPath+\"/OPMAnalysisDataNoFamBinary.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    OPMAnalysisDataNoFamBinary = unpickleObject(\"OPMAnalysisDataNoFamBinary\")\n",
    "else:\n",
    "\n",
    "    OPMAnalysisDataNoFamBinary = SampledOPMDataProf.copy()\n",
    "\n",
    "    cols = [\"GENDER\",\n",
    "            \"DATECODE\",\n",
    "            \"QTR\",\n",
    "            \"COUNT\",\n",
    "            \"AGYTYPT\",\n",
    "            \"AGYT\",\n",
    "            \"AGYSUB\",\n",
    "            \"AGYSUBT\",\n",
    "            \"QTR\",\n",
    "            \"AGELVLT\",\n",
    "            \"LOSLVL\",\n",
    "            \"LOSLVLT\",\n",
    "            \"LOCTYPT\",\n",
    "            \"LOCT\",\n",
    "            \"OCCTYP\",\n",
    "            \"OCCTYPT\",\n",
    "            \"OCCFAM\",\n",
    "            \"OCCFAMT\",\n",
    "            \"OCC\",\n",
    "            \"OCCT\",\n",
    "            \"PATCO\",\n",
    "            \"PPGRD\",\n",
    "            \"PATCOT\",\n",
    "            \"PPTYPT\",\n",
    "            \"PPGROUPT\",\n",
    "            \"PAYPLAN\",\n",
    "            \"PAYPLANT\",\n",
    "            \"SALLVLT\",\n",
    "            \"TOATYPT\",\n",
    "            \"TOAT\",\n",
    "            \"WSTYP\",\n",
    "            \"WSTYPT\",\n",
    "            \"WORKSCH\",\n",
    "            \"WORKSCHT\",\n",
    "            \"SALARY\",\n",
    "            \"LOS\",\n",
    "            \"SEPCount_EFDATE_OCC\",\n",
    "            \"SEPCount_EFDATE_LOC\"\n",
    "           ]\n",
    "\n",
    "\n",
    "\n",
    "    #delete cols from analysis data\n",
    "    for col in cols:\n",
    "        if col in list(OPMAnalysisDataNoFamBinary.columns):\n",
    "            del OPMAnalysisDataNoFamBinary[col]\n",
    "\n",
    "    OPMAnalysisDataNoFamBinary.info()\n",
    "\n",
    "    cols = [\"AGELVL\",\n",
    "            \"LOC\",\n",
    "            \"SALLVL\",\n",
    "            \"TOA\",\n",
    "            \"AGYTYP\",\n",
    "            \"AGY\",\n",
    "            \"LOCTYP\",\n",
    "            \"PPTYP\",\n",
    "            \"PPGROUP\",\n",
    "            \"TOATYP\"\n",
    "           ]\n",
    "\n",
    "    #Split Values for cols \n",
    "    for col in cols:\n",
    "        if col in list(OPMAnalysisDataNoFamBinary.columns):\n",
    "            AttSplit = pd.get_dummies(OPMAnalysisDataNoFamBinary[col],prefix=col)\n",
    "            display(AttSplit.head())\n",
    "            OPMAnalysisDataNoFamBinary = pd.concat((OPMAnalysisDataNoFamBinary,AttSplit),axis=1) # add back into the dataframe\n",
    "            del OPMAnalysisDataNoFamBinary[col]\n",
    "\n",
    "    pickleObject(OPMAnalysisDataNoFamBinary, \"OPMAnalysisDataNoFamBinary\")\n",
    "        \n",
    "display(OPMAnalysisDataNoFamBinary.head())\n",
    "print(\"Number of Columns: \",len(OPMAnalysisDataNoFamBinary.columns))\n",
    "OPMAnalysisDataNoFamBinary.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a display of all remaining attributes and their corresponding data types for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_type = []\n",
    "for idx, col in enumerate(OPMAnalysisDataNoFamBinary.columns):\n",
    "    data_type.append(OPMAnalysisDataNoFamBinary.dtypes[idx])\n",
    "\n",
    "summary_df = {'Attribute Name' : pd.Series(OPMAnalysisDataNoFamBinary.columns, index = range(len(OPMAnalysisDataNoFamBinary.columns))), 'Data Type' : pd.Series(data_type, index = range(len(OPMAnalysisDataNoFamBinary.columns)))}\n",
    "summary_df = pd.DataFrame(summary_df)\n",
    "display(summary_df)\n",
    "\n",
    "del data_type, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction using Principal Component Analysis\n",
    "\n",
    "We also scale the data values to remove bias in our models due to different attribute scales. Without scaling the data, attributes such as SALARY and LOS would carry heavier weights when compared against the binary encoded attributes and BLS data. This would cause unbalanced and improperly analyzed data for model creation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "OPMScaledAnalysisData = OPMAnalysisDataNoFamBinary.copy()\n",
    "del OPMScaledAnalysisData[\"SEP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "OPMAnalysisScalerFit = MinMaxScaler().fit(OPMScaledAnalysisData)\n",
    "## Pickle for later re-use if needed\n",
    "pickleObject(OPMAnalysisScalerFit, \"OPMAnalysisScalerFit\")\n",
    "\n",
    "OPMScaledAnalysisData = pd.DataFrame(OPMAnalysisScalerFit.transform(OPMScaledAnalysisData), columns = OPMScaledAnalysisData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(OPMScaledAnalysisData.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Principal Components defined\n",
    "\n",
    "Our objective, is to reduce dimensionality through identification of principal components. We have chosen to use the full column input (99) as the maximum number of components to be produced. Given our hopes are to reduce the number of attributes needed for a model, we expect to find much smaller than 99 as our Principal components which explain over 80% variance within the dataset. We will review each component's explained variance further to determine the proper number of components to be included later during model generation. Note randomized PCA was chosen in order to use singular value decomposition in our dimensionality reduction efforts due to the large size of our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seed = len(OPMScaledAnalysisData)\n",
    "\n",
    "print(OPMScaledAnalysisData.shape)\n",
    "pca_class = PCA(n_components=len(OPMScaledAnalysisData.columns), svd_solver='randomized', random_state=seed)\n",
    "\n",
    "pca_class.fit(OPMScaledAnalysisData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the resulting components have been ordered by eigenvector value and these values portrayed as ratios of variance explained by each component. In order to identify the principal components to be included during model generation, we review the rate at which explained variance decreases in significance from one principal component to the next. Accompanying these proportion values is a scree plot representing these same values in visual form. By plotting the scree plot, it is easier to judge where this rate of decreasing explained variance occurs. Note the rate of change in explained variance among the first 8 principal components, with another less significant change through the 22th component. After the 22th component, the rate of decreasing explained variance begins to somewhat flatten out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#The amount of variance that each PC explains\n",
    "var= pca_class.explained_variance_ratio_\n",
    "\n",
    "sns.set(font_scale=1.7)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(range(1,len(OPMScaledAnalysisData.columns)+1), var*100, marker = '.', color = 'red', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.axis([0, len(OPMScaledAnalysisData.columns)+1, -0.1, 9])\n",
    "plt.annotate('22nd Component', xy=(22, 1.2), xytext=(40, 4),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(var, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now referring to the cumulative variance values and associated plot below, it may be seen that the cumulative variance increases in a fairly consistent parabola curve. In attempts to acheive a cumulative variance explained of greater than 80%, we end at 22 principal components. For this reason, 22 principal components may be selected as being the most appropriate for separation classification modeling given the variables among these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca_class.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(range(1,len(OPMScaledAnalysisData.columns)+1), var1, marker = '.', color = 'green', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance (Sum %)')\n",
    "plt.title('Cumulative Variance Plot')\n",
    "plt.axis([0, len(OPMScaledAnalysisData.columns)+1, 10, len(OPMScaledAnalysisData.columns)+1])\n",
    "plt.annotate('22nd Component', xy=(22, 80.54), xytext=(40, 60),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to analyze the first 4 component Feature Loadings more carefully. See below, plots of the top 10 loadings for each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 12)\n",
    "fig = plt.figure()\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rc('xtick', labelsize=15)\n",
    "plt.rc('ytick', labelsize=15) \n",
    "for i in range(0,4):\n",
    "    components = pd.Series(pca_class.components_[i], index=OPMScaledAnalysisData.columns)\n",
    "\n",
    "    maxcomponent = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(10)).index)\n",
    "\n",
    "    matplotlib.rc('xtick', labelsize=12)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(2,2,i + 1)\n",
    "       \n",
    "    weightsplot = pd.Series(components, index=maxcomponent)\n",
    "    weightsplot.plot(title = \"Principal Component \"+ str(i+1), kind='bar', color = 'Tomato', ax = ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxPC = 22\n",
    "\n",
    "PCList = []\n",
    "for i in range(0,MaxPC):\n",
    "    components = pd.Series(pca_class.components_[i], index=OPMScaledAnalysisData.columns)\n",
    "\n",
    "    maxcomponent = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "    PCList.append(maxcomponent)\n",
    "\n",
    "PCList = pd.concat(PCList).drop_duplicates().sort_values(ascending=True).reset_index(drop = True)\n",
    "print(PCList)\n",
    "PCList = list(PCList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total of 50 features of the original 99 are identified, by taking the top 15 feature loadings within the first 22 components as determined above as the appropriate components to maximize variance explained. We may now, optionally utilize these 50 features identified, or utilize principal component vectors for analysis in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Separation Response Weights\n",
    "Due to the unproportional number of observations in each separation type in our dataset, we need to create weightings. using SciKit's class_weight algorithm, we compute an array of weights to be used downstream in our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Separation\n",
    "We have chosen to utilize Stratified KFold Cross Validation for our classification analysis, with 5 folds. This means, that from our original sample size of 16,638, each \"fold\" will save off approximately 20% as test observations utilizing the rest as training observations all while keeping the ratio of classes equal amongst customers and subscribers. This process will occur through 5 iterations, or folds, to allow us to cross validate our results amongst different test/train combinations. We have utilized a random_state seed equal to the length of the original sampled dataset to ensure reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = len(OPMAnalysisDataNoFamBinary)\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 5, random_state = seed)\n",
    "print(OPMAnalysisDataNoFamBinary.shape)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classification\n",
    "\n",
    "**Max Depth**\n",
    "The maximum depth (levels) in the tree. When a value is set, the tree may not split further once this level has been met regardless of how many nodes are in the leaf. \n",
    "\n",
    "**Max Features**\n",
    "Number of features to consider when looking for a split. \n",
    "\n",
    "**Minimum Samples in Leaf**\n",
    "Minimum number of samples required to be in a leaf node. Splits may not occur which cause the number of samples in a leaf to be less than this value. Too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**Minimum Samples to Split**\n",
    "Minimum number fo samples required to split a node. Care was taken during parameter tests to keep the ratio between Min Samples in Leaf and Min Samples to Split equal to that of the default values (1:2). This was done to allow an even 50/50 split on nodes which match the lowest granularity split criteria. similar to the min samples in leaf, too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**n_estimators**\n",
    "Number of Trees generated in the forest. Increasing the number of trees, in our models increased accuracy while decreasing performance. We tuned to provide output that completed all 10 iterations in under 10 minutes.\n",
    "\n",
    "###Not Complete#### After 13 iterations of modifying the above parameters, we land on a final winner based on the highest average Accuracy value across all iterations. Average Accuracy values in our 10 test/train iterations ranged from 70.2668 % from default inputs of the random forest classification model to a value of 72.5192 % in the best tuned model fit. Although the run-time of this model parameter choice is the largest performed, we decided to remain with these inputs due to the amount increase in accuracy. As mentioned previously, we tuned the n_estimators parameter to ensure we stayed under 10 minutes execution. Parameter inputs for the final Random Forest Classification model with the KD Tree Algorithm are as follows: ###Not Complete#### \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>n_estimators</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>TBD</th>\n",
    "      <td>TBD</td>\n",
    "      <td>TBD</td>\n",
    "      <td>TBD</td>\n",
    "      <td>TBD</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "def rfc_explor(n_estimators,\n",
    "               max_features,\n",
    "               max_depth, \n",
    "               min_samples_split,\n",
    "               min_samples_leaf,\n",
    "               Data        = OPMAnalysisDataNoFam,\n",
    "               cols        = PCList,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    X = Data[cols].as_matrix()\n",
    "    \n",
    "    rfc_clf = RandomForestClassifier(n_estimators=n_estimators, max_features = max_features, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, class_weight = \"balanced\", n_jobs=-1, random_state = seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler', MinMaxScaler()),\n",
    "         ('CLF',rfc_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    print(TotalTime)\n",
    "    print(accuracy)\n",
    "    \n",
    "    return accuracy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "def rfc_explor_w_PCA(n_estimators,\n",
    "               max_features,\n",
    "               max_depth, \n",
    "               min_samples_split,\n",
    "               min_samples_leaf,\n",
    "               PCA,\n",
    "               Data        = OPMAnalysisDataNoFam,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    X = Data.drop(\"SEP\", axis=1).as_matrix()\n",
    "    \n",
    "    rfc_clf = RandomForestClassifier(n_estimators=n_estimators, max_features = max_features, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, class_weight = \"balanced\", n_jobs=-1, random_state = seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler', MinMaxScaler()),\n",
    "         ('PCA', PCA),\n",
    "         ('CLF',rfc_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "def rfc_explor_w_PCA(n_estimators,\n",
    "               max_features,\n",
    "               max_depth, \n",
    "               min_samples_split,\n",
    "               min_samples_leaf,\n",
    "               PCA,\n",
    "               Data        = OPMAnalysisDataNoFam,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    X = Data.drop(\"SEP\", axis=1).as_matrix()\n",
    "    \n",
    "    rfc_clf = RandomForestClassifier(n_estimators=n_estimators, max_features = max_features, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, class_weight = \"balanced\", n_jobs=-1, random_state = seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler', MinMaxScaler()),\n",
    "         ('PCA', PCA),\n",
    "         ('CLF',rfc_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have created a function to be re-used for our cross-validation Accuracy Scores. Inputs of PCA components, Model CLF object, original sample data, and a CV containing our test/train splits allow us to easily produce an array of Accuracy Scores for the different permutations of models tested. A XXXXXXTBDXXXXX plot is also displayed depicting a view of the misclassification values for each iteration. Finally, a confusion matrix is displayed for the last test/train iteration for further interpretation on results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "acclist = [] \n",
    "fullColumns = list(OPMAnalysisDataNoFam.columns)\n",
    "\n",
    "for i in fullColumns:\n",
    "    if i == \"SEP\": fullColumns.remove(i)\n",
    "\n",
    "n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10  , 5    , 15   ]  \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "\n",
    "##Model with all Raw Scaled Features\n",
    "for i in range(0,len(n_estimators)):\n",
    "    acclist.append(rfc_explor(n_estimators      = n_estimators[i],\n",
    "                              max_features      = max_features[i],\n",
    "                              max_depth         = max_depth[i],\n",
    "                              min_samples_split = min_samples_split[i],\n",
    "                              min_samples_leaf  = min_samples_leaf[i],\n",
    "                              cols              = fullColumns\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({  \"ModelVersion\": \"All Raw Features\",\n",
    "                                                \"n_estimators\": n_estimators,          \n",
    "                                                \"max_features\": max_features,         \n",
    "                                                \"max_depth\": max_depth,        \n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion', 'max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "del rfcdf, acclist\n",
    "\n",
    "acclist = []\n",
    "\n",
    "n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10  , 5    , 15   ]  \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "\n",
    "## Model with only top 15 raw Scaled Principal Features \n",
    "for i in range(0,len(n_estimators)):\n",
    "    acclist.append(rfc_explor(n_estimators      = n_estimators[i],\n",
    "                              max_features      = max_features[i],\n",
    "                              max_depth         = max_depth[i],\n",
    "                              min_samples_split = min_samples_split[i],\n",
    "                              min_samples_leaf  = min_samples_leaf[i]\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({  \"ModelVersion\": \"Top 15 Raw from PC\",\n",
    "                                                \"n_estimators\": n_estimators,          \n",
    "                                                \"max_features\": max_features,         \n",
    "                                                \"max_depth\": max_depth,        \n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion', 'max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "del rfcdf, acclist\n",
    "\n",
    "### Model with PCA\n",
    "acclist = []\n",
    "\n",
    "n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10  , 5    , 15   ]  \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "\n",
    "for i in range(0,len(n_estimators)):\n",
    "    acclist.append(rfc_explor_w_PCA(n_estimators      = n_estimators[i],\n",
    "                                    max_features      = max_features[i],\n",
    "                                    max_depth         = max_depth[i],\n",
    "                                    min_samples_split = min_samples_split[i],\n",
    "                                    min_samples_leaf  = min_samples_leaf[i],\n",
    "                                    PCA               = PCA(n_components=22, svd_solver='randomized', random_state = seed)\n",
    "                                   )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({  \"ModelVersion\": \"With PCA\",\n",
    "                                                \"n_estimators\": n_estimators,          \n",
    "                                                \"max_features\": max_features,         \n",
    "                                                \"max_depth\": max_depth,        \n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion', 'max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "\n",
    "#'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    #This function prints and plots the confusion matrix.\n",
    "    #Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = (18, 6)\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.rc('xtick', labelsize=18)\n",
    "    plt.rc('ytick', labelsize=18) \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize = 18)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j],2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize = 18)\n",
    "    plt.xlabel('Predicted label', fontsize = 18)\n",
    "\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "def compute_kfold_scores_Classification( clf,\n",
    "                                         Data     = OPMAnalysisDataNoFam,\n",
    "                                         cols     = PCList,\n",
    "                                         cv       = cv):\n",
    "\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    y = np.where(y == 'NS', 0, \n",
    "                 np.where(y == 'SA', 1,\n",
    "                          np.where(y == 'SC', 2,\n",
    "                                   np.where(y == 'SD', 3,\n",
    "                                            np.where(y == 'SH', 4,\n",
    "                                                     5\n",
    "                                                    )\n",
    "                                           )\n",
    "                                  )\n",
    "                         )\n",
    "                )\n",
    "    \n",
    "    X = Data[cols].as_matrix()\n",
    "\n",
    "\n",
    "    # Run classifier with cross-validation and plot ROC curves\n",
    "\n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler', MinMaxScaler()),\n",
    "         ('CLF',clf)]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    accuracy = []\n",
    "    #logloss = []\n",
    "    \n",
    "    for (train, test), color in zip(cv.split(X, y), colors):\n",
    "        clf_pipe.fit(X[train],y[train])  # train object\n",
    "        y_hat = clf_pipe.predict(X[test]) # get test set preditions\n",
    "        \n",
    "        a = float(mt.accuracy_score(y[test],y_hat))\n",
    "        #l = float(mt.log_loss(y[test], y_hat))\n",
    "        \n",
    "        accuracy.append(round(a,5)) \n",
    "\n",
    "        #logloss.append(round(l,5)) \n",
    "    \n",
    "    #print(\"Accuracy Ratings across all iterations: {0}\\n\\n\\\n",
    "#Average Accuracy: {1}\\n\\n\\\n",
    "#Log Loss Values across all iterations: {2}\\n\\n\\\n",
    "#Average Log Loss: {3}\\n\".format(accuracy, round(sum(accuracy)/len(accuracy),5), logloss,round(sum(logloss)/len(logloss),5)))\n",
    "\n",
    "    print(\"Accuracy Ratings across all iterations: {0}\\n\\n\\\n",
    "Average Accuracy: {1}\\n\".format(accuracy, round(sum(accuracy)/len(accuracy),5)))\n",
    "\n",
    "    \n",
    "    ytestnames = np.where(y[test] ==  0,'NS', \n",
    "                          np.where(y[test] ==  1,'SA',\n",
    "                                   np.where(y[test] ==  2,'SC',\n",
    "                                            np.where(y[test] ==  3,'SD',\n",
    "                                                     np.where(y[test] ==  4,'SH',\n",
    "                                                              'SI'\n",
    "                                                             )\n",
    "                                                    )\n",
    "                                           )\n",
    "                                  )\n",
    "                         )\n",
    "    \n",
    "    yhatnames  = np.where(y_hat ==  0,'NS', \n",
    "                          np.where(y_hat ==  1,'SA',\n",
    "                                   np.where(y_hat ==  2,'SC',\n",
    "                                            np.where(y_hat ==  3,'SD',\n",
    "                                                     np.where(y_hat ==  4,'SH',\n",
    "                                                              'SI'\n",
    "                                                             )\n",
    "                                                    )\n",
    "                                           )\n",
    "                                  )\n",
    "                         )\n",
    "    #print(set(list(y_hat)))\n",
    "    print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(ytestnames, yhatnames, rownames = ['True'], colnames = ['Predicted'], margins = True)))\n",
    "        \n",
    "        # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusion_matrix(y[test], y_hat), \n",
    "                          classes   =[\"NS\",  \"SA\",   \"SC\", \"SD\",  \"SI\"], \n",
    "                          normalize =True,\n",
    "                          title     ='Confusion matrix, with normalization')\n",
    "    \n",
    "    return clf_pipe.named_steps['CLF'], accuracy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "rfc_clf = RandomForestClassifier(n_estimators       = 15, \n",
    "                                 max_features       = 14, \n",
    "                                 max_depth          = 1000.0, \n",
    "                                 min_samples_split  = 50, \n",
    "                                 min_samples_leaf   = 25, \n",
    "                                 class_weight       = \"balanced\",\n",
    "                                 n_jobs             = -1, \n",
    "                                 random_state       = seed) # get object\n",
    "    \n",
    "rfc_clf, rfc_acc = compute_kfold_scores_Classification(rfc_clf, cols = fullColumns)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "list(OPMAnalysisDataNoFam.SEP.unique())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**Experimenting with multiclass ROC curves below.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%%time\n",
    "\n",
    "def compute_kfold_scores_Classification( clf,\n",
    "                                         Data     = OPMAnalysisDataNoFam,\n",
    "                                         cols     = PCList,\n",
    "                                         cv       = cv):\n",
    "\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    y = np.where(y == 'NS', 0, \n",
    "                 np.where(y == 'SA', 1,\n",
    "                          np.where(y == 'SC', 2,\n",
    "                                   np.where(y == 'SD', 3,\n",
    "                                            np.where(y == 'SH', 4,\n",
    "                                                     5\n",
    "                                                    )\n",
    "                                           )\n",
    "                                  )\n",
    "                         )\n",
    "                )\n",
    "    \n",
    "    X = Data[cols].as_matrix()\n",
    "    \n",
    "    # Binarize the output\n",
    "    y_bin = label_binarize(Data[\"SEP\"].values, list(Data.SEP.unique()))\n",
    "    n_classes = y_bin.shape[1]\n",
    "\n",
    "    # Run classifier with cross-validation and plot ROC curves\n",
    "\n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler', MinMaxScaler()),\n",
    "         ('CLF',clf)]\n",
    "    )\n",
    "    \n",
    "    colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'pink', 'darkred', 'dimgray', 'maroon', 'coral'])\n",
    "\n",
    "    accuracy = []\n",
    "    #logloss = []\n",
    "    \n",
    "    for (train, test), color in zip(cv.split(X, y), colors):\n",
    "        clf_pipe.fit(X[train],y[train])  # train object\n",
    "        y_hat = clf_pipe.predict(X[test]) # get test set preditions\n",
    "        \n",
    "        a = float(mt.accuracy_score(y[test],y_hat))\n",
    "        #l = float(mt.log_loss(y[test], y_hat))\n",
    "        \n",
    "        accuracy.append(round(a,5)) \n",
    "\n",
    "        #logloss.append(round(l,5))\n",
    "        \n",
    "        # Compute ROC curve and area the curve\n",
    "        #fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "        #mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        #mean_tpr[0] = 0.0\n",
    "        #roc_auc = auc(fpr, tpr)\n",
    "        #\n",
    "        #plt.rcParams['figure.figsize'] = (12, 6)\n",
    "        #\n",
    "        #plt.plot(fpr, tpr, lw=lw, color=color,\n",
    "        #         label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "#\n",
    "        #i += 1\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y[test][:, i], y_hat[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Plot of a ROC curve for a specific class\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    #print(\"Accuracy Ratings across all iterations: {0}\\n\\n\\\n",
    "#Average Accuracy: {1}\\n\\n\\\n",
    "#Log Loss Values across all iterations: {2}\\n\\n\\\n",
    "#Average Log Loss: {3}\\n\".format(accuracy, round(sum(accuracy)/len(accuracy),5), logloss,round(sum(logloss)/len(logloss),5)))\n",
    "\n",
    "    print(\"Accuracy Ratings across all iterations: {0}\\n\\n\\\n",
    "Average Accuracy: {1}\\n\".format(accuracy, round(sum(accuracy)/len(accuracy),5)))\n",
    "\n",
    "    \n",
    "    ytestnames = np.where(y[test] ==  0,'NS', \n",
    "                          np.where(y[test] ==  1,'SA',\n",
    "                                   np.where(y[test] ==  2,'SC',\n",
    "                                            np.where(y[test] ==  3,'SD',\n",
    "                                                     np.where(y[test] ==  4,'SH',\n",
    "                                                              'SI'\n",
    "                                                             )\n",
    "                                                    )\n",
    "                                           )\n",
    "                                  )\n",
    "                         )\n",
    "    \n",
    "    yhatnames  = np.where(y_hat ==  0,'NS', \n",
    "                          np.where(y_hat ==  1,'SA',\n",
    "                                   np.where(y_hat ==  2,'SC',\n",
    "                                            np.where(y_hat ==  3,'SD',\n",
    "                                                     np.where(y_hat ==  4,'SH',\n",
    "                                                              'SI'\n",
    "                                                             )\n",
    "                                                    )\n",
    "                                           )\n",
    "                                  )\n",
    "                         )\n",
    "    #print(set(list(y_hat)))\n",
    "    print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(ytestnames, yhatnames, rownames = ['True'], colnames = ['Predicted'], margins = True)))\n",
    "        \n",
    "        # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusion_matrix(y[test], y_hat), \n",
    "                          classes   =[\"NS\",  \"SA\",   \"SC\", \"SD\",  \"SH\",  \"SI\"], \n",
    "                          normalize =True,\n",
    "                          title     ='Confusion matrix, with normalization')\n",
    "    \n",
    "    return clf_pipe.named_steps['CLF'], accuracy'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''%%time\n",
    "\n",
    "rfc_clf = OneVsRestClassifier(RandomForestClassifier(n_estimators       = 15, \n",
    "                                 max_features       = 14, \n",
    "                                 max_depth          = 1000.0, \n",
    "                                 min_samples_split  = 50, \n",
    "                                 min_samples_leaf   = 25, \n",
    "                                 class_weight       = \"balanced\",\n",
    "                                 n_jobs             = -1, \n",
    "                                 random_state       = seed)) # get object\n",
    "    \n",
    "rfc_clf, rfc_acc = compute_kfold_scores_Classification(rfc_clf, cols = fullColumns)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''%%time\n",
    "\n",
    "rfc_clf = OneVsRestClassifier(RandomForestClassifier(n_estimators       = 15, \n",
    "                                 max_features       = 14, \n",
    "                                 max_depth          = 1000.0, \n",
    "                                 min_samples_split  = 50, \n",
    "                                 min_samples_leaf   = 25, \n",
    "                                 class_weight       = \"balanced\",\n",
    "                                 n_jobs             = -1, \n",
    "                                 random_state       = seed)) # get object\n",
    "\n",
    "y = OPMAnalysisDataNoFam[\"SEP\"].values # get the labels we want\n",
    "    \n",
    "y = np.where(y == 'NS', 0, \n",
    "             np.where(y == 'SA', 1,\n",
    "                      np.where(y == 'SC', 2,\n",
    "                               np.where(y == 'SD', 3,\n",
    "                                        np.where(y == 'SH', 4,\n",
    "                                                 5\n",
    "                                                )\n",
    "                                       )\n",
    "                              )\n",
    "                     )\n",
    "            )\n",
    "\n",
    "X = OPMAnalysisDataNoFam[fullColumns].as_matrix()\n",
    "\n",
    "# Binarize the output\n",
    "#y_bin = label_binarize(OPMAnalysisDataNoFam[\"SEP\"].values, list(OPMAnalysisDataNoFam.SEP.unique()))\n",
    "n_classes = len(set(y))\n",
    "\n",
    "#classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "#                                 random_state=random_state))\n",
    "#y_score = rfc_clf.fit(X[train], y[train]).decision_function(X_test)\n",
    "#\n",
    "colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'pink', 'darkred', 'dimgray', 'maroon', 'coral'])\n",
    "#\n",
    "#accuracy = []\n",
    "#\n",
    "\n",
    "#for (train, test), color in zip(cv.split(X, y), colors):\n",
    "#    probas_ = rfc_clf.fit(X[train], y[train]).predict_proba(X[test])    \n",
    "#    \n",
    "#    #rfc_clf.fit(X[train],y[train])  # train object\n",
    "#    y_hat = rfc_clf.predict(X[test]) # get test set preditions\n",
    "#    #y_hat = rfc_clf.fit(X[train],y[train]).decision_function(X[test])\n",
    "#    \n",
    "#    fpr = dict()\n",
    "#    tpr = dict()\n",
    "#    roc_auc = dict()\n",
    "#    for i in range(n_classes):\n",
    "#        #fpr[i], tpr[i], _ = roc_curve(y[test][:, i], y_hat[:, i])\n",
    "#        print(len(probas_[:, i]))\n",
    "#        print(y[test])\n",
    "#        #fpr[i], tpr[i], thresholds = roc_curve(y[test][:, i], probas_[:, i])\n",
    "#        #roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=seed)\n",
    "\n",
    "probas_ = rfc_clf.fit(X_train, y_train).predict_proba(X_test)    \n",
    "    \n",
    "#rfc_clf.fit(X[train],y[train])  # train object\n",
    "#y_hat = rfc_clf.predict(X_test) # get test set preditions\n",
    "#y_hat = rfc_clf.fit(X_train,y_train).decision_function(X_test)\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    #fpr[i], tpr[i], _ = roc_curve(y[test][:, i], y_hat[:, i])\n",
    "    print(probas_[:, i])\n",
    "    print(y_test)\n",
    "    #fpr[i], tpr[i], thresholds = roc_curve(y[test][i], probas_[:, i])\n",
    "    #roc_auc[i] = auc(fpr[i], tpr[i])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building a Binary Classification model\n",
    "### Reducing data to Non-Separation and Quit Separation Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Separation Response Weights\n",
    "As was done before, we assess weights across classes. Since stratification was performed previously, we have equal weights. Thus, we can ignore weighting in our binary classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPMClassWeights = class_weight.compute_class_weight(\"balanced\", OPMAnalysisDataNoFamBinary[\"SEP\"].drop_duplicates(), OPMAnalysisDataNoFamBinary[\"SEP\"])\n",
    "\n",
    "display(stratumProf.merge(pd.DataFrame({\"Weight\": OPMClassWeights, \"SEP\": OPMAnalysisDataNoFamBinary[\"SEP\"].drop_duplicates()}),on=\"SEP\", how=\"inner\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Separation\n",
    "We have chosen to utilize Stratified KFold Cross Validation for our classification analysis, with 5 folds. This means, that from our original sample size of 8002, each \"fold\" will save off approximately 20% as test observations utilizing the rest as training observations all while keeping the ratio of classes equal amongst customers and subscribers. This process will occur through 5 iterations, or folds, to allow us to cross validate our results amongst different test/train combinations. We have utilized a random_state seed equal to the length of the original sampled dataset to ensure reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = len(OPMAnalysisDataNoFamBinary)\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 5, random_state = seed)\n",
    "print(OPMAnalysisDataNoFamBinary.shape)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classification\n",
    "\n",
    "**Max Depth**\n",
    "The maximum depth (levels) in the tree. When a value is set, the tree may not split further once this level has been met regardless of how many nodes are in the leaf. \n",
    "\n",
    "**Max Features**\n",
    "Number of features to consider when looking for a split. \n",
    "\n",
    "**Minimum Samples in Leaf**\n",
    "Minimum number of samples required to be in a leaf node. Splits may not occur which cause the number of samples in a leaf to be less than this value. Too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**Minimum Samples to Split**\n",
    "Minimum number fo samples required to split a node. Care was taken during parameter tests to keep the ratio between Min Samples in Leaf and Min Samples to Split equal to that of the default values (1:2). This was done to allow an even 50/50 split on nodes which match the lowest granularity split criteria. similar to the min samples in leaf, too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**n_estimators**\n",
    "Number of Trees generated in the forest. Increasing the number of trees, in our models increased accuracy while decreasing performance. We tuned to provide output that completed all 10 iterations in under 10 minutes.\n",
    "\n",
    "###Not Complete#### After 13 iterations of modifying the above parameters, we land on a final winner based on the highest average Accuracy value across all iterations. Average Accuracy values in our 10 test/train iterations ranged from 70.2668 % from default inputs of the random forest classification model to a value of 72.5192 % in the best tuned model fit. Although the run-time of this model parameter choice is the largest performed, we decided to remain with these inputs due to the amount increase in accuracy. As mentioned previously, we tuned the n_estimators parameter to ensure we stayed under 10 minutes execution. Parameter inputs for the final Random Forest Classification model with the KD Tree Algorithm are as follows: ###Not Complete#### \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>n_estimators</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>TBD</th>\n",
    "      <td>TBD</td>\n",
    "      <td>TBD</td>\n",
    "      <td>TBD</td>\n",
    "      <td>TBD</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def rfc_explorBinary(n_estimators,\n",
    "               max_features,\n",
    "               max_depth, \n",
    "               min_samples_split,\n",
    "               min_samples_leaf,\n",
    "               Data        = OPMAnalysisDataNoFamBinary,\n",
    "               cols        = PCList,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    X = Data[cols].as_matrix()\n",
    "    \n",
    "    rfc_clf = RandomForestClassifier(n_estimators=n_estimators, max_features = max_features, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, n_jobs=-1, random_state = seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler', MinMaxScaler()),\n",
    "         ('CLF',rfc_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def rfc_explorBinary_w_PCA(n_estimators,\n",
    "               max_features,\n",
    "               max_depth, \n",
    "               min_samples_split,\n",
    "               min_samples_leaf,\n",
    "               PCA,\n",
    "               Data        = OPMAnalysisDataNoFamBinary,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    X = Data.drop(\"SEP\", axis=1).as_matrix()\n",
    "    \n",
    "    rfc_clf = RandomForestClassifier(n_estimators=n_estimators, max_features = max_features, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, n_jobs=-1, random_state = seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler', MinMaxScaler()),\n",
    "         ('PCA', PCA),\n",
    "         ('CLF',rfc_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "FinalResultsDF = pd.DataFrame(columns= ['ModelVersion', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy'])\n",
    "\n",
    "TopResultsDF = pd.DataFrame(columns= ['ModelVersion', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy'])\n",
    "\n",
    "acclist = [] \n",
    "fullColumns = list(OPMAnalysisDataNoFamBinary.columns)\n",
    "\n",
    "for i in fullColumns:\n",
    "    if i == \"SEP\": fullColumns.remove(i)\n",
    "\n",
    "n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10   , 10   , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 15    , 20    , 30    , 50    ]  \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 5     , 10    , 15   , 20   , None  , 15    , 15    , 15    , 15    , 15    , 15    , 15    , 15    , 15    , 15    , 15    , 15    , 15    , 15    , 15    ] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , None  , None  , None , None , None  , 10    , 15    , 20    , 25    , 30    , 17    , 18    , 19    , 21    , 22    , 23    , 15    , 15    , 15    , 15    ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 18    , 20    , 24    , 36    , 48    , 36    , 36    , 36    , 36  , 36    , 36    , 36    , 36    , 36    , 36    , 36    , 36    , 36    , 36    , 36    , 36    , 36    , 36    , 36    , 36    ] \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 9     , 10    , 12    , 18    , 24    , 18    , 18    , 18    , 18  , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    ]\n",
    "\n",
    "##Model with all Raw Scaled Features\n",
    "for i in range(0,len(n_estimators)):\n",
    "    acclist.append(rfc_explorBinary(n_estimators      = n_estimators[i],\n",
    "                              max_features      = max_features[i],\n",
    "                              max_depth         = max_depth[i],\n",
    "                              min_samples_split = min_samples_split[i],\n",
    "                              min_samples_leaf  = min_samples_leaf[i],\n",
    "                              cols              = fullColumns\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({  \"ModelVersion\": \"Random Forest: All Raw Features\",\n",
    "                                                \"n_estimators\": n_estimators,          \n",
    "                                                \"max_features\": max_features,         \n",
    "                                                \"max_depth\": max_depth,        \n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion', 'max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del rfcdf, acclist\n",
    "\n",
    "\n",
    "acclist = []\n",
    "\n",
    "n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10   , 10   , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 15    , 20    , 30    , 50    ]  \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 5     , 10    , 15   , 20   , None  , 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto'] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , None  , None  , None , None , None  , 10    , 15    , 20    , 25    , 30    , 17    , 18    , 19    , 21    , 22    , 23    , 17    , 17    , 17    , 17    ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 18    , 20    , 24    , 36    , 48    , 48    , 48    , 48   , 48   , 48    , 48    , 48    , 48    , 48    , 48    , 48    , 48    , 48    , 48    , 48    , 48    , 48    , 48    , 48    , 48    ]  \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 9     , 10    , 12    , 18    , 24    , 24    , 24    , 24   , 24   , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    ]\n",
    "\n",
    "\n",
    "## Model with only top 15 raw Scaled Principal Features \n",
    "for i in range(0,len(n_estimators)):\n",
    "    acclist.append(rfc_explorBinary(n_estimators      = n_estimators[i],\n",
    "                              max_features      = max_features[i],\n",
    "                              max_depth         = max_depth[i],\n",
    "                              min_samples_split = min_samples_split[i],\n",
    "                              min_samples_leaf  = min_samples_leaf[i]\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({  \"ModelVersion\": \"Random Forest: Top 15 Raw from PC\",\n",
    "                                                \"n_estimators\": n_estimators,          \n",
    "                                                \"max_features\": max_features,         \n",
    "                                                \"max_depth\": max_depth,        \n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion', 'max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del rfcdf, acclist\n",
    "\n",
    "\n",
    "### Model with PCA\n",
    "acclist = []\n",
    "\n",
    "n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10   , 10   , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 15    , 20    , 30    , 50    ]  \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 5     , 10    , 15   , 20   , None  , 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto'] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , None  , None , None , None  , 10    , 15    , 20    , 25    , 30    , 17    , 18    , 19    , 21    , 22    , 23    , 15    , 15    , 15    , 15    ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 18    , 20    , 24    , 36    ,  24   , 24    , 24    , 24    , 24  , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    , 24    ]   \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 9     , 10    , 12    , 18    ,  12   , 12    , 12    , 12    , 12  , 12    , 12    , 12    , 12    , 12    , 12    , 12    , 12    , 12    , 12    , 12    , 12    , 12    , 12    , 12    ]       \n",
    "\n",
    "\n",
    "for i in range(0,len(n_estimators)):\n",
    "    acclist.append(rfc_explorBinary_w_PCA(n_estimators      = n_estimators[i],\n",
    "                                    max_features      = max_features[i],\n",
    "                                    max_depth         = max_depth[i],\n",
    "                                    min_samples_split = min_samples_split[i],\n",
    "                                    min_samples_leaf  = min_samples_leaf[i],\n",
    "                                    PCA               = PCA(n_components=22, svd_solver='randomized', random_state = seed)\n",
    "                                   )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({  \"ModelVersion\": \"Random Forest: With PCA\",\n",
    "                                                \"n_estimators\": n_estimators,          \n",
    "                                                \"max_features\": max_features,         \n",
    "                                                \"max_depth\": max_depth,        \n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion', 'max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "\n",
    "display(rfcdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del rfcdf, acclist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayAccuracies(resultData):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"hls\", n_colors=len(resultData), desat=None, color_codes=False)\n",
    "    plot = resultData[[\"Iteration 0\",\"Iteration 1\",\"Iteration 2\",\"Iteration 3\",\"Iteration 4\"]].transpose().plot.line(title = \"Top Results Among Varying Model Feature Inputs\",rot=45)\n",
    "    plot.set_xlabel(\"Iterations\")\n",
    "    plot.set_ylabel(\"Accuracies\")\n",
    "    plot.legend(loc='center left', bbox_to_anchor=(1.01, .5))\n",
    "\n",
    "display(TopResultsDF)  \n",
    "displayAccuracies(TopResultsDF)\n",
    "\n",
    "FinalResultsDF = pd.concat([FinalResultsDF, TopResultsDF.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "TopResultsDF = pd.DataFrame(columns= ['ModelVersion', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have created a function to be re-used for our cross-validation Accuracy Scores. Inputs of PCA components, Model CLF object, original sample data, and a CV containing our test/train splits allow us to easily produce an array of Accuracy Scores for the different permutations of models tested. A XXXXXXTBDXXXXX plot is also displayed depicting a view of the misclassification values for each iteration. Finally, a confusion matrix is displayed for the last test/train iteration for further interpretation on results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrixBinary(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = (18, 6)\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.rc('xtick', labelsize=18)\n",
    "    plt.rc('ytick', labelsize=18) \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize = 18)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.grid(False)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j],2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize = 18)\n",
    "    plt.xlabel('Predicted label', fontsize = 18)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_ROC_curve(X, y, mean_tpr, mean_fpr, cv = cv, ):\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "    lw = 2\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='black',\n",
    "             label='Luck')\n",
    "\n",
    "    mean_tpr /= cv.get_n_splits(X, y)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='gray', linestyle='--',\n",
    "             label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def compute_kfold_scores_ClassificationBinary( clf,\n",
    "                                         PCA = \"No\",\n",
    "                                         Data     = OPMAnalysisDataNoFamBinary,\n",
    "                                         cols     = PCList,\n",
    "                                         cv       = cv):\n",
    "\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    y = np.where(y == 'NS', 0, 1) # NS = 0; SC = 1\n",
    "    \n",
    "    X = Data[cols].as_matrix()\n",
    "\n",
    "\n",
    "    # Run classifier with cross-validation and plot ROC curves\n",
    "\n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    if(PCA == \"No\"):\n",
    "        clf_pipe = Pipeline(\n",
    "            [('minMaxScaler', MinMaxScaler()),\n",
    "             ('CLF',clf)]\n",
    "        )\n",
    "    else:\n",
    "        clf_pipe = Pipeline(\n",
    "            [('minMaxScaler', MinMaxScaler()),\n",
    "             ('PCA', PCA),\n",
    "             ('CLF',clf)]\n",
    "        )\n",
    "    \n",
    "    colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'pink', 'darkred', 'dimgray', 'maroon', 'coral'])\n",
    "    \n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    lw = 2\n",
    "    i = 0\n",
    "    \n",
    "    accuracy = []\n",
    "    #logloss = []\n",
    "    \n",
    "    for (train, test), color in zip(cv.split(X, y), colors):\n",
    "        clf_pipe.fit(X[train],y[train])  # train object\n",
    "        y_hat = clf_pipe.predict(X[test]) # get test set preditions\n",
    "        \n",
    "        probas_ = clf_pipe.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        \n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.rcParams['figure.figsize'] = (12, 6)\n",
    "        \n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.set_palette(\"hls\", n_colors=6, desat=None, color_codes=False)\n",
    "        \n",
    "        plt.plot(fpr, tpr, lw=lw, #color=color,\n",
    "                 label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    plot_ROC_curve(X, y, mean_tpr, mean_fpr)         \n",
    "        #logloss.append(round(l,5)) \n",
    "    \n",
    "    #print(\"Accuracy Ratings across all iterations: {0}\\n\\n\\\n",
    "#Average Accuracy: {1}\\n\\n\\\n",
    "#Log Loss Values across all iterations: {2}\\n\\n\\\n",
    "#Average Log Loss: {3}\\n\".format(accuracy, round(sum(accuracy)/len(accuracy),5), logloss,round(sum(logloss)/len(logloss),5)))\n",
    "\n",
    "    for (train, test), color in zip(cv.split(X, y), colors):\n",
    "        clf_pipe.fit(X[train],y[train])  # train object\n",
    "        y_hat = clf_pipe.predict(X[test]) # get test set preditions\n",
    "        \n",
    "        a = float(mt.accuracy_score(y[test],y_hat))\n",
    "        #l = float(mt.log_loss(y[test], y_hat))\n",
    "        \n",
    "        accuracy.append(round(a,5)) \n",
    "        \n",
    "        ytestnames = np.where(y[test] ==  0,'NS','SC')\n",
    "\n",
    "        yhatnames  = np.where(y_hat ==  0,'NS', 'SC')\n",
    "\n",
    "        #print(set(list(y_hat)))\n",
    "        print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(ytestnames, yhatnames, rownames = ['True'], colnames = ['Predicted'], margins = True)))\n",
    "\n",
    "            # Plot non-normalized confusion matrix\n",
    "        plt.figure()\n",
    "        plot_confusion_matrixBinary(confusion_matrix(y[test], y_hat), \n",
    "                              classes   =[\"NS\",  \"SC\"], \n",
    "                              normalize =True,\n",
    "                              title     ='Confusion matrix, with normalization')\n",
    "    \n",
    "    print(\"Accuracy Ratings across all iterations: {0}\\n\\n\\\n",
    "        Average Accuracy: {1}\\n\".format(accuracy, round(sum(accuracy)/len(accuracy),5)))\n",
    "\n",
    "    return clf_pipe.named_steps['CLF'], accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_clf = RandomForestClassifier(n_estimators       = 10, \n",
    "                                 max_features       = 'auto', \n",
    "                                 max_depth          = 17.0, \n",
    "                                 min_samples_split  = 48, \n",
    "                                 min_samples_leaf   = 24,\n",
    "                                 n_jobs             = -1, \n",
    "                                 random_state       = seed) # get object\n",
    "    \n",
    "rfc_clf, rfc_acc = compute_kfold_scores_ClassificationBinary(rfc_clf, \n",
    "                                                             ##PCA = PCA(n_components=22, svd_solver='randomized', random_state = seed),\n",
    "                                                             cols = PCList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Binary KNN Model\n",
    "\n",
    "#### KNN\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "Options include \"Ball Tree\" and \"KD Tree\". \n",
    "* Ball Trees are binary trees formed from nodes of multidimensional hyperspheres, or \"balls\". Node hyperspheres may intersect, but each point is assigned to one according to distance from the hypersphere center. \n",
    "* KD Trees are binary trees formed from nodes of multidimensional hyperplanes. Every node in the tree is associated with one of the dimensions, with the hyperplane perpendicular to that dimension's axis.\n",
    "\n",
    "Our findings, were that the Ball Tree algorithm was considerably less efficient to produce results for all 10 iterations in comparison to the KD Tree Algorithm.\n",
    "\n",
    "**Leaf Size**\n",
    "\n",
    "The size for leaf nodes in the KNN Tree.\n",
    "\n",
    "**Number of Neighbors**\n",
    "\n",
    "After 24 iterations of modifying the above parameters, we land on a final winner based on the highest average Accuracy value across all iterations. Average Accuracy values in our 10 test/train iterations ranged from 66.5216 % from the worst parameter inputs of the Ball_Tree Algorith to a value of 69.5528 % in best tuned KNN Classification model fit. We have chosen to utilize the best input for KD tree, although losing an improvement of .0004 % due to the cost(slower runtime of 07 Minutes 25 Seconds through 10 iterations) of fitting the model as Ball Tree. Parameter inputs for the final K Nearest Neighbor Classification model with the KD Tree Algorithm are as follows:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>algorithm</th>\n",
    "      <th>leaf_size</th>\n",
    "      <th>n_neighbors</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>kd_tree</th>\n",
    "      <td>50</td>\n",
    "      <td>150</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def knn_explorBinary_w_PCA(n_neighbors,\n",
    "               algorithm ,\n",
    "               leaf_size,\n",
    "               PCA,\n",
    "               Data        = OPMAnalysisDataNoFamBinary,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    X = Data.drop(\"SEP\", axis=1).as_matrix() \n",
    "    \n",
    "    knn_clf = KNeighborsClassifier(n_neighbors = n_neighbors, algorithm = algorithm, leaf_size = leaf_size, n_jobs=-1) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler', MinMaxScaler()),\n",
    "         ('PCA', PCA),\n",
    "         ('CLF',knn_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def knn_explorBinary(n_neighbors,\n",
    "               algorithm ,\n",
    "               leaf_size,\n",
    "               Data        = OPMAnalysisDataNoFamBinary,\n",
    "               cols        = PCList,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    if (\"SEP\" in cols):    X = Data[cols].drop(\"SEP\", axis=1).as_matrix() \n",
    "    else: X = Data[cols]\n",
    "    \n",
    "    knn_clf = KNeighborsClassifier(n_neighbors = n_neighbors, algorithm = algorithm, leaf_size = leaf_size, n_jobs=-1) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler', MinMaxScaler()),\n",
    "         ('CLF',knn_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "###Full Columns\n",
    "acclist = [] \n",
    "\n",
    "n_neighbors =  [5          , 10         , 15         , 20         , 30         , 40         , 50         , 100        , 150        , 200        , 250        , 150        , 150        , 150        , 150        , 150        , 150        ]\n",
    "algorithm   =  'kd_tree'\n",
    "leaf_size   =  [30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 2          , 3          , 4          , 5          , 10         , 20         ]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(n_neighbors)):\n",
    "    acclist.append(knn_explorBinary(n_neighbors = n_neighbors[i],\n",
    "                              algorithm   = algorithm,\n",
    "                              leaf_size   = leaf_size[i],\n",
    "                              cols = fullColumns\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"KNN: \" + algorithm + \", Full Raw Columns\",\n",
    "                                                \"n_neighbors\": n_neighbors,          \n",
    "                                                \"algorithm\": algorithm,         \n",
    "                                                \"leaf_size\": leaf_size  \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion','algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "display(rfcdf)\n",
    "del rfcdf, acclist\n",
    "\n",
    "\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "n_neighbors =  [5          , 10         , 15         , 20         , 30         , 40         , 50         , 100        , 150        , 200        , 250        , 150        , 150        , 150        , 150        , 150        , 150        ]\n",
    "algorithm   =  'ball_tree'\n",
    "leaf_size   =  [30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 2          , 3          , 4          , 5          , 10         , 20         ]\n",
    "\n",
    "for i in range(0,len(n_neighbors)):\n",
    "    acclist.append(knn_explorBinary(n_neighbors = n_neighbors[i],\n",
    "                              algorithm   = algorithm,\n",
    "                              leaf_size   = leaf_size[i],\n",
    "                              cols = fullColumns\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"KNN: \" + algorithm + \", Full Raw Columns\",\n",
    "                                                \"n_neighbors\": n_neighbors,          \n",
    "                                                \"algorithm\": algorithm,         \n",
    "                                                \"leaf_size\": leaf_size  \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion','algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del rfcdf, acclist\n",
    "\n",
    "###Reduced Columns\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "n_neighbors =  [5          , 10         , 15         , 20         , 30         , 40         , 50         , 100        , 150        , 200        , 250        , 300        , 350        , 400        ,  50        ,  50        ,  50        ,  50        ,  50        ,  50        ]\n",
    "algorithm   =  'kd_tree'\n",
    "leaf_size   =  [30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 2          , 3          , 4          , 5          , 10         , 20         ]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(n_neighbors)):\n",
    "    acclist.append(knn_explorBinary(n_neighbors = n_neighbors[i],\n",
    "                              algorithm   = algorithm,\n",
    "                              leaf_size   = leaf_size[i]\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"KNN: \" + algorithm + \", Reduced Raw Columns\",\n",
    "                                                \"n_neighbors\": n_neighbors,          \n",
    "                                                \"algorithm\": algorithm,         \n",
    "                                                \"leaf_size\": leaf_size  \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion','algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del rfcdf, acclist\n",
    "\n",
    "\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "n_neighbors =  [5          , 10         , 15         , 20         , 30         , 40         , 50         , 100        , 150        , 200        , 250        , 300        , 350        , 400        ,  50        ,  50        ,  50        ,  50        ,  50        ,  50        ]\n",
    "algorithm   =  'ball_tree'\n",
    "leaf_size   =  [30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 2          , 3          , 4          , 5          , 10         , 20         ]\n",
    "\n",
    "for i in range(0,len(n_neighbors)):\n",
    "    acclist.append(knn_explorBinary(n_neighbors = n_neighbors[i],\n",
    "                              algorithm   = algorithm,\n",
    "                              leaf_size   = leaf_size[i]\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"KNN: \" + algorithm + \", Reduced Raw Columns\",\n",
    "                                                \"n_neighbors\": n_neighbors,          \n",
    "                                                \"algorithm\": algorithm,         \n",
    "                                                \"leaf_size\": leaf_size  \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion','algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del rfcdf, acclist\n",
    "\n",
    "\n",
    "#### WITH PCA\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "n_neighbors =  [5          , 10         , 15         , 20         , 30         , 40         , 50         , 100        , 150        , 200        , 250        , 100        , 100        , 100        , 100        , 100        , 100        ]\n",
    "algorithm   =  'kd_tree'\n",
    "leaf_size   =  [30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 2          , 3          , 4          , 5          , 10         , 20         ]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(n_neighbors)):\n",
    "    acclist.append(knn_explorBinary_w_PCA(n_neighbors = n_neighbors[i],\n",
    "                              algorithm   = algorithm,\n",
    "                              leaf_size   = leaf_size[i],\n",
    "                              PCA = PCA(n_components=22, svd_solver='randomized', random_state = seed)\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"KNN: \" + algorithm + \", With PCA\",\n",
    "                                                \"n_neighbors\": n_neighbors,          \n",
    "                                                \"algorithm\": algorithm,         \n",
    "                                                \"leaf_size\": leaf_size  \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion','algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del rfcdf, acclist\n",
    "\n",
    "\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "n_neighbors =  [5          , 10         , 15         , 20         , 30         , 40         , 50         , 100        , 150        , 200        , 250        , 100        , 100        , 100        , 100        , 100        , 100        ]\n",
    "algorithm   =  'ball_tree'\n",
    "leaf_size   =  [30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 2          , 3          , 4          , 5          , 10         , 20         ]\n",
    "\n",
    "for i in range(0,len(n_neighbors)):\n",
    "    acclist.append(knn_explorBinary_w_PCA(n_neighbors = n_neighbors[i],\n",
    "                              algorithm   = algorithm,\n",
    "                              leaf_size   = leaf_size[i],\n",
    "                              PCA = PCA(n_components=22, svd_solver='randomized', random_state = seed)\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"KNN: \" + algorithm + \", With PCA\",\n",
    "                                                \"n_neighbors\": n_neighbors,          \n",
    "                                                \"algorithm\": algorithm,         \n",
    "                                                \"leaf_size\": leaf_size  \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion','algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del rfcdf, acclist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(TopResultsDF)  \n",
    "displayAccuracies(TopResultsDF)\n",
    "\n",
    "FinalResultsDF = pd.concat([FinalResultsDF, TopResultsDF.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "TopResultsDF = pd.DataFrame(columns= ['ModelVersion', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 150, algorithm = 'ball_tree',leaf_size = 30, n_jobs=-1) # get object\n",
    "\n",
    "knn_clf, knn_acc = compute_kfold_scores_ClassificationBinary(clf         = knn_clf,\n",
    "                                                             cols        = fullColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "We have chosen to manipulate the cost variable (C) within our logistic regression analyzing accuracies at {1.0, .01, .05, 5}. This parameter is essentially an inverted regularization strength equal to 1/lambda per scikit-learn class function code (lambda being the actual regularization item). Therefore, the smaller the cost value the stronger the regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = {'NS':0, 'SC':1}\n",
    "y = OPMAnalysisDataNoFamBinary.replace({'SEP': mapping})\n",
    "y = y.SEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R -i OPMAnalysisDataNoFamBinary,fullColumns,y\n",
    "#install.packages(\"car\") ## Selection 55\n",
    "library(car)\n",
    "str(OPMAnalysisDataNoFamBinary)\n",
    "print(unlist(fullColumns))\n",
    "print(paste(\"# of SEP observations = \", length(y)))\n",
    "print(paste(\"SEP type = \", unique(y)))\n",
    "print(paste(\"SEP class = \", class(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "vars <- names(OPMAnalysisDataNoFamBinary[,22:ncol(OPMAnalysisDataNoFamBinary)])\n",
    "OPMAnalysisDataNoFamBinary[, vars] <- sapply(OPMAnalysisDataNoFamBinary[,22:ncol(OPMAnalysisDataNoFamBinary)], as.numeric)\n",
    "\n",
    "print(summary(OPMAnalysisDataNoFamBinary))\n",
    "\n",
    "# Apply Min/Max scaler function to data\n",
    "OPMAnalysisDataNoFamBinary[,-1] <- sapply(OPMAnalysisDataNoFamBinary[,-1], function(x) (x-min(x))/(max(x)-min(x)))\n",
    "\n",
    "cat(\"\\n\\n\\n\")\n",
    "print(summary(OPMAnalysisDataNoFamBinary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOCTYP_1 and PPTYP_1 have only single level and need removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R sapply(OPMAnalysisDataNoFamBinary, function(x) length(unique(x[!is.na(x)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique counts with LOCTYP_1 and PPTYP_1 removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%R sapply(OPMAnalysisDataNoFamBinary[,-c(95,96)], function(x) length(unique(x[!is.na(x)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data example with LOCTYP_1 and PPTYP_1 removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%R OPMAnalysisDataNoFamBinary[,-c(95,96)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform logistic regression with intent to extract only most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "vars <- unlist(fullColumns)\n",
    "vars <- vars[-c(94,95)]\n",
    "#print(vars)\n",
    "fla <- paste(\"SEP ~\", paste(vars, collapse=\"+\"))\n",
    "fla <- as.formula(fla)\n",
    "\n",
    "OPMAnalysisDataNoFamBinary$SEP <- as.vector(y)\n",
    "BinLogit <- glm(fla, data = OPMAnalysisDataNoFamBinary, family = \"binomial\")\n",
    "summary(BinLogit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "alias(BinLogit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "tmp <- alias(BinLogit)$Complete\n",
    "#print(attributes(tmp))\n",
    "aliased <- dimnames(tmp)[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#aliased <- ifelse(grepl('[[:digit:]]$', aliased), substr(aliased, 1, nchar(aliased)-1), aliased)\n",
    "print(c(\"Following attributes will be dropped from model due to multicollinearity:\", aliased))\n",
    "\n",
    "paste(as.character(length(vars) - length(vars[!vars %in% c(aliased)])), \"attributes removed from model input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "runLogit <- function(less, vars){\n",
    "    fla <- paste(\"SEP ~\", paste(vars, collapse=\"+\"))\n",
    "    fla <- as.formula(fla)\n",
    "\n",
    "    binLog <- glm(fla, data = OPMAnalysisDataNoFamBinary, family = \"binomial\")\n",
    "    return(binLog)\n",
    "}\n",
    "\n",
    "vars <- vars[!(vars %in% c(aliased))]\n",
    "BinLogit2 <- runLogit(aliased, vars)\n",
    "summary(BinLogit2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#install.packages(\"car\")\n",
    "#require(car)\n",
    "library(car)\n",
    "runVifs <- function(logit){\n",
    "    tmp <- as.data.frame(vif(logit))\n",
    "    colnames(tmp) <- \"VIF\"\n",
    "\n",
    "    scipen.default <- getOption(\"scipen\")\n",
    "    options(scipen=999)\n",
    "    print(tmp)\n",
    "    options(scipen=scipen.default)\n",
    "    return(tmp)\n",
    "}\n",
    "\n",
    "vifs.BinLogit2 <- runVifs(BinLogit2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "vifs.BinLogitRepeat <- vifs.BinLogit2\n",
    "vars.Repeat <- vars\n",
    "\n",
    "vif.removed <- vector(mode=\"character\", length=0)\n",
    "ndev.vect <- vector(mode=\"character\", length=0)\n",
    "ndf.vect <- vector(mode=\"character\", length=0)\n",
    "pchisq.vect <- vector(mode=\"character\", length=0)\n",
    "logLik.vect <- vector(mode=\"character\", length=0)\n",
    "AIC.vect <- vector(mode=\"character\", length=0)\n",
    "BIC.vect <- vector(mode=\"character\", length=0)\n",
    "\n",
    "\n",
    "for(i in seq(1,13)){\n",
    "    remove <- rownames(vifs.BinLogitRepeat)[which.max(vifs.BinLogitRepeat$VIF)]\n",
    "    vif.removed <- c(vif.removed, remove)\n",
    "    cat(\"\\n\\n\\nRemoved BEFORE this step:\", remove, \"\\n\")\n",
    "    vars.Repeat <- vars.Repeat[!(vars.Repeat %in% c(remove))]\n",
    "    BinLogitRepeat <- runLogit(remove, vars.Repeat)\n",
    "    print(summary(BinLogitRepeat))\n",
    "    vifs.BinLogitRepeat <- runVifs(BinLogitRepeat)\n",
    "    \n",
    "    ##goodness of fit\n",
    "    ndev.vect <- c(ndev.vect, with(BinLogitRepeat, null.deviance - deviance))\n",
    "    ndf.vect <- c(ndf.vect, with(BinLogitRepeat, df.null - df.residual))\n",
    "    pchisq.vect <- c(pchisq.vect, with(BinLogitRepeat, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))\n",
    "    logLik.vect <- c(logLik.vect, logLik(BinLogitRepeat))\n",
    "    AIC.vect <- c(AIC.vect, AIC(BinLogitRepeat))\n",
    "    BIC.vect <- c(BIC.vect, BIC(BinLogitRepeat))\n",
    "}\n",
    "cat(\"\\nFollowing variables removed based on VIF values (in order of removal):\\n\")\n",
    "print(vif.removed)\n",
    "\n",
    "cat(\"\\n\\nNull Deviances (in order):\\n\")\n",
    "print(ndev.vect)\n",
    "cat(\"\\nMin value at iteration = \", which.min(ndev.vect))\n",
    "\n",
    "cat(\"\\n\\nDiff Degrees of Freedom (in order):\\n\")\n",
    "print(ndf.vect)\n",
    "cat(\"\\nMin value at iteration = \", which.min(ndf.vect))\n",
    "\n",
    "#cat(\"\\n\\nP-ChiSquare (in order):\\n\")\n",
    "#print(pchisq.vect)\n",
    "#cat(\"\\nMin value at iteration = \", which.min(pchisq.vect))\n",
    "\n",
    "cat(\"\\n\\nLog Likelihoods (in order):\\n\")\n",
    "print(logLik.vect)\n",
    "cat(\"\\nMin value at iteration = \", which.min(logLik.vect))\n",
    "\n",
    "cat(\"\\n\\nAIC values (in order):\\n\")\n",
    "print(AIC.vect)\n",
    "cat(\"\\nMin value at iteration = \", which.min(AIC.vect))\n",
    "\n",
    "cat(\"\\n\\nBIC values (in order):\\n\")\n",
    "print(BIC.vect)\n",
    "cat(\"\\nMin value at iteration = \", which.min(BIC.vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "#vars.Repeat <- replace(vars.Repeat, vars.Repeat == \"IndAvgSalary\", \"IndAvgSalaryLog\")\n",
    "vars.Repeat <- vars.Repeat[!vars.Repeat %in% c(\"IndAvgSalary\", \"IndAvgSalaryLog\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "##data.frame(summary(BinLogit)$coef[summary(BinLogit)$coef[,4] <= .05, 4]) #Review coefficients of p-value less than 0.05\n",
    "#LogitCoeffs <- data.frame(summary(BinLogit.P.Repeat)$coef[-1,4]) #Ignore Intercept and only look at p-values\n",
    "##LogitCoeffs[LogitCoeffs$`summary.BinLogit..coef..1..4.` == max(LogitCoeffs$`summary.BinLogit..coef..1..4.`),]\n",
    "#maxP <- cbind(rownames(LogitCoeffs)[LogitCoeffs$`summary.BinLogit.P.Repeat..coef..1..4.` == max(LogitCoeffs$`summary.BinLogit.P.Repeat..coef..1..4.`)],\n",
    "#      max(LogitCoeffs))\n",
    "#maxP[1,1]\n",
    "\n",
    "varsP.Repeat <- vars.Repeat\n",
    "\n",
    "P.removed <- vector(mode=\"character\", length=0)\n",
    "ndev.vect <- vector(mode=\"character\", length=0)\n",
    "ndf.vect <- vector(mode=\"character\", length=0)\n",
    "pchisq.vect <- vector(mode=\"character\", length=0)\n",
    "logLik.vect <- vector(mode=\"character\", length=0)\n",
    "AIC.vect <- vector(mode=\"character\", length=0)\n",
    "BIC.vect <- vector(mode=\"character\", length=0)\n",
    "\n",
    "for(i in seq(1,44)){\n",
    "    BinLogit.P.Repeat <- runLogit(remove, varsP.Repeat)\n",
    "    print(summary(BinLogit.P.Repeat))\n",
    "    \n",
    "    LogitCoeffs <- data.frame(summary(BinLogit.P.Repeat)$coef[-1,4]) #Ignore Intercept and only look at p-values\n",
    "    maxP <- cbind(rownames(LogitCoeffs)[LogitCoeffs$`summary.BinLogit.P.Repeat..coef..1..4.` == max(LogitCoeffs$`summary.BinLogit.P.Repeat..coef..1..4.`)],\n",
    "          max(LogitCoeffs))\n",
    "    \n",
    "    vifs.BinLogitP.Repeat <- runVifs(BinLogit.P.Repeat)\n",
    "    \n",
    "    remove <- maxP[1,1]\n",
    "    #remove <- ifelse(grepl('[[:digit:]]$', remove), substr(remove, 1, nchar(remove)-1), remove)\n",
    "    P.removed <- c(P.removed, remove)\n",
    "    cat(\"\\nRemoved AFTER this step:\", remove, \"\\n\\n\\n\")\n",
    "    varsP.Repeat <- varsP.Repeat[!(varsP.Repeat %in% c(remove))]\n",
    "    \n",
    "    ##goodness of fit\n",
    "    ndev.vect <- c(ndev.vect, with(BinLogit.P.Repeat, null.deviance - deviance))\n",
    "    ndf.vect <- c(ndf.vect, with(BinLogit.P.Repeat, df.null - df.residual))\n",
    "    pchisq.vect <- c(pchisq.vect, with(BinLogit.P.Repeat, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))\n",
    "    logLik.vect <- c(logLik.vect, logLik(BinLogit.P.Repeat))\n",
    "    AIC.vect <- c(AIC.vect, AIC(BinLogit.P.Repeat))\n",
    "    BIC.vect <- c(BIC.vect, BIC(BinLogit.P.Repeat))\n",
    "}\n",
    "\n",
    "cat(\"\\nFollowing variables removed based on p-values (in order of removal):\\n\")\n",
    "print(P.removed)\n",
    "\n",
    "cat(\"\\n\\nNull Deviances (in order):\\n\")\n",
    "print(ndev.vect)\n",
    "cat(\"\\nMin value at iteration = \", which.min(ndev.vect))\n",
    "\n",
    "cat(\"\\n\\nDiff Degrees of Freedom (in order):\\n\")\n",
    "print(ndf.vect)\n",
    "cat(\"\\nMin value at iteration = \", which.min(ndf.vect))\n",
    "\n",
    "#cat(\"\\n\\nP-ChiSquare (in order):\\n\")\n",
    "#print(pchisq.vect)\n",
    "#cat(\"\\nMin value at iteration = \", which.min(pchisq.vect))\n",
    "\n",
    "cat(\"\\n\\nLog Likelihoods (in order):\\n\")\n",
    "print(logLik.vect)\n",
    "cat(\"\\nMin value at iteration = \", which.min(logLik.vect))\n",
    "\n",
    "cat(\"\\n\\nAIC values (in order):\\n\")\n",
    "print(AIC.vect)\n",
    "cat(\"\\nMin value at iteration = \", which.min(AIC.vect))\n",
    "\n",
    "cat(\"\\n\\nBIC values (in order):\\n\")\n",
    "print(BIC.vect)\n",
    "cat(\"\\nMin value at iteration = \", which.min(BIC.vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R LRSigCols <- varsP.Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%R -o LRSigCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from rpy2.robjects import pandas2ri\n",
    "#pandas2ri.activate()\n",
    "\n",
    "#LRSigCols\n",
    "LRSigCols = pandas2ri.ri2py(LRSigCols)\n",
    "LRSigCols = LRSigCols.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only following variables remain after Logistic Regression manual selection:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRSigCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def lr_explorBinary(  cost,\n",
    "                      Data        = OPMAnalysisDataNoFamBinary,\n",
    "                      cols        = PCList,\n",
    "                      cv          = cv,\n",
    "                      seed        = seed):\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    if (\"SEP\" in cols):    X = Data[cols].drop(\"SEP\", axis=1).as_matrix() \n",
    "    else: X = Data[cols]\n",
    "    \n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None, random_state=seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler',MinMaxScaler()),\n",
    "         ('CLF',lr_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def lr_explorBinary_w_PCA(cost,\n",
    "                          PCA,\n",
    "                          Data        = OPMAnalysisDataNoFamBinary,\n",
    "                          cv          = cv,\n",
    "                          seed        = seed):\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    y = Data[\"SEP\"].values # get the labels we want    \n",
    "    \n",
    "    X = Data.drop(\"SEP\", axis=1).as_matrix() \n",
    "    \n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None, random_state=seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('minMaxScaler',MinMaxScaler()),\n",
    "         ('PCA',PCA),\n",
    "         ('CLF',lr_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "##Full Columns\n",
    "acclist = [] \n",
    "\n",
    "cost    = [.00000001, .0001, .001, .01, .05, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "\n",
    "for i in range(0,len(cost)):\n",
    "    acclist.append(lr_explorBinary(cost       = cost[i],\n",
    "                             cols       = fullColumns))\n",
    "\n",
    "LRdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"Logistic Regression: All Raw Features\",\n",
    "                                                \"Cost\": cost\n",
    "                                              })[[\"ModelVersion\", \"Cost\"]],\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "LRdf.columns = ['ModelVersion','Cost', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(LRdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, LRdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del LRdf, acclist\n",
    "\n",
    "##Reduced Columns\n",
    "acclist = [] \n",
    "\n",
    "cost    = [.00000001, .0001, .001, .01, .05, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "\n",
    "for i in range(0,len(cost)):\n",
    "    acclist.append(lr_explorBinary(cost       = cost[i]))\n",
    "\n",
    "LRdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"Logistic Regression: Top 15 from PCA Raw Features\",\n",
    "                                                \"Cost\": cost\n",
    "                                              })[[\"ModelVersion\", \"Cost\"]],\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "LRdf.columns = ['ModelVersion','Cost', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(LRdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, LRdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del LRdf, acclist\n",
    "\n",
    "##With PCA\n",
    "acclist = [] \n",
    "\n",
    "cost    = [.00000001, .0001, .001, .01, .05, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "\n",
    "for i in range(0,len(cost)):\n",
    "    acclist.append(lr_explorBinary_w_PCA(cost       = cost[i],\n",
    "                                   PCA        = PCA(n_components=23, svd_solver='randomized', random_state = seed)))\n",
    "\n",
    "LRdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"Logistic Regression: With PCA\",\n",
    "                                                \"Cost\": cost\n",
    "                                              })[[\"ModelVersion\", \"Cost\"]],\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "LRdf.columns = ['ModelVersion','Cost', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(LRdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, LRdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del LRdf, acclist\n",
    "\n",
    "\n",
    "##Significant Column List from Manual Tuning in R\n",
    "acclist = [] \n",
    "\n",
    "cost    = [.00000001, .0001, .001, .01, .05, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "\n",
    "for i in range(0,len(cost)):\n",
    "    acclist.append(lr_explorBinary(cost       = cost[i],\n",
    "                             cols       = LRSigCols))\n",
    "\n",
    "LRdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"Logistic Regression: Manual Significant Features\",\n",
    "                                                \"Cost\": cost\n",
    "                                              })[[\"ModelVersion\", \"Cost\"]],\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "LRdf.columns = ['ModelVersion','Cost', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(LRdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, LRdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del LRdf, acclist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(TopResultsDF)  \n",
    "displayAccuracies(TopResultsDF)\n",
    "\n",
    "FinalResultsDF = pd.concat([FinalResultsDF, TopResultsDF.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "TopResultsDF = pd.DataFrame(columns= ['ModelVersion', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(penalty='l2', C=2, class_weight=None, random_state=seed) # get object\n",
    "\n",
    "lr_clf, lr_acc = compute_kfold_scores_ClassificationBinary(clf         = lr_clf,\n",
    "                                                           cols        = LRSigCols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Retest Random Forest and KNN with Logistic Regression Significant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Random Forest Sign. Cols\n",
    "\n",
    "acclist = []\n",
    "\n",
    "n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10   , 10   , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 15    , 20    , 30    , 50    ]  \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 5     , 10    , 15   , 20   , None  , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     , 5     ] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , None  , None , None , None  , 10    , 15    , 20    , 25    , 30    , 3     , 4     , 5     , 6     , 7     , 8     , 9     , 11    , 12    , 13    , 4     , 4     , 4     , 4     ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 18    , 20    , 24    , 36    , 18    , 18    , 18   , 18   , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    , 18    ] \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 9     , 10    , 12    , 18    , 9     , 9     , 9    , 9    , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     , 9     ]\n",
    "\n",
    "\n",
    "## Model with only top 15 raw Scaled Principal Features \n",
    "for i in range(0,len(n_estimators)):\n",
    "    acclist.append(rfc_explorBinary(n_estimators      = n_estimators[i],\n",
    "                              max_features      = max_features[i],\n",
    "                              max_depth         = max_depth[i],\n",
    "                              min_samples_split = min_samples_split[i],\n",
    "                              min_samples_leaf  = min_samples_leaf[i],\n",
    "                              cols = LRSigCols\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({  \"ModelVersion\": \"Random Forest: With LR Sig Cols\",\n",
    "                                                \"n_estimators\": n_estimators,          \n",
    "                                                \"max_features\": max_features,         \n",
    "                                                \"max_depth\": max_depth,        \n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion', 'max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del rfcdf, acclist\n",
    "\n",
    "\n",
    "#### KNN Sign. Cols\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "n_neighbors =  [5          , 10         , 15         , 20         , 30         , 40         , 50         , 100        , 150        , 200        , 250        , 200        , 200        , 200        , 200        , 200        , 200        , 200     , 200]\n",
    "algorithm   =  'ball_tree'\n",
    "leaf_size   =  [30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 2          , 3          , 4          , 5         , 20        , 50          , 100      , 150]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(n_neighbors)):\n",
    "    acclist.append(knn_explorBinary(n_neighbors = n_neighbors[i],\n",
    "                              algorithm   = algorithm,\n",
    "                              leaf_size   = leaf_size[i],\n",
    "                              cols = LRSigCols\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"KNN: \" + algorithm + \", With LR Sig Cols\",\n",
    "                                                \"n_neighbors\": n_neighbors,          \n",
    "                                                \"algorithm\": algorithm,         \n",
    "                                                \"leaf_size\": leaf_size  \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion','algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del rfcdf, acclist\n",
    "\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "n_neighbors =  [5          , 10         , 15         , 20         , 30         , 40         , 50         , 100        , 150        , 200        , 250        , 200        , 200        , 200        , 200        , 200        , 200        , 200     , 200]\n",
    "algorithm   =  'kd_tree'\n",
    "leaf_size   =  [30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 30         , 2          , 3          , 4          , 5         , 20        , 50          , 100      , 150]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(n_neighbors)):\n",
    "    acclist.append(knn_explorBinary(n_neighbors = n_neighbors[i],\n",
    "                              algorithm   = algorithm,\n",
    "                              leaf_size   = leaf_size[i],\n",
    "                              cols = LRSigCols\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"ModelVersion\": \"KNN: \" + algorithm + \", With LR Sig Cols\",\n",
    "                                                \"n_neighbors\": n_neighbors,          \n",
    "                                                \"algorithm\": algorithm,         \n",
    "                                                \"leaf_size\": leaf_size  \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['ModelVersion','algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "TopResultsDF = pd.concat([TopResultsDF, rfcdf.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "del rfcdf, acclist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(TopResultsDF)  \n",
    "displayAccuracies(TopResultsDF)\n",
    "\n",
    "FinalResultsDF = pd.concat([FinalResultsDF, TopResultsDF.sort_values(['MeanAccuracy'], ascending=False)[TopResultsDF.columns].head(1)]).sort_values(['MeanAccuracy'], ascending=False).reset_index(drop=True)\n",
    "TopResultsDF = pd.DataFrame(columns= ['ModelVersion', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'MeanAccuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_clf = RandomForestClassifier(n_estimators       =50, \n",
    "                                 max_features       = 5, \n",
    "                                 max_depth          = 4.0, \n",
    "                                 min_samples_split  = 18, \n",
    "                                 min_samples_leaf   = 9,\n",
    "                                 n_jobs             = -1, \n",
    "                                 random_state       = seed) # get object\n",
    "    \n",
    "rfc_clf, rfc_acc = compute_kfold_scores_ClassificationBinary(rfc_clf, \n",
    "                                                             ##PCA = PCA(n_components=22, svd_solver='randomized', random_state = seed),\n",
    "                                                             cols = LRSigCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "from sklearn import tree\n",
    "import pydotplus\n",
    "import six\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import SVG \n",
    "\n",
    "\n",
    "\n",
    "i_tree = 0\n",
    "for tree_in_forest in rfc_clf.estimators_:       \n",
    "    svgData = tree.export_graphviz(tree_in_forest, \n",
    "                         feature_names=fullColumns,\n",
    "                         class_names=[\"NS\", \"SC\"],\n",
    "                         filled=True,\n",
    "                         #rounded=True,\n",
    "                         rotate = True,\n",
    "                         label = 'All',\n",
    "                         out_file=None)\n",
    "\n",
    "\n",
    "    graph=pydotplus.graph_from_dot_data(svgData)\n",
    "    \n",
    "    if not os.path.exists('images'):\n",
    "        os.makedirs('images')\n",
    "    \n",
    "    graph.write_svg('images/tree'+ str(i_tree) +'.svg')\n",
    "        \n",
    "    i_tree = i_tree + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SVG(filename='images/tree0.svg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(FinalResultsDF)  \n",
    "displayAccuracies(FinalResultsDF)\n",
    "del FinalResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance of fit models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classifier Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(lr_clf.coef_[0])\n",
    "coef = pd.Series(lr_clf.coef_[0], index=LRSigCols)\n",
    "\n",
    "maxcoef = pd.Series(pd.DataFrame(abs(coef).sort_values(ascending=False).head(20)).index)\n",
    "       \n",
    "weightsplot = pd.Series(coef, index=maxcoef)\n",
    "weightsplot.plot(title = \"Logistic Regression Coefficients\", kind='bar', color = 'Tomato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Rmaxcoef = pd.Series(pd.DataFrame(coef.sort_values(ascending=False)).index)\n",
    "Rweightsplot = pd.Series(coef, index=Rmaxcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i Rweightsplot\n",
    "\n",
    "#install.packages(\"ggplot2\")\n",
    "str(Rweightsplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "Rweightsplot <- as.data.frame(Rweightsplot)\n",
    "Rweightsplot <- cbind(feature = c('Non-Perm. ES - Schedule C (TOA_44)',\n",
    "                                  'Non-Perm. CS (TOA_20)',\n",
    "                                  'BLS_FEDERAL_OtherSep_Rate',\n",
    "                                  'Non-Perm. ES - Schedule B (TOA_42)',\n",
    "                                  'Age 50-54 (AGELVL_H)',\n",
    "                                  'South Dakota (LOC_46)',\n",
    "                                  'Age 45-49 (AGELVL_G)',\n",
    "                                  'Age 55-59 (AGELVL_I)',\n",
    "                                  'Montana (LOC_30)',\n",
    "                                  'BLS_FEDERAL_JobOpenings_Level',\n",
    "                                  'Kansas (LOC_20)',\n",
    "                                  'Arizona (LOC_04)',\n",
    "                                  'Age 40-44 (AGELVL_F)',\n",
    "                                  'New Mexico (LOC_35)',\n",
    "                                  'California (LOC_06)',\n",
    "                                  'Washington (LOC_53)',\n",
    "                                  'Perm. ES - Schedule A (TOA_30)',\n",
    "                                  'Perm. ES - Other (TOA_38)',\n",
    "                                  'Age 35-39 (AGELVL_E)',\n",
    "                                  'Texas (LOC_48)',\n",
    "                                  'Virgina (LOC_51)',\n",
    "                                  'Age 25-29 (AGELVL_C)',\n",
    "                                  'SEPCount_EFDATE_OCCLog',\n",
    "                                  'Ohio (LOC_39)',\n",
    "                                  'Pennsylvania (LOC_42)',\n",
    "                                  'Missouri (LOC_29)',\n",
    "                                  'Stand. Sch. or Eq. Grd. (PPGROUP_11)',\n",
    "                                  'Age 20-24 (AGELVL_B)',\n",
    "                                  'Perm. ES - Schedule D (TOA_35)',\n",
    "                                  'GSEGRD',\n",
    "                                  'LowerLimitAge',\n",
    "                                  'LOSSqrt'),\n",
    "                      Rweightsplot)\n",
    "#Rweightsplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "Rweightsplot$Effect <- ifelse(Rweightsplot$Rweightsplot > 0, 'increase', 'reduce')\n",
    "str(Rweightsplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(ggplot2)\n",
    "theme_set(theme_bw())\n",
    "\n",
    "lolly = ggplot(Rweightsplot, aes(x=reorder(feature,Rweightsplot),\n",
    "                                 y = Rweightsplot, label = round(Rweightsplot, digits = 2), color = Effect)) +\n",
    "            geom_segment(aes(y = 0, x=reorder(feature,Rweightsplot), yend = Rweightsplot, xend = feature), color = \"black\") +\n",
    "            geom_point(stat='identity', fill=\"black\", size=3)  +\n",
    "            geom_text(color=\"black\", size=4, nudge_y = ifelse(Rweightsplot$Effect == \"increase\", 0.9, -1), family=\"Times\") +\n",
    "            labs(title=\"Significant Features\", subtitle=\"Input Variable Log Odds Coefficient Values\", x='Features', y='Log Odds Estimate') +\n",
    "            ylim(-8, 3) +\n",
    "            theme_minimal() +\n",
    "            theme(text = element_text(family = \"Times\"),\n",
    "                  axis.title=element_text(size=12),\n",
    "                  axis.text.y = element_text(size = 12),\n",
    "                  axis.text.x = element_text(size = 12),\n",
    "                  legend.text=element_text(size = 12),\n",
    "                  plot.title = element_text(size = 20),\n",
    "                  plot.subtitle = element_text(size = 12, color = \"darkslategrey\")) +\n",
    "            coord_flip()\n",
    "lolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "pdf(\"images/Coeffs.pdf\", width=8, height=10)\n",
    "print(lolly)\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Fit on full train Professional Data\n",
    "\n",
    "Using the full datset, to create our model fit allows us to fully utilize our dataset instead of simply utilizing the last 80% training fold fit on external data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = OPMAnalysisDataNoFamBinary[\"SEP\"].values # get the labels we want    \n",
    "y = np.where(y==\"NS\",0,1) # turn into numeric binary\n",
    "X = OPMAnalysisDataNoFamBinary.drop(\"SEP\", axis=1)\n",
    "\n",
    "XFC = pd.DataFrame(OPMAnalysisScalerFit.transform(X),columns=X.columns)[fullColumns].as_matrix() \n",
    "XPCC = pd.DataFrame(OPMAnalysisScalerFit.transform(X),columns=X.columns)[PCList]#.as_matrix() \n",
    "XSigC = pd.DataFrame(OPMAnalysisScalerFit.transform(X),columns=X.columns)[LRSigCols].as_matrix() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_clf = RandomForestClassifier(n_estimators       =50, \n",
    "                                 max_features       = 5, \n",
    "                                 max_depth          = 4.0, \n",
    "                                 min_samples_split  = 18, \n",
    "                                 min_samples_leaf   = 9,\n",
    "                                 n_jobs             = -1, \n",
    "                                 random_state       = seed) # get object\n",
    "\n",
    "rfc_clf.fit(XSigC,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors = 250, algorithm = 'kd_tree',leaf_size = 30, n_jobs=-1) # get object\n",
    "\n",
    "knn_clf.fit(XPCC,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(penalty='l2', C=1, class_weight=None, random_state=seed) # get object\n",
    "\n",
    "lr_clf.fit(XSigC,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Admin from the Professional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.isfile(PickleJarPath+\"/OPMAnalysisDataNoFamAdminBinary.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    OPMAnalysisDataNoFamAdminBinary = unpickleObject(\"OPMAnalysisDataNoFamAdminBinary\")\n",
    "else:\n",
    "\n",
    "    OPMAnalysisDataNoFamAdminBinary = SampledOPMDataAdmin.copy()\n",
    "\n",
    "    cols = [\"GENDER\",\n",
    "            \"DATECODE\",\n",
    "            \"QTR\",\n",
    "            \"COUNT\",\n",
    "            \"AGYTYPT\",\n",
    "            \"AGYT\",\n",
    "            \"AGYSUB\",\n",
    "            \"AGYSUBT\",\n",
    "            \"QTR\",\n",
    "            \"AGELVLT\",\n",
    "            \"LOSLVL\",\n",
    "            \"LOSLVLT\",\n",
    "            \"LOCTYPT\",\n",
    "            \"LOCT\",\n",
    "            \"OCCTYP\",\n",
    "            \"OCCTYPT\",\n",
    "            \"OCCFAM\",\n",
    "            \"OCCFAMT\",\n",
    "            \"OCC\",\n",
    "            \"OCCT\",\n",
    "            \"PATCO\",\n",
    "            \"PPGRD\",\n",
    "            \"PATCOT\",\n",
    "            \"PPTYPT\",\n",
    "            \"PPGROUPT\",\n",
    "            \"PAYPLAN\",\n",
    "            \"PAYPLANT\",\n",
    "            \"SALLVLT\",\n",
    "            \"TOATYPT\",\n",
    "            \"TOAT\",\n",
    "            \"WSTYP\",\n",
    "            \"WSTYPT\",\n",
    "            \"WORKSCH\",\n",
    "            \"WORKSCHT\",\n",
    "            \"SALARY\",\n",
    "            \"LOS\",\n",
    "            \"SEPCount_EFDATE_OCC\",\n",
    "            \"SEPCount_EFDATE_LOC\"\n",
    "           ]\n",
    "\n",
    "\n",
    "\n",
    "    #delete cols from analysis data\n",
    "    for col in cols:\n",
    "        if col in list(OPMAnalysisDataNoFamAdminBinary.columns):\n",
    "            del OPMAnalysisDataNoFamAdminBinary[col]\n",
    "\n",
    "    OPMAnalysisDataNoFamAdminBinary.info()\n",
    "\n",
    "    cols = [\"AGELVL\",\n",
    "            \"LOC\",\n",
    "            \"SALLVL\",\n",
    "            \"TOA\",\n",
    "            \"AGYTYP\",\n",
    "            \"AGY\",\n",
    "            \"LOCTYP\",\n",
    "            \"PPTYP\",\n",
    "            \"PPGROUP\",\n",
    "            \"TOATYP\"\n",
    "           ]\n",
    "\n",
    "    #Split Values for cols \n",
    "    for col in cols:\n",
    "        if col in list(OPMAnalysisDataNoFamAdminBinary.columns):\n",
    "            AttSplit = pd.get_dummies(OPMAnalysisDataNoFamAdminBinary[col],prefix=col)\n",
    "            display(AttSplit.head())\n",
    "            OPMAnalysisDataNoFamAdminBinary = pd.concat((OPMAnalysisDataNoFamAdminBinary,AttSplit),axis=1) # add back into the dataframe\n",
    "            del OPMAnalysisDataNoFamAdminBinary[col]\n",
    "\n",
    "    pickleObject(OPMAnalysisDataNoFamAdminBinary, \"OPMAnalysisDataNoFamAdminBinary\")\n",
    "        \n",
    "display(OPMAnalysisDataNoFamAdminBinary.head())\n",
    "print(\"Number of Columns: \",len(OPMAnalysisDataNoFamAdminBinary.columns))\n",
    "OPMAnalysisDataNoFamAdminBinary.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(PickleJarPath+\"/OPMAnalysisScalerFit.pkl\"):\n",
    "    print(\"Found the File! Loading Pickle Now!\")\n",
    "    OPMAnalysisScalerFit = unpickleObject(\"OPMAnalysisScalerFit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#OPMAnalysisDataNoFamAdminBinary = OPMAnalysisDataNoFamAdmin[(OPMAnalysisDataNoFamAdmin[\"SEP\"] == 'NS') | (OPMAnalysisDataNoFamAdmin[\"SEP\"] == 'SC')].reset_index()\n",
    "OPMAnalysisDataNoFamAdminBinaryScaled = OPMAnalysisDataNoFamAdminBinary[OPMScaledAnalysisData.columns]\n",
    "\n",
    "print(OPMAnalysisDataNoFamAdminBinaryScaled.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Admin with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "OPMAnalysisDataNoFamAdminBinaryScaled = pd.DataFrame(OPMAnalysisScalerFit.transform(OPMAnalysisDataNoFamAdminBinaryScaled), columns = OPMAnalysisDataNoFamAdminBinaryScaled.columns)\n",
    "\n",
    "print(\"Overall Accuracy, predicting Admin Binary Separation from Professional Model: \", rfc_clf.score(OPMAnalysisDataNoFamAdminBinaryScaled[LRSigCols], np.where(OPMAnalysisDataNoFamAdminBinary[\"SEP\"]==\"NS\",0,1)))\n",
    "\n",
    "results = pd.concat([OPMAnalysisDataNoFamAdminBinary, pd.DataFrame({\"Prediction\": rfc_clf.predict(OPMAnalysisDataNoFamAdminBinaryScaled[LRSigCols])})], axis = 1)\n",
    "results[\"SEPNum\"] = np.where(results[\"SEP\"]==\"NS\",0,1)\n",
    "results[\"PredictTxt\"] = np.where(results[\"Prediction\"]==0,\"NS\",\"SC\")\n",
    "\n",
    "display(pd.DataFrame({'Cnt' : results.groupby([\"SEP\"]).size()}).reset_index())\n",
    "display(pd.DataFrame({'Cnt' : results.groupby([\"SEP\", \"PredictTxt\"]).size()}).reset_index())\n",
    "\n",
    "print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(results.PredictTxt, results.SEP, rownames = ['True'], colnames = ['Predicted'], margins = True)))\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrixBinary(confusion_matrix(results.Prediction, results.SEPNum), \n",
    "                      classes   =[\"NS\",  \"SC\"], \n",
    "                      normalize =True,\n",
    "                      title     ='Confusion matrix, with normalization')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Admin with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Overall Accuracy, predicting Admin Binary Separation from Professional Model: \", knn_clf.score(OPMAnalysisDataNoFamAdminBinaryScaled[PCList], np.where(OPMAnalysisDataNoFamAdminBinary[\"SEP\"]==\"NS\",0,1)))\n",
    "\n",
    "results = pd.concat([OPMAnalysisDataNoFamAdminBinary, pd.DataFrame({\"Prediction\": knn_clf.predict(OPMAnalysisDataNoFamAdminBinaryScaled[PCList])})], axis = 1)\n",
    "results[\"SEPNum\"] = np.where(results[\"SEP\"]==\"NS\",0,1)\n",
    "results[\"PredictTxt\"] = np.where(results[\"Prediction\"]==0,\"NS\",\"SC\")\n",
    "\n",
    "display(pd.DataFrame({'Cnt' : results.groupby([\"SEP\"]).size()}).reset_index())\n",
    "display(pd.DataFrame({'Cnt' : results.groupby([\"SEP\", \"PredictTxt\"]).size()}).reset_index())\n",
    "\n",
    "print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(results.PredictTxt, results.SEP, rownames = ['True'], colnames = ['Predicted'], margins = True)))\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrixBinary(confusion_matrix(results.Prediction, results.SEPNum), \n",
    "                      classes   =[\"NS\",  \"SC\"], \n",
    "                      normalize =True,\n",
    "                      title     ='Confusion matrix, with normalization')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Admin with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Overall Accuracy, predicting Admin Binary Separation from Professional Model: \", lr_clf.score(OPMAnalysisDataNoFamAdminBinaryScaled[LRSigCols], np.where(OPMAnalysisDataNoFamAdminBinary[\"SEP\"]==\"NS\",0,1)))\n",
    "\n",
    "results = pd.concat([OPMAnalysisDataNoFamAdminBinary, pd.DataFrame({\"Prediction\": lr_clf.predict(OPMAnalysisDataNoFamAdminBinaryScaled[LRSigCols])})], axis = 1)\n",
    "results[\"SEPNum\"] = np.where(results[\"SEP\"]==\"NS\",0,1)\n",
    "results[\"PredictTxt\"] = np.where(results[\"Prediction\"]==0,\"NS\",\"SC\")\n",
    "\n",
    "display(pd.DataFrame({'Cnt' : results.groupby([\"SEP\"]).size()}).reset_index())\n",
    "display(pd.DataFrame({'Cnt' : results.groupby([\"SEP\", \"PredictTxt\"]).size()}).reset_index())\n",
    "\n",
    "print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(results.PredictTxt, results.SEP, rownames = ['True'], colnames = ['Predicted'], margins = True)))\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrixBinary(confusion_matrix(results.Prediction, results.SEPNum), \n",
    "                      classes   =[\"NS\",  \"SC\"], \n",
    "                      normalize =True,\n",
    "                      title     ='Confusion matrix, with normalization')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OPMAnalysisDataNoFamBinary.loc[:, OPMAnalysisDataNoFamBinary.max() != 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCols = [\"SEP\", \"LOSSqrt\", \"LowerLimitAge\", \"GSEGRD\", \"BLS_FEDERAL_JobOpenings_Level\"]\n",
    "\n",
    "vizyDataNonBin = OPMAnalysisDataNoFamBinary[plotCols].copy()\n",
    "\n",
    "#Random matrix doesn't work since all our variables on different scale\n",
    "#noise = np.random.normal(0,0.2,[len(vizyDataNonBin), len(vizyDataNonBin.columns)-2])\n",
    "#vizyDataNonBin.iloc[:,2:] = vizyDataNonBin.drop([\"SEP\", \"LOSSqrt\"], axis=1) + noise\n",
    "\n",
    "vizyDataNonBin.LowerLimitAge += np.random.normal(0,2,len(vizyDataNonBin))\n",
    "vizyDataNonBin.GSEGRD += np.random.normal(0,0.2,len(vizyDataNonBin))\n",
    "vizyDataNonBin.BLS_FEDERAL_JobOpenings_Level += np.random.normal(0,1,len(vizyDataNonBin))\n",
    "#vizyDataNonBin.TOA_44 += np.random.normal(0,0.1,len(vizyDataNonBin))\n",
    "#vizyDataNonBin.TOA_42 += np.random.normal(0,0.1,len(vizyDataNonBin))\n",
    "#vizyDataNonBin.LOC_46 += np.random.normal(0,0.1,len(vizyDataNonBin))\n",
    "\n",
    "vizyDataNonBin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizyDataNonBin.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def infer_cmap(color):\n",
    "    hues = sns.color_palette('Set1')\n",
    "    if color == hues[0]:\n",
    "        return 'Reds'\n",
    "    elif color == hues[1]:\n",
    "        return 'Blues'\n",
    "    \n",
    "def kde_color_plot(x, y, **kwargs):\n",
    "    cmap = infer_cmap(kwargs['color'])\n",
    "    ax = sns.kdeplot(x, y, shade=True, shade_lowest=False, cmap=cmap, **kwargs)\n",
    "    return ax\n",
    "\n",
    "vizyDataNonBin = vizyDataNonBin.rename(index=str, columns={\"BLS_FEDERAL_JobOpenings_Level\": \"BLS_JobOpenings_Level\"})\n",
    "vizyDataNonBin = vizyDataNonBin.drop(\"BLS_JobOpenings_Level\", axis=1)\n",
    "\n",
    "sns.set(font_scale=3.7, palette=\"pastel\", style=\"whitegrid\")\n",
    "\n",
    "g = sns.PairGrid(vizyDataNonBin, hue='SEP', palette={\"NS\":\"#377eb8\", \"SC\":\"#e41a1c\"}, size = 10)\n",
    "g.map_diag(plt.hist)\n",
    "g.map_upper(plt.scatter, s=60, edgecolor=\"white\", alpha=0.2, lw=0)\n",
    "g.map_lower(kde_color_plot, alpha = 0.5)\n",
    "g.add_legend()\n",
    "g.savefig(\"images/ScatterMatrix.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#vizyDataNonBin = vizyDataNonBin.rename(index=str, columns={\"BLS_FEDERAL_JobOpenings_Level\": \"BLS_JobOpenings_Level\"})\n",
    "#\n",
    "#sns.set(font_scale=3.7, palette=\"pastel\", style=\"whitegrid\")\n",
    "#\n",
    "#g = sns.PairGrid(vizyDataNonBin, hue='SEP', palette={\"NS\": \"#2E86C1\", \"SC\": \"#EC7063\"}, size = 10)\n",
    "#g.map_diag(plt.hist)\n",
    "#g.map_offdiag(plt.scatter, s=100, edgecolor=\"white\", alpha=0.2, lw=0)\n",
    "#g.add_legend()\n",
    "#g.savefig(\"images/ScatterMatrix.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#sns.set(font=\"Times\")\n",
    "\n",
    "plotCols = list(OPMAnalysisDataNoFamBinary[LRSigCols].loc[:,OPMAnalysisDataNoFamBinary[LRSigCols].max() == 1].columns)\n",
    "plotCols.extend((\"LOSSqrt\", \"SEP\"))\n",
    "\n",
    "vizyDataBin = OPMAnalysisDataNoFamBinary[plotCols].copy()\n",
    "ageBrackets = ['20-24', '25-29', '35-39', '40-44', '45-49', '50-54', '55-59']\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True, font_scale=2)\n",
    "\n",
    "for i, col in enumerate(vizyDataBin.iloc[:,:-2]):\n",
    "    plt.figure(i).set_size_inches(11, 8)\n",
    "    g = sns.violinplot(x=col, y=\"LOSSqrt\", hue=\"SEP\", data=vizyDataBin, split=True,\n",
    "                   inner=\"quart\", palette={\"NS\": \"g\", \"SC\": \"r\"}, scale = 'count')\n",
    "    \n",
    "    g.set_xticklabels(['No','Yes'])\n",
    "    \n",
    "    if i < 7: g.set(xlabel=('Ages ' + ageBrackets[i]))#, ylabel='Length of Service (Sqrt)')\n",
    "    elif col == 'PPGROUP_11': g.set(xlabel='Pay Plan Group 11')#, ylabel='Length of Service (Sqrt)')\n",
    "    #else: g.set(ylabel='Length of Service (Sqrt)')\n",
    "        \n",
    "    plt.savefig(\"images/Violin_%s.pdf\" % col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCols = list(OPMAnalysisDataNoFamBinary[LRSigCols].loc[:,OPMAnalysisDataNoFamBinary[LRSigCols].max() == 1].columns)\n",
    "plotCols.extend((\"GSEGRD\", \"SEP\"))\n",
    "\n",
    "vizyDataBin = OPMAnalysisDataNoFamBinary[plotCols].copy()\n",
    "\n",
    "g = sns.violinplot(x='PPGROUP_11', y='GSEGRD', hue='SEP', data=vizyDataBin, split=True,\n",
    "                   inner=\"quart\", palette={\"NS\": \"g\", \"SC\": \"r\"}, scale = 'count')\n",
    "\n",
    "g.set_xticklabels(['No','Yes'])\n",
    "g.set(xlabel='Pay Plan Group 11')#, ylabel='Gen. Sch. & Eq. Grade')\n",
    "        \n",
    "plt.savefig(\"images/Violin_GSEGRD_PPGROUP_11.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizyDataBin.groupby([\"PPGROUP_11\", \"SEP\"]).size().reset_index(name=\"Time\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
